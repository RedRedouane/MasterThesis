[2022-07-20 01:43:40,625][HYDRA] Launching 1 jobs locally
[2022-07-20 01:43:40,626][HYDRA] 	#0 : 
[2022-07-20 01:43:42,888][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'RoBERTa_encdec', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 4, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 4, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 20000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [64], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'roberta_enc_dec', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'adaptive_input': False, 'encoder': {'_name': None, 'embed_path': None, 'embed_dim': 768, 'ffn_embed_dim': 3072, 'layers': 12, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None}, 'max_source_positions': 1024, 'decoder': {'_name': None, 'embed_path': None, 'embed_dim': 512, 'ffn_embed_dim': 2048, 'layers': 6, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None, 'input_dim': 512, 'output_dim': 512}, 'max_target_positions': 1024, 'share_decoder_input_output_embed': True, 'share_all_embeddings': True, 'no_token_positional_embeddings': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'layernorm_embedding': False, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'no_cross_attention': False, 'cross_self_attention': False, 'quant_noise': {'_name': None, 'pq': 0.0, 'pq_block_size': 8, 'scalar': 0.0}, 'min_params_to_wrap': 100000000, 'char_inputs': False, 'relu_dropout': 0.0, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'export': False, 'no_decoder_final_norm': False, 'max_positions': 512, 'encoder_embed_dim': 768, 'encoder_layers': 12, 'encoder_ffn_embed_dim': 3072, 'hack_layernorm_embedding': False, 'pretrained_mlm_checkpoint': '/home/dahmanir/lisa/Models/RoBERTa_25_finetune_maskless.pt', 'load_pretrained_mlm_checkpoint': None}, 'task': {'_name': 'translation', 'data': '/home/dahmanir/lisa/Datasets/wiki_binarized', 'source_lang': 'source', 'target_lang': 'target', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 512, 'max_target_positions': 512, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 20000.0, 'lr': [1e-05]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2022-07-20 01:43:43,056][fairseq.tasks.translation][INFO] - [source] dictionary: 39984 types
[2022-07-20 01:43:43,057][fairseq.tasks.translation][INFO] - [target] dictionary: 39984 types
[2022-07-20 01:43:49,076][fairseq.models.fairseq_model][WARNING] - using 'args' is deprecated, please update your code to use dataclass config
encoder.embed_tokens.weight False torch.Size([39984, 768])
encoder.layers.0.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.0.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.0.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.0.fc1.weight False torch.Size([3072, 768])
encoder.layers.0.fc1.bias False torch.Size([3072])
encoder.layers.0.fc2.weight False torch.Size([768, 3072])
encoder.layers.0.fc2.bias False torch.Size([768])
encoder.layers.0.final_layer_norm.weight False torch.Size([768])
encoder.layers.0.final_layer_norm.bias False torch.Size([768])
encoder.layers.1.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.1.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.1.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.1.fc1.weight False torch.Size([3072, 768])
encoder.layers.1.fc1.bias False torch.Size([3072])
encoder.layers.1.fc2.weight False torch.Size([768, 3072])
encoder.layers.1.fc2.bias False torch.Size([768])
encoder.layers.1.final_layer_norm.weight False torch.Size([768])
encoder.layers.1.final_layer_norm.bias False torch.Size([768])
encoder.layers.2.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.2.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.2.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.2.fc1.weight False torch.Size([3072, 768])
encoder.layers.2.fc1.bias False torch.Size([3072])
encoder.layers.2.fc2.weight False torch.Size([768, 3072])
encoder.layers.2.fc2.bias False torch.Size([768])
encoder.layers.2.final_layer_norm.weight False torch.Size([768])
encoder.layers.2.final_layer_norm.bias False torch.Size([768])
encoder.layers.3.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.3.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.3.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.3.fc1.weight False torch.Size([3072, 768])
encoder.layers.3.fc1.bias False torch.Size([3072])
encoder.layers.3.fc2.weight False torch.Size([768, 3072])
encoder.layers.3.fc2.bias False torch.Size([768])
encoder.layers.3.final_layer_norm.weight False torch.Size([768])
encoder.layers.3.final_layer_norm.bias False torch.Size([768])
encoder.layers.4.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.4.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.4.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.4.fc1.weight False torch.Size([3072, 768])
encoder.layers.4.fc1.bias False torch.Size([3072])
encoder.layers.4.fc2.weight False torch.Size([768, 3072])
encoder.layers.4.fc2.bias False torch.Size([768])
encoder.layers.4.final_layer_norm.weight False torch.Size([768])
encoder.layers.4.final_layer_norm.bias False torch.Size([768])
encoder.layers.5.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.5.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.5.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.5.fc1.weight False torch.Size([3072, 768])
encoder.layers.5.fc1.bias False torch.Size([3072])
encoder.layers.5.fc2.weight False torch.Size([768, 3072])
encoder.layers.5.fc2.bias False torch.Size([768])
encoder.layers.5.final_layer_norm.weight False torch.Size([768])
encoder.layers.5.final_layer_norm.bias False torch.Size([768])
encoder.layers.6.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.6.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.6.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.6.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.6.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.6.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.6.fc1.weight False torch.Size([3072, 768])
encoder.layers.6.fc1.bias False torch.Size([3072])
encoder.layers.6.fc2.weight False torch.Size([768, 3072])
encoder.layers.6.fc2.bias False torch.Size([768])
encoder.layers.6.final_layer_norm.weight False torch.Size([768])
encoder.layers.6.final_layer_norm.bias False torch.Size([768])
encoder.layers.7.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.7.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.7.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.7.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.7.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.7.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.7.fc1.weight False torch.Size([3072, 768])
encoder.layers.7.fc1.bias False torch.Size([3072])
encoder.layers.7.fc2.weight False torch.Size([768, 3072])
encoder.layers.7.fc2.bias False torch.Size([768])
encoder.layers.7.final_layer_norm.weight False torch.Size([768])
encoder.layers.7.final_layer_norm.bias False torch.Size([768])
encoder.layers.8.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.8.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.8.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.8.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.8.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.8.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.8.fc1.weight False torch.Size([3072, 768])
encoder.layers.8.fc1.bias False torch.Size([3072])
encoder.layers.8.fc2.weight False torch.Size([768, 3072])
encoder.layers.8.fc2.bias False torch.Size([768])
encoder.layers.8.final_layer_norm.weight False torch.Size([768])
encoder.layers.8.final_layer_norm.bias False torch.Size([768])
encoder.layers.9.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.9.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.9.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.9.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.9.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.9.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.9.fc1.weight False torch.Size([3072, 768])
encoder.layers.9.fc1.bias False torch.Size([3072])
encoder.layers.9.fc2.weight False torch.Size([768, 3072])
encoder.layers.9.fc2.bias False torch.Size([768])
encoder.layers.9.final_layer_norm.weight False torch.Size([768])
encoder.layers.9.final_layer_norm.bias False torch.Size([768])
encoder.layers.10.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.10.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.10.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.10.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.10.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.10.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.10.fc1.weight False torch.Size([3072, 768])
encoder.layers.10.fc1.bias False torch.Size([3072])
encoder.layers.10.fc2.weight False torch.Size([768, 3072])
encoder.layers.10.fc2.bias False torch.Size([768])
encoder.layers.10.final_layer_norm.weight False torch.Size([768])
encoder.layers.10.final_layer_norm.bias False torch.Size([768])
encoder.layers.11.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.11.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.11.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.11.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.11.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.11.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.11.fc1.weight False torch.Size([3072, 768])
encoder.layers.11.fc1.bias False torch.Size([3072])
encoder.layers.11.fc2.weight False torch.Size([768, 3072])
encoder.layers.11.fc2.bias False torch.Size([768])
encoder.layers.11.final_layer_norm.weight False torch.Size([768])
encoder.layers.11.final_layer_norm.bias False torch.Size([768])
decoder.layers.0.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.0.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.0.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.0.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.0.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.0.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.0.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.0.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.0.fc1.weight True torch.Size([3072, 768])
decoder.layers.0.fc1.bias True torch.Size([3072])
decoder.layers.0.fc2.weight True torch.Size([768, 3072])
decoder.layers.0.fc2.bias True torch.Size([768])
decoder.layers.0.final_layer_norm.weight True torch.Size([768])
decoder.layers.0.final_layer_norm.bias True torch.Size([768])
decoder.layers.1.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.1.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.1.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.1.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.1.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.1.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.1.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.1.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.1.fc1.weight True torch.Size([3072, 768])
decoder.layers.1.fc1.bias True torch.Size([3072])
decoder.layers.1.fc2.weight True torch.Size([768, 3072])
decoder.layers.1.fc2.bias True torch.Size([768])
decoder.layers.1.final_layer_norm.weight True torch.Size([768])
decoder.layers.1.final_layer_norm.bias True torch.Size([768])
decoder.layers.2.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.2.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.2.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.2.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.2.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.2.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.2.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.2.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.2.fc1.weight True torch.Size([3072, 768])
decoder.layers.2.fc1.bias True torch.Size([3072])
decoder.layers.2.fc2.weight True torch.Size([768, 3072])
decoder.layers.2.fc2.bias True torch.Size([768])
decoder.layers.2.final_layer_norm.weight True torch.Size([768])
decoder.layers.2.final_layer_norm.bias True torch.Size([768])
decoder.layers.3.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.3.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.3.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.3.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.3.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.3.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.3.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.3.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.3.fc1.weight True torch.Size([3072, 768])
decoder.layers.3.fc1.bias True torch.Size([3072])
decoder.layers.3.fc2.weight True torch.Size([768, 3072])
decoder.layers.3.fc2.bias True torch.Size([768])
decoder.layers.3.final_layer_norm.weight True torch.Size([768])
decoder.layers.3.final_layer_norm.bias True torch.Size([768])
decoder.layers.4.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.4.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.4.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.4.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.4.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.4.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.4.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.4.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.4.fc1.weight True torch.Size([3072, 768])
decoder.layers.4.fc1.bias True torch.Size([3072])
decoder.layers.4.fc2.weight True torch.Size([768, 3072])
decoder.layers.4.fc2.bias True torch.Size([768])
decoder.layers.4.final_layer_norm.weight True torch.Size([768])
decoder.layers.4.final_layer_norm.bias True torch.Size([768])
decoder.layers.5.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.5.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.5.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.5.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.5.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.5.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.5.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.5.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.5.fc1.weight True torch.Size([3072, 768])
decoder.layers.5.fc1.bias True torch.Size([3072])
decoder.layers.5.fc2.weight True torch.Size([768, 3072])
decoder.layers.5.fc2.bias True torch.Size([768])
decoder.layers.5.final_layer_norm.weight True torch.Size([768])
decoder.layers.5.final_layer_norm.bias True torch.Size([768])
decoder.layers.6.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.6.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.6.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.6.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.6.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.6.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.6.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.6.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.6.fc1.weight True torch.Size([3072, 768])
decoder.layers.6.fc1.bias True torch.Size([3072])
decoder.layers.6.fc2.weight True torch.Size([768, 3072])
decoder.layers.6.fc2.bias True torch.Size([768])
decoder.layers.6.final_layer_norm.weight True torch.Size([768])
decoder.layers.6.final_layer_norm.bias True torch.Size([768])
decoder.layers.7.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.7.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.7.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.7.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.7.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.7.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.7.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.7.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.7.fc1.weight True torch.Size([3072, 768])
decoder.layers.7.fc1.bias True torch.Size([3072])
decoder.layers.7.fc2.weight True torch.Size([768, 3072])
decoder.layers.7.fc2.bias True torch.Size([768])
decoder.layers.7.final_layer_norm.weight True torch.Size([768])
decoder.layers.7.final_layer_norm.bias True torch.Size([768])
decoder.layers.8.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.8.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.8.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.8.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.8.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.8.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.8.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.8.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.8.fc1.weight True torch.Size([3072, 768])
decoder.layers.8.fc1.bias True torch.Size([3072])
decoder.layers.8.fc2.weight True torch.Size([768, 3072])
decoder.layers.8.fc2.bias True torch.Size([768])
decoder.layers.8.final_layer_norm.weight True torch.Size([768])
decoder.layers.8.final_layer_norm.bias True torch.Size([768])
decoder.layers.9.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.9.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.9.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.9.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.9.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.9.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.9.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.9.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.9.fc1.weight True torch.Size([3072, 768])
decoder.layers.9.fc1.bias True torch.Size([3072])
decoder.layers.9.fc2.weight True torch.Size([768, 3072])
decoder.layers.9.fc2.bias True torch.Size([768])
decoder.layers.9.final_layer_norm.weight True torch.Size([768])
decoder.layers.9.final_layer_norm.bias True torch.Size([768])
decoder.layers.10.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.10.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.10.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.10.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.10.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.10.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.10.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.10.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.10.fc1.weight True torch.Size([3072, 768])
decoder.layers.10.fc1.bias True torch.Size([3072])
decoder.layers.10.fc2.weight True torch.Size([768, 3072])
decoder.layers.10.fc2.bias True torch.Size([768])
decoder.layers.10.final_layer_norm.weight True torch.Size([768])
decoder.layers.10.final_layer_norm.bias True torch.Size([768])
decoder.layers.11.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.11.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.11.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.11.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.11.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.11.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.11.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.11.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.11.fc1.weight True torch.Size([3072, 768])
decoder.layers.11.fc1.bias True torch.Size([3072])
decoder.layers.11.fc2.weight True torch.Size([768, 3072])
decoder.layers.11.fc2.bias True torch.Size([768])
decoder.layers.11.final_layer_norm.weight True torch.Size([768])
decoder.layers.11.final_layer_norm.bias True torch.Size([768])
decoder.output_projection.bias False torch.Size([39984])
decoder.output_projection.dense.weight True torch.Size([768, 768])
decoder.output_projection.dense.bias True torch.Size([768])
decoder.output_projection.layer_norm.weight True torch.Size([768])
decoder.output_projection.layer_norm.bias True torch.Size([768])
[2022-07-20 01:43:53,958][fairseq_cli.train][INFO] - RobertaEncDecModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39984, 768, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39984, 768, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): RobertaLMHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[2022-07-20 01:43:53,964][fairseq_cli.train][INFO] - task: TranslationTask
[2022-07-20 01:43:53,964][fairseq_cli.train][INFO] - model: RobertaEncDecModel
[2022-07-20 01:43:53,964][fairseq_cli.train][INFO] - criterion: LabelSmoothedCrossEntropyCriterion
[2022-07-20 01:43:53,968][fairseq_cli.train][INFO] - num. shared model params: 229,815,600 (num. trained: 114,013,440)
[2022-07-20 01:43:53,971][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2022-07-20 01:43:53,980][fairseq.data.data_utils][INFO] - loaded 3,123 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/valid.source-target.source
[2022-07-20 01:43:53,982][fairseq.data.data_utils][INFO] - loaded 3,123 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/valid.source-target.target
[2022-07-20 01:43:53,983][fairseq.tasks.translation][INFO] - /home/dahmanir/lisa/Datasets/wiki_binarized valid source-target 3123 examples
[2022-07-20 01:43:58,163][fairseq.trainer][INFO] - detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
[2022-07-20 01:43:58,164][fairseq.trainer][INFO] - detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
[2022-07-20 01:43:58,165][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-07-20 01:43:58,165][fairseq.utils][INFO] - rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = NVIDIA GeForce GTX 1080 Ti              
[2022-07-20 01:43:58,165][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-07-20 01:43:58,165][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2022-07-20 01:43:58,166][fairseq_cli.train][INFO] - max tokens per device = None and max sentences per device = 4
[2022-07-20 01:43:58,168][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2022-07-20 01:43:58,168][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2022-07-20 01:43:58,168][fairseq.trainer][INFO] - loading train data for epoch 1
[2022-07-20 01:43:58,192][fairseq.data.data_utils][INFO] - loaded 21,866 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/train.source-target.source
[2022-07-20 01:43:58,198][fairseq.data.data_utils][INFO] - loaded 21,866 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/train.source-target.target
[2022-07-20 01:43:58,198][fairseq.tasks.translation][INFO] - /home/dahmanir/lisa/Datasets/wiki_binarized train source-target 21866 examples
[2022-07-20 01:43:58,208][fairseq.tasks.fairseq_task][WARNING] - 6 samples have invalid sizes and will be skipped, max_positions=(512, 512), first few sample ids=[4345, 8071, 5665, 126, 8210, 2220]
[2022-07-20 01:43:58,229][fairseq.trainer][INFO] - NOTE: your device does NOT support faster training with --fp16 or --amp, please switch to FP32 which is likely to be faster
[2022-07-20 01:43:58,259][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
wandb: Currently logged in as: redredouane. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.21 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.19
wandb: Run data is saved locally in /home/dahmanir/multirun/2022-07-20/01-43-38/0/wandb/run-20220720_014403-20t1e08b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run checkpoints
wandb:  View project at https://wandb.ai/redredouane/RoBERTa_encdec
wandb:  View run at https://wandb.ai/redredouane/RoBERTa_encdec/runs/20t1e08b
[2022-07-20 01:44:08,008][fairseq.trainer][INFO] - begin training epoch 1
[2022-07-20 01:44:08,010][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 01:44:17,986][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
[2022-07-20 01:44:28,454][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 01:44:38,858][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2022-07-20 01:44:49,667][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2022-07-20 01:45:00,456][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-20 01:45:11,480][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 01:56:12,256][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 01:57:17,927][valid][INFO] - {"epoch": 1, "valid_loss": "12.056", "valid_nll_loss": "11.337", "valid_ppl": "2587.27", "valid_wps": "2330.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "80"}
[2022-07-20 01:57:17,934][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 80 updates
[2022-07-20 01:57:17,936][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 01:57:23,872][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 01:57:27,364][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 80 updates, score 12.056) (writing took 9.430254245875403 seconds)
[2022-07-20 01:57:27,365][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2022-07-20 01:57:27,372][train][INFO] - {"epoch": 1, "train_loss": "15.892", "train_nll_loss": "15.46", "train_ppl": "45058.3", "train_wps": "1353.2", "train_ups": "0.11", "train_wpb": "12418.6", "train_bsz": "254.1", "train_num_updates": "80", "train_lr": "1.6e-06", "train_gnorm": "34.025", "train_clip": "100", "train_loss_scale": "2", "train_train_wall": "703", "train_gb_free": "8", "train_wall": "809"}
[2022-07-20 01:57:27,388][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 01:57:27,427][fairseq.trainer][INFO] - begin training epoch 2
[2022-07-20 01:57:27,428][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 02:09:14,004][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 02:10:19,769][valid][INFO] - {"epoch": 2, "valid_loss": "11.284", "valid_nll_loss": "10.528", "valid_ppl": "1476.49", "valid_wps": "2327.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "166", "valid_best_loss": "11.284"}
[2022-07-20 02:10:19,773][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 166 updates
[2022-07-20 02:10:19,777][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 02:10:26,085][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 02:10:29,948][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 166 updates, score 11.284) (writing took 10.174611384049058 seconds)
[2022-07-20 02:10:29,949][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2022-07-20 02:10:29,955][train][INFO] - {"epoch": 2, "train_loss": "11.649", "train_nll_loss": "10.922", "train_ppl": "1939.97", "train_wps": "1371.9", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "166", "train_lr": "3.32e-06", "train_gnorm": "1.943", "train_clip": "100", "train_loss_scale": "2", "train_train_wall": "686", "train_gb_free": "7.7", "train_wall": "1592"}
[2022-07-20 02:10:29,968][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 02:10:31,062][fairseq.trainer][INFO] - begin training epoch 3
[2022-07-20 02:10:31,063][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 02:15:13,286][train_inner][INFO] - {"epoch": 3, "update": 2.395, "loss": "13.274", "nll_loss": "12.659", "ppl": "6466.4", "wps": "1389.5", "ups": "0.11", "wpb": "12505.1", "bsz": "254.4", "num_updates": "200", "lr": "4e-06", "gnorm": "14.659", "clip": "100", "loss_scale": "2", "train_wall": "1663", "gb_free": "7.6", "wall": "1875"}
[2022-07-20 02:22:17,526][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 02:23:23,176][valid][INFO] - {"epoch": 3, "valid_loss": "10.35", "valid_nll_loss": "9.41", "valid_ppl": "680.22", "valid_wps": "2331.6", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "252", "valid_best_loss": "10.35"}
[2022-07-20 02:23:23,182][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 252 updates
[2022-07-20 02:23:23,185][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 02:23:29,426][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 02:23:33,311][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 252 updates, score 10.35) (writing took 10.129704086109996 seconds)
[2022-07-20 02:23:33,312][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2022-07-20 02:23:33,318][train][INFO] - {"epoch": 3, "train_loss": "11.033", "train_nll_loss": "10.233", "train_ppl": "1203.12", "train_wps": "1370.6", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "252", "train_lr": "5.04e-06", "train_gnorm": "1.766", "train_clip": "100", "train_loss_scale": "2", "train_train_wall": "686", "train_gb_free": "7.7", "train_wall": "2375"}
[2022-07-20 02:23:33,335][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 02:23:33,374][fairseq.trainer][INFO] - begin training epoch 4
[2022-07-20 02:23:33,377][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 02:35:19,517][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 02:36:25,158][valid][INFO] - {"epoch": 4, "valid_loss": "9.679", "valid_nll_loss": "8.581", "valid_ppl": "382.83", "valid_wps": "2332.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "338", "valid_best_loss": "9.679"}
[2022-07-20 02:36:25,161][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 338 updates
[2022-07-20 02:36:25,164][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 02:36:31,432][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 02:36:35,341][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 338 updates, score 9.679) (writing took 10.179072750965133 seconds)
[2022-07-20 02:36:35,343][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2022-07-20 02:36:35,352][train][INFO] - {"epoch": 4, "train_loss": "10.189", "train_nll_loss": "9.234", "train_ppl": "602.23", "train_wps": "1372.9", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "338", "train_lr": "6.76e-06", "train_gnorm": "2.75", "train_clip": "100", "train_loss_scale": "4", "train_train_wall": "685", "train_gb_free": "8.1", "train_wall": "3157"}
[2022-07-20 02:36:35,367][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 02:36:35,405][fairseq.trainer][INFO] - begin training epoch 5
[2022-07-20 02:36:35,406][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 02:45:08,125][train_inner][INFO] - {"epoch": 5, "update": 4.721, "loss": "10.21", "nll_loss": "9.261", "ppl": "613.64", "wps": "1387.9", "ups": "0.11", "wpb": "12455.6", "bsz": "254.4", "num_updates": "400", "lr": "8e-06", "gnorm": "2.658", "clip": "100", "loss_scale": "4", "train_wall": "1595", "gb_free": "7.5", "wall": "3670"}
[2022-07-20 02:48:22,261][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 02:49:28,057][valid][INFO] - {"epoch": 5, "valid_loss": "9.318", "valid_nll_loss": "8.17", "valid_ppl": "288.08", "valid_wps": "2328.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "424", "valid_best_loss": "9.318"}
[2022-07-20 02:49:28,060][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 424 updates
[2022-07-20 02:49:28,064][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 02:49:35,088][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 02:49:39,231][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 424 updates, score 9.318) (writing took 11.170468122931197 seconds)
[2022-07-20 02:49:39,234][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2022-07-20 02:49:39,244][train][INFO] - {"epoch": 5, "train_loss": "9.663", "train_nll_loss": "8.617", "train_ppl": "392.55", "train_wps": "1369.6", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "424", "train_lr": "8.48e-06", "train_gnorm": "3.185", "train_clip": "100", "train_loss_scale": "4", "train_train_wall": "686", "train_gb_free": "6.9", "train_wall": "3941"}
[2022-07-20 02:49:39,262][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 02:49:39,301][fairseq.trainer][INFO] - begin training epoch 6
[2022-07-20 02:49:39,302][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 03:01:24,613][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 03:02:30,245][valid][INFO] - {"epoch": 6, "valid_loss": "9.061", "valid_nll_loss": "7.866", "valid_ppl": "233.36", "valid_wps": "2332.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "510", "valid_best_loss": "9.061"}
[2022-07-20 03:02:30,250][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 510 updates
[2022-07-20 03:02:30,253][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 03:02:36,711][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 03:02:40,636][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 510 updates, score 9.061) (writing took 10.3855580620002 seconds)
[2022-07-20 03:02:40,636][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2022-07-20 03:02:40,642][train][INFO] - {"epoch": 6, "train_loss": "9.35", "train_nll_loss": "8.261", "train_ppl": "306.68", "train_wps": "1374", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "510", "train_lr": "9.99487e-06", "train_gnorm": "3.35", "train_clip": "100", "train_loss_scale": "4", "train_train_wall": "685", "train_gb_free": "7.6", "train_wall": "4722"}
[2022-07-20 03:02:40,658][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 03:02:40,698][fairseq.trainer][INFO] - begin training epoch 7
[2022-07-20 03:02:40,700][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 03:14:26,131][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 03:15:31,626][valid][INFO] - {"epoch": 7, "valid_loss": "8.827", "valid_nll_loss": "7.613", "valid_ppl": "195.76", "valid_wps": "2336.4", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "596", "valid_best_loss": "8.827"}
[2022-07-20 03:15:31,629][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 596 updates
[2022-07-20 03:15:31,631][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 03:15:38,700][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 03:15:42,467][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 596 updates, score 8.827) (writing took 10.837671858957037 seconds)
[2022-07-20 03:15:42,468][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2022-07-20 03:15:42,475][train][INFO] - {"epoch": 7, "train_loss": "9.078", "train_nll_loss": "7.952", "train_ppl": "247.58", "train_wps": "1373.2", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "596", "train_lr": "9.95077e-06", "train_gnorm": "3.213", "train_clip": "100", "train_loss_scale": "8", "train_train_wall": "685", "train_gb_free": "7.9", "train_wall": "5504"}
[2022-07-20 03:15:42,488][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 03:15:42,523][fairseq.trainer][INFO] - begin training epoch 8
[2022-07-20 03:15:42,525][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 03:16:15,875][train_inner][INFO] - {"epoch": 8, "update": 7.047, "loss": "9.244", "nll_loss": "8.139", "ppl": "281.97", "wps": "1336", "ups": "0.11", "wpb": "12476.9", "bsz": "253.7", "num_updates": "600", "lr": "9.94872e-06", "gnorm": "3.32", "clip": "100", "loss_scale": "8", "train_wall": "1590", "gb_free": "7.2", "wall": "5538"}
[2022-07-20 03:27:28,808][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 03:28:34,302][valid][INFO] - {"epoch": 8, "valid_loss": "8.63", "valid_nll_loss": "7.395", "valid_ppl": "168.26", "valid_wps": "2336.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "682", "valid_best_loss": "8.63"}
[2022-07-20 03:28:34,307][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 682 updates
[2022-07-20 03:28:34,309][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 03:28:40,691][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 03:28:45,374][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 682 updates, score 8.63) (writing took 11.067002387018874 seconds)
[2022-07-20 03:28:45,375][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2022-07-20 03:28:45,382][train][INFO] - {"epoch": 8, "train_loss": "8.861", "train_nll_loss": "7.71", "train_ppl": "209.33", "train_wps": "1371.4", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "682", "train_lr": "9.90667e-06", "train_gnorm": "2.849", "train_clip": "100", "train_loss_scale": "8", "train_train_wall": "686", "train_gb_free": "7.9", "train_wall": "6287"}
[2022-07-20 03:28:45,395][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 03:28:45,431][fairseq.trainer][INFO] - begin training epoch 9
[2022-07-20 03:28:45,432][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 03:40:31,045][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 03:41:36,438][valid][INFO] - {"epoch": 9, "valid_loss": "8.497", "valid_nll_loss": "7.233", "valid_ppl": "150.44", "valid_wps": "2340", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "768", "valid_best_loss": "8.497"}
[2022-07-20 03:41:36,443][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 768 updates
[2022-07-20 03:41:36,446][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 03:41:42,631][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 03:41:46,544][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 9 @ 768 updates, score 8.497) (writing took 10.101305465912446 seconds)
[2022-07-20 03:41:46,545][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2022-07-20 03:41:46,553][train][INFO] - {"epoch": 9, "train_loss": "8.685", "train_nll_loss": "7.512", "train_ppl": "182.56", "train_wps": "1374.4", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "768", "train_lr": "9.86256e-06", "train_gnorm": "2.962", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "685", "train_gb_free": "8", "train_wall": "7068"}
[2022-07-20 03:41:46,565][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 03:41:46,605][fairseq.trainer][INFO] - begin training epoch 10
[2022-07-20 03:41:46,606][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 03:46:10,271][train_inner][INFO] - {"epoch": 10, "update": 9.372, "loss": "8.744", "nll_loss": "7.579", "ppl": "191.17", "wps": "1395.4", "ups": "0.11", "wpb": "12519.2", "bsz": "254.4", "num_updates": "800", "lr": "9.84615e-06", "gnorm": "2.865", "clip": "100", "loss_scale": "16", "train_wall": "1594", "gb_free": "7", "wall": "7332"}
[2022-07-20 03:53:31,920][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 03:54:37,446][valid][INFO] - {"epoch": 10, "valid_loss": "8.383", "valid_nll_loss": "7.112", "valid_ppl": "138.31", "valid_wps": "2335.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "854", "valid_best_loss": "8.383"}
[2022-07-20 03:54:37,449][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 854 updates
[2022-07-20 03:54:37,451][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 03:54:43,816][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 03:54:47,854][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 854 updates, score 8.383) (writing took 10.405017391080037 seconds)
[2022-07-20 03:54:47,855][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2022-07-20 03:54:47,861][train][INFO] - {"epoch": 10, "train_loss": "8.545", "train_nll_loss": "7.354", "train_ppl": "163.62", "train_wps": "1374.2", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "854", "train_lr": "9.81846e-06", "train_gnorm": "2.771", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "685", "train_gb_free": "7.9", "train_wall": "7850"}
[2022-07-20 03:54:47,875][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 03:54:47,917][fairseq.trainer][INFO] - begin training epoch 11
[2022-07-20 03:54:47,918][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 04:06:31,727][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 04:07:37,233][valid][INFO] - {"epoch": 11, "valid_loss": "8.272", "valid_nll_loss": "6.989", "valid_ppl": "127.07", "valid_wps": "2336.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "940", "valid_best_loss": "8.272"}
[2022-07-20 04:07:37,237][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 940 updates
[2022-07-20 04:07:37,238][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 04:07:43,663][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 04:07:47,809][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 11 @ 940 updates, score 8.272) (writing took 10.572847037110478 seconds)
[2022-07-20 04:07:47,810][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2022-07-20 04:07:47,816][train][INFO] - {"epoch": 11, "train_loss": "8.424", "train_nll_loss": "7.215", "train_ppl": "148.6", "train_wps": "1376.5", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "940", "train_lr": "9.77436e-06", "train_gnorm": "2.907", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "683", "train_gb_free": "8.1", "train_wall": "8630"}
[2022-07-20 04:07:47,831][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 04:07:47,879][fairseq.trainer][INFO] - begin training epoch 12
[2022-07-20 04:07:47,881][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 04:16:01,862][train_inner][INFO] - {"epoch": 12, "update": 11.698, "loss": "8.42", "nll_loss": "7.211", "ppl": "148.11", "wps": "1390.6", "ups": "0.11", "wpb": "12456.9", "bsz": "254.4", "num_updates": "1000", "lr": "9.74359e-06", "gnorm": "2.854", "clip": "100", "loss_scale": "16", "train_wall": "1591", "gb_free": "7.6", "wall": "9124"}
[2022-07-20 04:19:31,647][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 04:20:37,166][valid][INFO] - {"epoch": 12, "valid_loss": "8.183", "valid_nll_loss": "6.886", "valid_ppl": "118.25", "valid_wps": "2335.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1026", "valid_best_loss": "8.183"}
[2022-07-20 04:20:37,172][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 1026 updates
[2022-07-20 04:20:37,175][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 04:20:43,470][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 04:20:47,521][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 1026 updates, score 8.183) (writing took 10.34908670396544 seconds)
[2022-07-20 04:20:47,522][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2022-07-20 04:20:47,527][train][INFO] - {"epoch": 12, "train_loss": "8.322", "train_nll_loss": "7.099", "train_ppl": "137.06", "train_wps": "1377", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1026", "train_lr": "9.73026e-06", "train_gnorm": "2.791", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "683", "train_gb_free": "7.2", "train_wall": "9409"}
[2022-07-20 04:20:47,544][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 04:20:47,589][fairseq.trainer][INFO] - begin training epoch 13
[2022-07-20 04:20:47,591][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 04:32:30,537][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 04:33:36,043][valid][INFO] - {"epoch": 13, "valid_loss": "8.116", "valid_nll_loss": "6.796", "valid_ppl": "111.13", "valid_wps": "2336.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1112", "valid_best_loss": "8.116"}
[2022-07-20 04:33:36,048][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 1112 updates
[2022-07-20 04:33:36,051][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 04:33:42,371][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 04:33:46,510][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 13 @ 1112 updates, score 8.116) (writing took 10.462496278109029 seconds)
[2022-07-20 04:33:46,511][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2022-07-20 04:33:46,517][train][INFO] - {"epoch": 13, "train_loss": "8.228", "train_nll_loss": "6.991", "train_ppl": "127.16", "train_wps": "1378.3", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1112", "train_lr": "9.68615e-06", "train_gnorm": "2.71", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "682", "train_gb_free": "7.7", "train_wall": "10188"}
[2022-07-20 04:33:46,531][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 04:33:46,574][fairseq.trainer][INFO] - begin training epoch 14
[2022-07-20 04:33:46,576][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 04:45:30,343][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 04:46:35,975][valid][INFO] - {"epoch": 14, "valid_loss": "8.021", "valid_nll_loss": "6.697", "valid_ppl": "103.75", "valid_wps": "2331.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1198", "valid_best_loss": "8.021"}
[2022-07-20 04:46:35,979][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 1198 updates
[2022-07-20 04:46:35,980][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 04:46:42,263][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 04:46:46,538][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 1198 updates, score 8.021) (writing took 10.558886567130685 seconds)
[2022-07-20 04:46:46,539][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2022-07-20 04:46:46,548][train][INFO] - {"epoch": 14, "train_loss": "8.142", "train_nll_loss": "6.891", "train_ppl": "118.67", "train_wps": "1376.4", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1198", "train_lr": "9.64205e-06", "train_gnorm": "2.736", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "683", "train_gb_free": "8", "train_wall": "10968"}
[2022-07-20 04:46:46,563][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 04:46:46,605][fairseq.trainer][INFO] - begin training epoch 15
[2022-07-20 04:46:46,606][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 04:47:03,274][train_inner][INFO] - {"epoch": 15, "update": 14.023, "loss": "8.198", "nll_loss": "6.955", "ppl": "124.09", "wps": "1339.2", "ups": "0.11", "wpb": "12464.1", "bsz": "253.7", "num_updates": "1200", "lr": "9.64103e-06", "gnorm": "2.729", "clip": "100", "loss_scale": "32", "train_wall": "1585", "gb_free": "6.8", "wall": "10985"}
[2022-07-20 04:58:10,826][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 04:58:30,706][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 04:59:36,298][valid][INFO] - {"epoch": 15, "valid_loss": "7.966", "valid_nll_loss": "6.63", "valid_ppl": "99.02", "valid_wps": "2333.4", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1283", "valid_best_loss": "7.966"}
[2022-07-20 04:59:36,302][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 1283 updates
[2022-07-20 04:59:36,305][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 04:59:42,609][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 04:59:46,769][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 15 @ 1283 updates, score 7.966) (writing took 10.466884318040684 seconds)
[2022-07-20 04:59:46,770][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2022-07-20 04:59:46,775][train][INFO] - {"epoch": 15, "train_loss": "8.067", "train_nll_loss": "6.803", "train_ppl": "111.7", "train_wps": "1359", "train_ups": "0.11", "train_wpb": "12474", "train_bsz": "254.2", "train_num_updates": "1283", "train_lr": "9.59846e-06", "train_gnorm": "2.848", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "683", "train_gb_free": "7.4", "train_wall": "11749"}
[2022-07-20 04:59:46,790][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 04:59:46,834][fairseq.trainer][INFO] - begin training epoch 16
[2022-07-20 04:59:46,835][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 05:11:30,046][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 05:12:35,516][valid][INFO] - {"epoch": 16, "valid_loss": "7.904", "valid_nll_loss": "6.555", "valid_ppl": "94.06", "valid_wps": "2337.4", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1369", "valid_best_loss": "7.904"}
[2022-07-20 05:12:35,521][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 1369 updates
[2022-07-20 05:12:35,524][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 05:12:41,789][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 05:12:47,061][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 16 @ 1369 updates, score 7.904) (writing took 11.539833120070398 seconds)
[2022-07-20 05:12:47,062][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2022-07-20 05:12:47,069][train][INFO] - {"epoch": 16, "train_loss": "7.997", "train_nll_loss": "6.722", "train_ppl": "105.59", "train_wps": "1376", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1369", "train_lr": "9.55436e-06", "train_gnorm": "2.783", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "682", "train_gb_free": "8.1", "train_wall": "12529"}
[2022-07-20 05:12:47,083][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 05:12:47,136][fairseq.trainer][INFO] - begin training epoch 17
[2022-07-20 05:12:47,138][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 05:17:02,737][train_inner][INFO] - {"epoch": 17, "update": 16.36, "loss": "8.014", "nll_loss": "6.742", "ppl": "107.05", "wps": "1389.5", "ups": "0.11", "wpb": "12501.7", "bsz": "254.4", "num_updates": "1400", "lr": "9.53846e-06", "gnorm": "2.784", "clip": "100", "loss_scale": "32", "train_wall": "1598", "gb_free": "7.5", "wall": "12785"}
[2022-07-20 05:24:31,413][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 05:25:37,010][valid][INFO] - {"epoch": 17, "valid_loss": "7.856", "valid_nll_loss": "6.498", "valid_ppl": "90.35", "valid_wps": "2333.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1455", "valid_best_loss": "7.856"}
[2022-07-20 05:25:37,014][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 1455 updates
[2022-07-20 05:25:37,015][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 05:25:43,933][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 05:25:48,003][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 17 @ 1455 updates, score 7.856) (writing took 10.98927702801302 seconds)
[2022-07-20 05:25:48,004][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2022-07-20 05:25:48,009][train][INFO] - {"epoch": 17, "train_loss": "7.929", "train_nll_loss": "6.644", "train_ppl": "100.02", "train_wps": "1374.8", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1455", "train_lr": "9.51026e-06", "train_gnorm": "2.702", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "683", "train_gb_free": "7", "train_wall": "13310"}
[2022-07-20 05:25:48,027][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 05:25:48,084][fairseq.trainer][INFO] - begin training epoch 18
[2022-07-20 05:25:48,086][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 05:37:31,227][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 05:38:36,768][valid][INFO] - {"epoch": 18, "valid_loss": "7.793", "valid_nll_loss": "6.418", "valid_ppl": "85.53", "valid_wps": "2334.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1541", "valid_best_loss": "7.793"}
[2022-07-20 05:38:36,773][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 1541 updates
[2022-07-20 05:38:36,776][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 05:38:43,050][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 05:38:47,129][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 18 @ 1541 updates, score 7.793) (writing took 10.356290633091703 seconds)
[2022-07-20 05:38:47,130][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2022-07-20 05:38:47,136][train][INFO] - {"epoch": 18, "train_loss": "7.868", "train_nll_loss": "6.574", "train_ppl": "95.27", "train_wps": "1378", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1541", "train_lr": "9.46615e-06", "train_gnorm": "2.705", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "683", "train_gb_free": "7.7", "train_wall": "14089"}
[2022-07-20 05:38:47,150][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 05:38:47,201][fairseq.trainer][INFO] - begin training epoch 19
[2022-07-20 05:38:47,202][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 05:38:55,569][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 05:47:02,227][train_inner][INFO] - {"epoch": 19, "update": 18.698, "loss": "7.871", "nll_loss": "6.578", "ppl": "95.51", "wps": "1391.2", "ups": "0.11", "wpb": "12517.1", "bsz": "254.4", "num_updates": "1600", "lr": "9.4359e-06", "gnorm": "2.725", "clip": "100", "loss_scale": "32", "train_wall": "1598", "gb_free": "7.3", "wall": "14584"}
[2022-07-20 05:50:31,403][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 05:51:36,884][valid][INFO] - {"epoch": 19, "valid_loss": "7.749", "valid_nll_loss": "6.37", "valid_ppl": "82.71", "valid_wps": "2336.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1626", "valid_best_loss": "7.749"}
[2022-07-20 05:51:36,887][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 1626 updates
[2022-07-20 05:51:36,889][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 05:51:43,375][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 05:51:47,410][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 19 @ 1626 updates, score 7.749) (writing took 10.52281543193385 seconds)
[2022-07-20 05:51:47,411][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2022-07-20 05:51:47,418][train][INFO] - {"epoch": 19, "train_loss": "7.809", "train_nll_loss": "6.505", "train_ppl": "90.81", "train_wps": "1360.6", "train_ups": "0.11", "train_wpb": "12489.7", "train_bsz": "254.2", "train_num_updates": "1626", "train_lr": "9.42256e-06", "train_gnorm": "2.697", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "683", "train_gb_free": "7.6", "train_wall": "14869"}
[2022-07-20 05:51:47,431][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 05:51:47,489][fairseq.trainer][INFO] - begin training epoch 20
[2022-07-20 05:51:47,490][fairseq_cli.train][INFO] - Start iterating over samples
wandb: Network error (ReadTimeout), entering retry loop.
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-20 06:03:31,692][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 06:04:37,390][valid][INFO] - {"epoch": 20, "valid_loss": "7.698", "valid_nll_loss": "6.325", "valid_ppl": "80.15", "valid_wps": "2329.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1712", "valid_best_loss": "7.698"}
[2022-07-20 06:04:37,394][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 1712 updates
[2022-07-20 06:04:37,397][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 06:04:44,829][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 06:04:48,902][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 20 @ 1712 updates, score 7.698) (writing took 11.507777035003528 seconds)
[2022-07-20 06:04:48,903][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2022-07-20 06:04:48,909][train][INFO] - {"epoch": 20, "train_loss": "7.758", "train_nll_loss": "6.446", "train_ppl": "87.19", "train_wps": "1373.8", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1712", "train_lr": "9.37846e-06", "train_gnorm": "2.775", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "684", "train_gb_free": "7.8", "train_wall": "15651"}
[2022-07-20 06:04:48,922][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 06:04:48,970][fairseq.trainer][INFO] - begin training epoch 21
[2022-07-20 06:04:48,971][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 06:16:32,392][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 06:17:38,010][valid][INFO] - {"epoch": 21, "valid_loss": "7.662", "valid_nll_loss": "6.265", "valid_ppl": "76.89", "valid_wps": "2332.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1798", "valid_best_loss": "7.662"}
[2022-07-20 06:17:38,013][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 1798 updates
[2022-07-20 06:17:38,015][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 06:17:44,281][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 06:17:48,416][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 21 @ 1798 updates, score 7.662) (writing took 10.403116607107222 seconds)
[2022-07-20 06:17:48,417][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2022-07-20 06:17:48,423][train][INFO] - {"epoch": 21, "train_loss": "7.706", "train_nll_loss": "6.387", "train_ppl": "83.68", "train_wps": "1377.3", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1798", "train_lr": "9.33436e-06", "train_gnorm": "2.687", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "683", "train_gb_free": "7.7", "train_wall": "16430"}
[2022-07-20 06:17:48,437][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 06:17:48,483][fairseq.trainer][INFO] - begin training epoch 22
[2022-07-20 06:17:48,485][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 06:17:57,214][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 06:18:13,570][train_inner][INFO] - {"epoch": 22, "update": 21.035, "loss": "7.738", "nll_loss": "6.424", "ppl": "85.84", "wps": "1328", "ups": "0.11", "wpb": "12425.8", "bsz": "253.7", "num_updates": "1800", "lr": "9.33333e-06", "gnorm": "2.721", "clip": "100", "loss_scale": "32", "train_wall": "1594", "gb_free": "7.2", "wall": "16455"}
[2022-07-20 06:29:32,071][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 06:30:37,528][valid][INFO] - {"epoch": 22, "valid_loss": "7.626", "valid_nll_loss": "6.228", "valid_ppl": "74.98", "valid_wps": "2338.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1883", "valid_best_loss": "7.626"}
[2022-07-20 06:30:37,532][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 1883 updates
[2022-07-20 06:30:37,535][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 06:30:43,815][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 06:30:47,924][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 22 @ 1883 updates, score 7.626) (writing took 10.391442943830043 seconds)
[2022-07-20 06:30:47,925][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2022-07-20 06:30:47,930][train][INFO] - {"epoch": 22, "train_loss": "7.66", "train_nll_loss": "6.333", "train_ppl": "80.6", "train_wps": "1361", "train_ups": "0.11", "train_wpb": "12480.8", "train_bsz": "254.2", "train_num_updates": "1883", "train_lr": "9.29077e-06", "train_gnorm": "2.783", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "683", "train_gb_free": "7.6", "train_wall": "17210"}
[2022-07-20 06:30:47,945][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 06:30:49,351][fairseq.trainer][INFO] - begin training epoch 23
[2022-07-20 06:30:49,353][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 06:42:32,388][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 06:43:37,891][valid][INFO] - {"epoch": 23, "valid_loss": "7.571", "valid_nll_loss": "6.182", "valid_ppl": "72.59", "valid_wps": "2336.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1969", "valid_best_loss": "7.571"}
[2022-07-20 06:43:37,895][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 1969 updates
[2022-07-20 06:43:37,896][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 06:43:44,218][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 06:43:48,282][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 23 @ 1969 updates, score 7.571) (writing took 10.387553338892758 seconds)
[2022-07-20 06:43:48,283][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2022-07-20 06:43:48,289][train][INFO] - {"epoch": 23, "train_loss": "7.613", "train_nll_loss": "6.278", "train_ppl": "77.62", "train_wps": "1375.8", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1969", "train_lr": "9.24667e-06", "train_gnorm": "2.608", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "682", "train_gb_free": "7.2", "train_wall": "17990"}
[2022-07-20 06:43:48,306][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 06:43:48,357][fairseq.trainer][INFO] - begin training epoch 24
[2022-07-20 06:43:48,358][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 06:48:03,353][train_inner][INFO] - {"epoch": 24, "update": 23.36, "loss": "7.625", "nll_loss": "6.293", "ppl": "78.4", "wps": "1399.6", "ups": "0.11", "wpb": "12524.5", "bsz": "254.4", "num_updates": "2000", "lr": "9.23077e-06", "gnorm": "2.707", "clip": "100", "loss_scale": "32", "train_wall": "1588", "gb_free": "6.9", "wall": "18245"}
[2022-07-20 06:54:40,029][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2022-07-20 06:55:32,116][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 06:56:37,494][valid][INFO] - {"epoch": 24, "valid_loss": "7.559", "valid_nll_loss": "6.158", "valid_ppl": "71.42", "valid_wps": "2341.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2054", "valid_best_loss": "7.559"}
[2022-07-20 06:56:37,499][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 2054 updates
[2022-07-20 06:56:37,502][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 06:56:43,858][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 06:56:47,882][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 24 @ 2054 updates, score 7.559) (writing took 10.383244195953012 seconds)
[2022-07-20 06:56:47,883][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2022-07-20 06:56:47,888][train][INFO] - {"epoch": 24, "train_loss": "7.568", "train_nll_loss": "6.227", "train_ppl": "74.9", "train_wps": "1359.9", "train_ups": "0.11", "train_wpb": "12472.6", "train_bsz": "254.2", "train_num_updates": "2054", "train_lr": "9.20308e-06", "train_gnorm": "2.829", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "683", "train_gb_free": "8.1", "train_wall": "18770"}
[2022-07-20 06:56:47,904][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 06:56:47,951][fairseq.trainer][INFO] - begin training epoch 25
[2022-07-20 06:56:47,952][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 07:08:31,580][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 07:09:37,074][valid][INFO] - {"epoch": 25, "valid_loss": "7.526", "valid_nll_loss": "6.112", "valid_ppl": "69.16", "valid_wps": "2336.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2140", "valid_best_loss": "7.526"}
[2022-07-20 07:09:37,077][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 2140 updates
[2022-07-20 07:09:37,079][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 07:09:43,370][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 07:09:47,791][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 25 @ 2140 updates, score 7.526) (writing took 10.713325249962509 seconds)
[2022-07-20 07:09:47,792][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2022-07-20 07:09:47,797][train][INFO] - {"epoch": 25, "train_loss": "7.53", "train_nll_loss": "6.183", "train_ppl": "72.64", "train_wps": "1376.6", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2140", "train_lr": "9.15897e-06", "train_gnorm": "2.73", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "683", "train_gb_free": "7.7", "train_wall": "19550"}
[2022-07-20 07:09:47,811][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 07:09:47,861][fairseq.trainer][INFO] - begin training epoch 26
[2022-07-20 07:09:47,862][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 07:18:03,088][train_inner][INFO] - {"epoch": 26, "update": 25.698, "loss": "7.523", "nll_loss": "6.174", "ppl": "72.21", "wps": "1388.3", "ups": "0.11", "wpb": "12493.1", "bsz": "254.4", "num_updates": "2200", "lr": "9.12821e-06", "gnorm": "2.739", "clip": "100", "loss_scale": "16", "train_wall": "1599", "gb_free": "8", "wall": "20045"}
[2022-07-20 07:21:30,561][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 07:22:36,027][valid][INFO] - {"epoch": 26, "valid_loss": "7.484", "valid_nll_loss": "6.078", "valid_ppl": "67.56", "valid_wps": "2336.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2226", "valid_best_loss": "7.484"}
[2022-07-20 07:22:36,030][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 2226 updates
[2022-07-20 07:22:36,032][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 07:22:42,369][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 07:22:46,468][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 26 @ 2226 updates, score 7.484) (writing took 10.43792916694656 seconds)
[2022-07-20 07:22:46,469][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2022-07-20 07:22:46,475][train][INFO] - {"epoch": 26, "train_loss": "7.491", "train_nll_loss": "6.137", "train_ppl": "70.37", "train_wps": "1378.8", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2226", "train_lr": "9.11487e-06", "train_gnorm": "2.688", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "682", "train_gb_free": "7.9", "train_wall": "20328"}
[2022-07-20 07:22:46,489][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 07:22:46,536][fairseq.trainer][INFO] - begin training epoch 27
[2022-07-20 07:22:46,537][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 07:34:29,194][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 07:35:34,583][valid][INFO] - {"epoch": 27, "valid_loss": "7.461", "valid_nll_loss": "6.048", "valid_ppl": "66.16", "valid_wps": "2339.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2312", "valid_best_loss": "7.461"}
[2022-07-20 07:35:34,588][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 2312 updates
[2022-07-20 07:35:34,591][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 07:35:40,940][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 07:35:44,988][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 27 @ 2312 updates, score 7.461) (writing took 10.399976432090625 seconds)
[2022-07-20 07:35:44,989][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2022-07-20 07:35:44,995][train][INFO] - {"epoch": 27, "train_loss": "7.454", "train_nll_loss": "6.095", "train_ppl": "68.35", "train_wps": "1379.1", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2312", "train_lr": "9.07077e-06", "train_gnorm": "2.753", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "682", "train_gb_free": "7.9", "train_wall": "21107"}
[2022-07-20 07:35:45,009][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 07:35:45,059][fairseq.trainer][INFO] - begin training epoch 28
[2022-07-20 07:35:45,061][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 07:47:28,351][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 07:48:33,838][valid][INFO] - {"epoch": 28, "valid_loss": "7.425", "valid_nll_loss": "6.014", "valid_ppl": "64.63", "valid_wps": "2337.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2398", "valid_best_loss": "7.425"}
[2022-07-20 07:48:33,841][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 2398 updates
[2022-07-20 07:48:33,843][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 07:48:40,164][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 07:48:44,332][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 28 @ 2398 updates, score 7.425) (writing took 10.49144308688119 seconds)
[2022-07-20 07:48:44,333][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2022-07-20 07:48:44,340][train][INFO] - {"epoch": 28, "train_loss": "7.419", "train_nll_loss": "6.054", "train_ppl": "66.42", "train_wps": "1377.6", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2398", "train_lr": "9.02667e-06", "train_gnorm": "2.664", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "682", "train_gb_free": "7.9", "train_wall": "21886"}
[2022-07-20 07:48:44,352][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 07:48:44,414][fairseq.trainer][INFO] - begin training epoch 29
[2022-07-20 07:48:44,415][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 07:49:01,037][train_inner][INFO] - {"epoch": 29, "update": 28.023, "loss": "7.449", "nll_loss": "6.089", "ppl": "68.07", "wps": "1338.9", "ups": "0.11", "wpb": "12437.7", "bsz": "253.7", "num_updates": "2400", "lr": "9.02564e-06", "gnorm": "2.718", "clip": "100", "loss_scale": "32", "train_wall": "1582", "gb_free": "7.6", "wall": "21903"}
[2022-07-20 08:00:27,539][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 08:01:33,195][valid][INFO] - {"epoch": 29, "valid_loss": "7.411", "valid_nll_loss": "5.988", "valid_ppl": "63.47", "valid_wps": "2331.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2484", "valid_best_loss": "7.411"}
[2022-07-20 08:01:33,199][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 2484 updates
[2022-07-20 08:01:33,203][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 08:01:39,527][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 08:01:43,597][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 29 @ 2484 updates, score 7.411) (writing took 10.397196820005774 seconds)
[2022-07-20 08:01:43,598][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2022-07-20 08:01:43,604][train][INFO] - {"epoch": 29, "train_loss": "7.386", "train_nll_loss": "6.016", "train_ppl": "64.69", "train_wps": "1377.8", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2484", "train_lr": "8.98256e-06", "train_gnorm": "2.74", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "683", "train_gb_free": "7.8", "train_wall": "22665"}
[2022-07-20 08:01:43,618][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 08:01:43,668][fairseq.trainer][INFO] - begin training epoch 30
[2022-07-20 08:01:43,669][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 08:12:10,013][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 08:13:27,735][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 08:14:33,323][valid][INFO] - {"epoch": 30, "valid_loss": "7.379", "valid_nll_loss": "5.952", "valid_ppl": "61.89", "valid_wps": "2333.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2569", "valid_best_loss": "7.379"}
[2022-07-20 08:14:33,326][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 2569 updates
[2022-07-20 08:14:33,328][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 08:14:39,682][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 08:14:43,775][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 30 @ 2569 updates, score 7.379) (writing took 10.448640026850626 seconds)
[2022-07-20 08:14:43,776][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2022-07-20 08:14:43,781][train][INFO] - {"epoch": 30, "train_loss": "7.355", "train_nll_loss": "5.98", "train_ppl": "63.12", "train_wps": "1359.2", "train_ups": "0.11", "train_wpb": "12475.4", "train_bsz": "254.2", "train_num_updates": "2569", "train_lr": "8.93897e-06", "train_gnorm": "2.797", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "683", "train_gb_free": "7.7", "train_wall": "23446"}
[2022-07-20 08:14:43,797][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 08:14:44,462][fairseq.trainer][INFO] - begin training epoch 31
[2022-07-20 08:14:44,464][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 08:19:01,438][train_inner][INFO] - {"epoch": 31, "update": 30.36, "loss": "7.362", "nll_loss": "5.988", "ppl": "63.48", "wps": "1387.7", "ups": "0.11", "wpb": "12491.6", "bsz": "254.4", "num_updates": "2600", "lr": "8.92308e-06", "gnorm": "2.757", "clip": "100", "loss_scale": "32", "train_wall": "1599", "gb_free": "7", "wall": "23703"}
[2022-07-20 08:26:35,327][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 08:27:40,732][valid][INFO] - {"epoch": 31, "valid_loss": "7.354", "valid_nll_loss": "5.923", "valid_ppl": "60.68", "valid_wps": "2339.6", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2655", "valid_best_loss": "7.354"}
[2022-07-20 08:27:40,736][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 2655 updates
[2022-07-20 08:27:40,740][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 08:27:47,071][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 08:27:51,170][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 31 @ 2655 updates, score 7.354) (writing took 10.434233244974166 seconds)
[2022-07-20 08:27:51,171][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2022-07-20 08:27:51,177][train][INFO] - {"epoch": 31, "train_loss": "7.323", "train_nll_loss": "5.943", "train_ppl": "61.52", "train_wps": "1363.5", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2655", "train_lr": "8.89487e-06", "train_gnorm": "2.708", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "690", "train_gb_free": "7.9", "train_wall": "24233"}
[2022-07-20 08:27:51,193][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 08:27:51,242][fairseq.trainer][INFO] - begin training epoch 32
[2022-07-20 08:27:51,243][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 08:39:45,440][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 08:40:50,849][valid][INFO] - {"epoch": 32, "valid_loss": "7.336", "valid_nll_loss": "5.896", "valid_ppl": "59.54", "valid_wps": "2341.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2741", "valid_best_loss": "7.336"}
[2022-07-20 08:40:50,852][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 2741 updates
[2022-07-20 08:40:50,854][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 08:40:57,180][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 08:41:01,307][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 32 @ 2741 updates, score 7.336) (writing took 10.454894246067852 seconds)
[2022-07-20 08:41:01,308][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2022-07-20 08:41:01,315][train][INFO] - {"epoch": 32, "train_loss": "7.293", "train_nll_loss": "5.908", "train_ppl": "60.03", "train_wps": "1358.8", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2741", "train_lr": "8.85077e-06", "train_gnorm": "2.719", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "693", "train_gb_free": "6.6", "train_wall": "25023"}
[2022-07-20 08:41:01,329][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 08:41:01,488][fairseq.trainer][INFO] - begin training epoch 33
[2022-07-20 08:41:01,489][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 08:49:14,735][train_inner][INFO] - {"epoch": 33, "update": 32.686, "loss": "7.295", "nll_loss": "5.91", "ppl": "60.14", "wps": "1379.2", "ups": "0.11", "wpb": "12504.1", "bsz": "254.4", "num_updates": "2800", "lr": "8.82051e-06", "gnorm": "2.739", "clip": "100", "loss_scale": "32", "train_wall": "1612", "gb_free": "6.6", "wall": "25517"}
[2022-07-20 08:51:28,418][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 08:52:55,130][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 08:54:00,648][valid][INFO] - {"epoch": 33, "valid_loss": "7.316", "valid_nll_loss": "5.88", "valid_ppl": "58.91", "valid_wps": "2338.4", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2826", "valid_best_loss": "7.316"}
[2022-07-20 08:54:00,651][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 2826 updates
[2022-07-20 08:54:00,653][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 08:54:07,903][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 08:54:12,028][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 33 @ 2826 updates, score 7.316) (writing took 11.376820471137762 seconds)
[2022-07-20 08:54:12,029][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2022-07-20 08:54:12,032][train][INFO] - {"epoch": 33, "train_loss": "7.267", "train_nll_loss": "5.877", "train_ppl": "58.78", "train_wps": "1341.9", "train_ups": "0.11", "train_wpb": "12482.6", "train_bsz": "254.2", "train_num_updates": "2826", "train_lr": "8.80718e-06", "train_gnorm": "2.802", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "692", "train_gb_free": "8", "train_wall": "25814"}
[2022-07-20 08:54:12,049][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 08:54:12,103][fairseq.trainer][INFO] - begin training epoch 34
[2022-07-20 08:54:12,105][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 09:06:06,352][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 09:07:11,869][valid][INFO] - {"epoch": 34, "valid_loss": "7.294", "valid_nll_loss": "5.854", "valid_ppl": "57.84", "valid_wps": "2337.4", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2912", "valid_best_loss": "7.294"}
[2022-07-20 09:07:11,873][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 2912 updates
[2022-07-20 09:07:11,874][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:07:18,239][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:07:22,504][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 34 @ 2912 updates, score 7.294) (writing took 10.631229948019609 seconds)
[2022-07-20 09:07:22,505][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2022-07-20 09:07:22,511][train][INFO] - {"epoch": 34, "train_loss": "7.238", "train_nll_loss": "5.844", "train_ppl": "57.46", "train_wps": "1358.2", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2912", "train_lr": "8.76308e-06", "train_gnorm": "2.66", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "693", "train_gb_free": "7.8", "train_wall": "26604"}
[2022-07-20 09:07:22,528][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 09:07:22,584][fairseq.trainer][INFO] - begin training epoch 35
[2022-07-20 09:07:22,585][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 09:19:16,938][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 09:20:22,216][valid][INFO] - {"epoch": 35, "valid_loss": "7.277", "valid_nll_loss": "5.822", "valid_ppl": "56.57", "valid_wps": "2347.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2998", "valid_best_loss": "7.277"}
[2022-07-20 09:20:22,221][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 2998 updates
[2022-07-20 09:20:22,224][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:20:28,525][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:20:32,565][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 35 @ 2998 updates, score 7.277) (writing took 10.343457642011344 seconds)
[2022-07-20 09:20:32,566][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2022-07-20 09:20:32,572][train][INFO] - {"epoch": 35, "train_loss": "7.213", "train_nll_loss": "5.815", "train_ppl": "56.3", "train_wps": "1358.9", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2998", "train_lr": "8.71897e-06", "train_gnorm": "2.738", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "693", "train_gb_free": "7.9", "train_wall": "27394"}
[2022-07-20 09:20:32,586][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 09:20:32,638][fairseq.trainer][INFO] - begin training epoch 36
[2022-07-20 09:20:32,640][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 09:20:50,355][train_inner][INFO] - {"epoch": 36, "update": 35.023, "loss": "7.227", "nll_loss": "5.831", "ppl": "56.92", "wps": "1315.5", "ups": "0.11", "wpb": "12468.4", "bsz": "253.7", "num_updates": "3000", "lr": "8.71795e-06", "gnorm": "2.717", "clip": "100", "loss_scale": "32", "train_wall": "1617", "gb_free": "7.5", "wall": "27412"}
[2022-07-20 09:30:51,507][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 09:32:26,877][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 09:33:32,120][valid][INFO] - {"epoch": 36, "valid_loss": "7.252", "valid_nll_loss": "5.804", "valid_ppl": "55.88", "valid_wps": "2346.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3083", "valid_best_loss": "7.252"}
[2022-07-20 09:33:32,125][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 3083 updates
[2022-07-20 09:33:32,127][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:33:38,382][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:33:42,410][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 36 @ 3083 updates, score 7.252) (writing took 10.285450652940199 seconds)
[2022-07-20 09:33:42,411][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2022-07-20 09:33:42,418][train][INFO] - {"epoch": 36, "train_loss": "7.186", "train_nll_loss": "5.783", "train_ppl": "55.08", "train_wps": "1342.1", "train_ups": "0.11", "train_wpb": "12470.8", "train_bsz": "254.2", "train_num_updates": "3083", "train_lr": "8.67538e-06", "train_gnorm": "2.676", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "693", "train_gb_free": "8.1", "train_wall": "28184"}
[2022-07-20 09:33:42,429][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 09:33:42,486][fairseq.trainer][INFO] - begin training epoch 37
[2022-07-20 09:33:42,487][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 09:45:33,094][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 09:46:38,559][valid][INFO] - {"epoch": 37, "valid_loss": "7.24", "valid_nll_loss": "5.787", "valid_ppl": "55.2", "valid_wps": "2338.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3169", "valid_best_loss": "7.24"}
[2022-07-20 09:46:38,564][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 3169 updates
[2022-07-20 09:46:38,567][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:46:45,065][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:46:49,233][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 37 @ 3169 updates, score 7.24) (writing took 10.669290171936154 seconds)
[2022-07-20 09:46:49,234][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2022-07-20 09:46:49,240][train][INFO] - {"epoch": 37, "train_loss": "7.16", "train_nll_loss": "5.753", "train_ppl": "53.93", "train_wps": "1364.5", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3169", "train_lr": "8.63128e-06", "train_gnorm": "2.749", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "690", "train_gb_free": "7.6", "train_wall": "28971"}
[2022-07-20 09:46:49,253][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 09:46:49,305][fairseq.trainer][INFO] - begin training epoch 38
[2022-07-20 09:46:49,306][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 09:51:05,949][train_inner][INFO] - {"epoch": 38, "update": 37.36, "loss": "7.173", "nll_loss": "5.768", "ppl": "54.51", "wps": "1377", "ups": "0.11", "wpb": "12500.2", "bsz": "254.4", "num_updates": "3200", "lr": "8.61538e-06", "gnorm": "2.715", "clip": "100", "loss_scale": "32", "train_wall": "1615", "gb_free": "6.9", "wall": "29228"}
[2022-07-20 09:58:34,172][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 09:59:40,003][valid][INFO] - {"epoch": 38, "valid_loss": "7.223", "valid_nll_loss": "5.761", "valid_ppl": "54.21", "valid_wps": "2330.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3255", "valid_best_loss": "7.223"}
[2022-07-20 09:59:40,006][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 3255 updates
[2022-07-20 09:59:40,008][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:59:46,399][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:59:50,477][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 38 @ 3255 updates, score 7.223) (writing took 10.470309657044709 seconds)
[2022-07-20 09:59:50,477][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2022-07-20 09:59:50,483][train][INFO] - {"epoch": 38, "train_loss": "7.138", "train_nll_loss": "5.728", "train_ppl": "53", "train_wps": "1374.3", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3255", "train_lr": "8.58718e-06", "train_gnorm": "2.704", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "684", "train_gb_free": "7.4", "train_wall": "29752"}
[2022-07-20 09:59:50,498][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 09:59:50,578][fairseq.trainer][INFO] - begin training epoch 39
[2022-07-20 09:59:50,579][fairseq_cli.train][INFO] - Start iterating over samples
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-20 10:09:56,420][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 10:11:47,293][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 10:13:55,861][valid][INFO] - {"epoch": 39, "valid_loss": "7.219", "valid_nll_loss": "5.754", "valid_ppl": "53.98", "valid_wps": "1191.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3340", "valid_best_loss": "7.219"}
[2022-07-20 10:13:55,867][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 3340 updates
[2022-07-20 10:13:55,869][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:14:05,476][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:14:11,942][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 39 @ 3340 updates, score 7.219) (writing took 16.07504079095088 seconds)
[2022-07-20 10:14:11,942][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2022-07-20 10:14:11,946][train][INFO] - {"epoch": 39, "train_loss": "7.117", "train_nll_loss": "5.704", "train_ppl": "52.13", "train_wps": "1232.4", "train_ups": "0.1", "train_wpb": "12490.4", "train_bsz": "254.2", "train_num_updates": "3340", "train_lr": "8.54359e-06", "train_gnorm": "2.727", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "695", "train_gb_free": "7.7", "train_wall": "30614"}
[2022-07-20 10:14:11,960][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 10:14:12,047][fairseq.trainer][INFO] - begin training epoch 40
[2022-07-20 10:14:12,070][fairseq_cli.train][INFO] - Start iterating over samples
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-20 10:23:30,215][train_inner][INFO] - {"epoch": 40, "update": 39.698, "loss": "7.109", "nll_loss": "5.694", "ppl": "51.77", "wps": "1284.1", "ups": "0.1", "wpb": "12483.5", "bsz": "254.4", "num_updates": "3400", "lr": "8.51282e-06", "gnorm": "2.716", "clip": "100", "loss_scale": "32", "train_wall": "1668", "gb_free": "7.4", "wall": "31172"}
[2022-07-20 10:27:16,657][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 10:28:41,783][valid][INFO] - {"epoch": 40, "valid_loss": "7.185", "valid_nll_loss": "5.736", "valid_ppl": "53.3", "valid_wps": "1798.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3426", "valid_best_loss": "7.185"}
[2022-07-20 10:28:41,799][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 3426 updates
[2022-07-20 10:28:41,801][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:28:48,204][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:28:52,556][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 40 @ 3426 updates, score 7.185) (writing took 10.757261842023581 seconds)
[2022-07-20 10:28:52,557][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2022-07-20 10:28:52,569][train][INFO] - {"epoch": 40, "train_loss": "7.091", "train_nll_loss": "5.674", "train_ppl": "51.05", "train_wps": "1219.2", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3426", "train_lr": "8.49949e-06", "train_gnorm": "2.739", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "757", "train_gb_free": "8", "train_wall": "31494"}
[2022-07-20 10:28:52,582][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 10:28:52,643][fairseq.trainer][INFO] - begin training epoch 41
[2022-07-20 10:28:52,645][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 10:41:52,491][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 10:43:02,654][valid][INFO] - {"epoch": 41, "valid_loss": "7.177", "valid_nll_loss": "5.72", "valid_ppl": "52.7", "valid_wps": "2185", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3512", "valid_best_loss": "7.177"}
[2022-07-20 10:43:02,671][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 3512 updates
[2022-07-20 10:43:02,672][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:43:09,859][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:43:15,520][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 41 @ 3512 updates, score 7.177) (writing took 12.849330910015851 seconds)
[2022-07-20 10:43:15,521][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2022-07-20 10:43:15,523][train][INFO] - {"epoch": 41, "train_loss": "7.071", "train_nll_loss": "5.65", "train_ppl": "50.21", "train_wps": "1244.1", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3512", "train_lr": "8.45538e-06", "train_gnorm": "2.724", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "752", "train_gb_free": "7.9", "train_wall": "32357"}
[2022-07-20 10:43:15,536][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 10:43:15,598][fairseq.trainer][INFO] - begin training epoch 42
[2022-07-20 10:43:15,600][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 10:54:59,881][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 10:56:44,814][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 10:57:58,233][valid][INFO] - {"epoch": 42, "valid_loss": "7.164", "valid_nll_loss": "5.699", "valid_ppl": "51.96", "valid_wps": "2084.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3597", "valid_best_loss": "7.164"}
[2022-07-20 10:57:58,236][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 3597 updates
[2022-07-20 10:57:58,238][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:58:04,815][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:58:10,335][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 42 @ 3597 updates, score 7.164) (writing took 12.098791409051046 seconds)
[2022-07-20 10:58:10,335][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2022-07-20 10:58:10,338][train][INFO] - {"epoch": 42, "train_loss": "7.052", "train_nll_loss": "5.628", "train_ppl": "49.45", "train_wps": "1186.6", "train_ups": "0.09", "train_wpb": "12491.2", "train_bsz": "254.2", "train_num_updates": "3597", "train_lr": "8.41179e-06", "train_gnorm": "2.715", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "782", "train_gb_free": "7.9", "train_wall": "33252"}
[2022-07-20 10:58:10,352][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 10:58:10,408][fairseq.trainer][INFO] - begin training epoch 43
[2022-07-20 10:58:10,410][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 10:58:35,948][train_inner][INFO] - {"epoch": 43, "update": 42.035, "loss": "7.067", "nll_loss": "5.645", "ppl": "50.05", "wps": "1183.1", "ups": "0.09", "wpb": "12456.3", "bsz": "253.7", "num_updates": "3600", "lr": "8.41026e-06", "gnorm": "2.724", "clip": "100", "loss_scale": "32", "train_wall": "1777", "gb_free": "7", "wall": "33278"}
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-20 11:11:01,323][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 11:12:12,909][valid][INFO] - {"epoch": 43, "valid_loss": "7.142", "valid_nll_loss": "5.678", "valid_ppl": "51.19", "valid_wps": "2141.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3683", "valid_best_loss": "7.142"}
[2022-07-20 11:12:12,912][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 3683 updates
[2022-07-20 11:12:12,914][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:12:20,570][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:12:26,508][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 43 @ 3683 updates, score 7.142) (writing took 13.596350317122415 seconds)
[2022-07-20 11:12:26,509][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2022-07-20 11:12:26,513][train][INFO] - {"epoch": 43, "train_loss": "7.028", "train_nll_loss": "5.6", "train_ppl": "48.51", "train_wps": "1254", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3683", "train_lr": "8.36769e-06", "train_gnorm": "2.669", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "744", "train_gb_free": "7.5", "train_wall": "34108"}
[2022-07-20 11:12:26,527][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 11:12:26,632][fairseq.trainer][INFO] - begin training epoch 44
[2022-07-20 11:12:26,635][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 11:25:21,052][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 11:26:36,433][valid][INFO] - {"epoch": 44, "valid_loss": "7.143", "valid_nll_loss": "5.664", "valid_ppl": "50.71", "valid_wps": "2032.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3769", "valid_best_loss": "7.142"}
[2022-07-20 11:26:36,436][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 3769 updates
[2022-07-20 11:26:36,438][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_last.pt
[2022-07-20 11:26:44,342][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_last.pt
[2022-07-20 11:26:44,387][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 44 @ 3769 updates, score 7.143) (writing took 7.950579690048471 seconds)
[2022-07-20 11:26:44,387][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2022-07-20 11:26:44,406][train][INFO] - {"epoch": 44, "train_loss": "7.008", "train_nll_loss": "5.578", "train_ppl": "47.77", "train_wps": "1251.5", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3769", "train_lr": "8.32359e-06", "train_gnorm": "2.657", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "747", "train_gb_free": "7.8", "train_wall": "34966"}
[2022-07-20 11:26:44,432][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 11:26:44,517][fairseq.trainer][INFO] - begin training epoch 45
[2022-07-20 11:26:44,518][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 11:31:29,905][train_inner][INFO] - {"epoch": 45, "update": 44.36, "loss": "7.011", "nll_loss": "5.581", "ppl": "47.88", "wps": "1268.8", "ups": "0.1", "wpb": "12522.4", "bsz": "254.4", "num_updates": "3800", "lr": "8.30769e-06", "gnorm": "2.673", "clip": "100", "loss_scale": "32", "train_wall": "1741", "gb_free": "7.9", "wall": "35252"}
[2022-07-20 11:39:20,802][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 11:39:41,290][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 11:41:02,679][valid][INFO] - {"epoch": 45, "valid_loss": "7.118", "valid_nll_loss": "5.646", "valid_ppl": "50.06", "valid_wps": "1879.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3854", "valid_best_loss": "7.118"}
[2022-07-20 11:41:02,683][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 45 @ 3854 updates
[2022-07-20 11:41:02,684][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:41:09,619][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:41:14,954][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 45 @ 3854 updates, score 7.118) (writing took 12.271290031028911 seconds)
[2022-07-20 11:41:14,954][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2022-07-20 11:41:14,957][train][INFO] - {"epoch": 45, "train_loss": "6.987", "train_nll_loss": "5.553", "train_ppl": "46.95", "train_wps": "1219.6", "train_ups": "0.1", "train_wpb": "12490.4", "train_bsz": "254.2", "train_num_updates": "3854", "train_lr": "8.28e-06", "train_gnorm": "2.714", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "749", "train_gb_free": "7.3", "train_wall": "35837"}
[2022-07-20 11:41:14,985][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 11:41:15,059][fairseq.trainer][INFO] - begin training epoch 46
[2022-07-20 11:41:15,068][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 11:53:59,023][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 11:55:14,614][valid][INFO] - {"epoch": 46, "valid_loss": "7.111", "valid_nll_loss": "5.637", "valid_ppl": "49.76", "valid_wps": "2026.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3940", "valid_best_loss": "7.111"}
[2022-07-20 11:55:14,618][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 3940 updates
[2022-07-20 11:55:14,620][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:55:22,660][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:55:28,402][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 46 @ 3940 updates, score 7.111) (writing took 13.784096280112863 seconds)
[2022-07-20 11:55:28,403][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2022-07-20 11:55:28,406][train][INFO] - {"epoch": 46, "train_loss": "6.971", "train_nll_loss": "5.534", "train_ppl": "46.33", "train_wps": "1258", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3940", "train_lr": "8.2359e-06", "train_gnorm": "2.73", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "737", "train_gb_free": "7.2", "train_wall": "36690"}
[2022-07-20 11:55:28,420][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 11:55:28,480][fairseq.trainer][INFO] - begin training epoch 47
[2022-07-20 11:55:28,483][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 12:04:40,212][train_inner][INFO] - {"epoch": 47, "update": 46.698, "loss": "6.973", "nll_loss": "5.537", "ppl": "46.43", "wps": "1257.4", "ups": "0.1", "wpb": "12512.9", "bsz": "254.4", "num_updates": "4000", "lr": "8.20513e-06", "gnorm": "2.691", "clip": "100", "loss_scale": "32", "train_wall": "1743", "gb_free": "7.8", "wall": "37242"}
[2022-07-20 12:08:34,184][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 12:09:55,686][valid][INFO] - {"epoch": 47, "valid_loss": "7.104", "valid_nll_loss": "5.621", "valid_ppl": "49.22", "valid_wps": "1878.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "4026", "valid_best_loss": "7.104"}
[2022-07-20 12:09:55,693][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 47 @ 4026 updates
[2022-07-20 12:09:55,703][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:10:02,775][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:10:07,938][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 47 @ 4026 updates, score 7.104) (writing took 12.24459070712328 seconds)
[2022-07-20 12:10:07,939][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2022-07-20 12:10:07,949][train][INFO] - {"epoch": 47, "train_loss": "6.951", "train_nll_loss": "5.511", "train_ppl": "45.61", "train_wps": "1220.7", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "4026", "train_lr": "8.19179e-06", "train_gnorm": "2.667", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "759", "train_gb_free": "7.3", "train_wall": "37570"}
[2022-07-20 12:10:07,961][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 12:10:08,026][fairseq.trainer][INFO] - begin training epoch 48
[2022-07-20 12:10:08,027][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 12:22:52,257][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 12:23:23,028][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 12:24:42,532][valid][INFO] - {"epoch": 48, "valid_loss": "7.077", "valid_nll_loss": "5.602", "valid_ppl": "48.58", "valid_wps": "1927", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "4111", "valid_best_loss": "7.077"}
[2022-07-20 12:24:42,536][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 4111 updates
[2022-07-20 12:24:42,537][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:24:49,374][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:24:55,984][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 48 @ 4111 updates, score 7.077) (writing took 13.447917556855828 seconds)
[2022-07-20 12:24:55,984][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2022-07-20 12:24:55,987][train][INFO] - {"epoch": 48, "train_loss": "6.935", "train_nll_loss": "5.493", "train_ppl": "45.02", "train_wps": "1195.1", "train_ups": "0.1", "train_wpb": "12485.3", "train_bsz": "254.2", "train_num_updates": "4111", "train_lr": "8.14821e-06", "train_gnorm": "2.721", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "767", "train_gb_free": "7.9", "train_wall": "38458"}
[2022-07-20 12:24:55,999][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 12:24:56,106][fairseq.trainer][INFO] - begin training epoch 49
[2022-07-20 12:24:56,107][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 12:38:27,551][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 12:39:47,471][valid][INFO] - {"epoch": 49, "valid_loss": "7.081", "valid_nll_loss": "5.597", "valid_ppl": "48.4", "valid_wps": "1917", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "4197", "valid_best_loss": "7.077"}
[2022-07-20 12:39:47,474][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 49 @ 4197 updates
[2022-07-20 12:39:47,476][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_last.pt
[2022-07-20 12:39:55,120][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_last.pt
[2022-07-20 12:39:55,147][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 49 @ 4197 updates, score 7.081) (writing took 7.672966020181775 seconds)
[2022-07-20 12:39:55,148][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2022-07-20 12:39:55,157][train][INFO] - {"epoch": 49, "train_loss": "6.917", "train_nll_loss": "5.472", "train_ppl": "44.37", "train_wps": "1194", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "4197", "train_lr": "8.1041e-06", "train_gnorm": "2.714", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "783", "train_gb_free": "7.7", "train_wall": "39357"}
[2022-07-20 12:39:55,169][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 12:39:55,231][fairseq.trainer][INFO] - begin training epoch 50
[2022-07-20 12:39:55,233][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 12:40:20,883][train_inner][INFO] - {"epoch": 50, "update": 49.035, "loss": "6.927", "nll_loss": "5.484", "ppl": "44.75", "wps": "1160.6", "ups": "0.09", "wpb": "12422.3", "bsz": "253.7", "num_updates": "4200", "lr": "8.10256e-06", "gnorm": "2.723", "clip": "100", "loss_scale": "32", "train_wall": "1801", "gb_free": "6.5", "wall": "39383"}
[2022-07-20 12:53:11,022][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 12:54:25,883][valid][INFO] - {"epoch": 50, "valid_loss": "7.061", "valid_nll_loss": "5.577", "valid_ppl": "47.75", "valid_wps": "2046.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "4283", "valid_best_loss": "7.061"}
[2022-07-20 12:54:25,886][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 4283 updates
[2022-07-20 12:54:25,888][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:54:32,502][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/01-43-38/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:54:37,706][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 50 @ 4283 updates, score 7.061) (writing took 11.819963048910722 seconds)
[2022-07-20 12:54:37,707][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2022-07-20 12:54:37,710][train][INFO] - {"epoch": 50, "train_loss": "6.899", "train_nll_loss": "5.451", "train_ppl": "43.74", "train_wps": "1216.5", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "4283", "train_lr": "8.06e-06", "train_gnorm": "2.657", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "767", "train_gb_free": "7.9", "train_wall": "40240"}
[2022-07-20 12:54:37,720][fairseq_cli.train][INFO] - done training in 40239.5 seconds
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: \ 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: | 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: / 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: - 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: \ 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: | 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: / 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: - 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: \ 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:              train/bsz 254.2
wandb:             train/clip 100.0
wandb:          train/gb_free 7.9
wandb:            train/gnorm 2.657
wandb:             train/loss 6.899
wandb:       train/loss_scale 32.0
wandb:               train/lr 1e-05
wandb:         train/nll_loss 5.451
wandb:              train/ppl 43.74
wandb:       train/train_wall 767.0
wandb:              train/ups 0.1
wandb:             train/wall 40240.0
wandb:              train/wpb 12484.1
wandb:              train/wps 1216.5
wandb:        train_inner/bsz 253.7
wandb:       train_inner/clip 100.0
wandb:    train_inner/gb_free 6.5
wandb:      train_inner/gnorm 2.723
wandb:       train_inner/loss 6.927
wandb: train_inner/loss_scale 32.0
wandb:         train_inner/lr 1e-05
wandb:   train_inner/nll_loss 5.484
wandb:        train_inner/ppl 44.75
wandb: train_inner/train_wall 1801.0
wandb:        train_inner/ups 0.09
wandb:       train_inner/wall 39383.0
wandb:        train_inner/wpb 12422.3
wandb:        train_inner/wps 1160.6
wandb:        valid/best_loss 7.061
wandb:              valid/bsz 4.0
wandb:             valid/loss 7.061
wandb:         valid/nll_loss 5.577
wandb:              valid/ppl 47.75
wandb:              valid/wpb 195.7
wandb:              valid/wps 2046.5
wandb: 
wandb: Synced checkpoints: https://wandb.ai/redredouane/RoBERTa_encdec/runs/20t1e08b
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./multirun/2022-07-20/01-43-38/0/wandb/run-20220720_014403-20t1e08b/logs
