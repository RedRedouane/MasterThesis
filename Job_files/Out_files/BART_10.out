2022-06-27 20:04:13 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:18746
2022-06-27 20:04:13 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:18746
2022-06-27 20:04:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-06-27 20:04:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-06-27 20:04:13 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2022-06-27 20:04:13 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2022-06-27 20:04:13 | INFO | fairseq.distributed.utils | initialized host r32n5.lisa.surfsara.nl as rank 1
2022-06-27 20:04:13 | INFO | fairseq.distributed.utils | initialized host r32n5.lisa.surfsara.nl as rank 0
2022-06-27 20:04:17 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'BART', 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18746', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3200, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3200, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': '/home/dahmanir/lisa/Models/bart.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=200, log_format='json', log_file=None, tensorboard_logdir=None, wandb_project='BART', azureml_logging=False, seed=4, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', simul_type=None, scoring='bleu', task='denoising', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=3200, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3200, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=2, distributed_num_procs=2, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=2, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='bart_base', max_epoch=0, max_update=500000, stop_time_hours=0, clip_norm=0.1, sentence_avg=False, update_freq=[32], lr=[0.0004], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints', restore_file='/home/dahmanir/lisa/Models/bart.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/home/dahmanir/lisa/Datasets/10_percent', tokens_per_sample=512, sample_break_mode='complete_doc', mask=0.3, mask_random=0.1, insert=0.0, permute=0.0, rotate=0.0, poisson_lambda=3.5, permute_sentences=1.0, mask_length='span-poisson', replace_length=1, shorten_method='none', shorten_data_split_list='', adam_betas=(0.9, 0.999), adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=500, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='500000', pad=1, eos=2, unk=3, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layers=6, encoder_attention_heads=12, decoder_layers=6, decoder_attention_heads=12, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=True, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=3072, decoder_normalize_before=False, decoder_learned_pos=True, relu_dropout=0.0, max_target_positions=1024, max_source_positions=1024, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=True, share_all_embeddings=True, decoder_output_dim=768, decoder_input_dim=768, no_scale_embedding=True, layernorm_embedding=True, activation_fn='gelu', pooler_activation_fn='tanh', pooler_dropout=0.0, _name='bart_base'), 'task': Namespace(no_progress_bar=False, log_interval=200, log_format='json', log_file=None, tensorboard_logdir=None, wandb_project='BART', azureml_logging=False, seed=4, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', simul_type=None, scoring='bleu', task='denoising', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=3200, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3200, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=2, distributed_num_procs=2, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=2, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='bart_base', max_epoch=0, max_update=500000, stop_time_hours=0, clip_norm=0.1, sentence_avg=False, update_freq=[32], lr=[0.0004], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints', restore_file='/home/dahmanir/lisa/Models/bart.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/home/dahmanir/lisa/Datasets/10_percent', tokens_per_sample=512, sample_break_mode='complete_doc', mask=0.3, mask_random=0.1, insert=0.0, permute=0.0, rotate=0.0, poisson_lambda=3.5, permute_sentences=1.0, mask_length='span-poisson', replace_length=1, shorten_method='none', shorten_data_split_list='', adam_betas=(0.9, 0.999), adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=500, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='500000', pad=1, eos=2, unk=3, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layers=6, encoder_attention_heads=12, decoder_layers=6, decoder_attention_heads=12, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=True, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=3072, decoder_normalize_before=False, decoder_learned_pos=True, relu_dropout=0.0, max_target_positions=1024, max_source_positions=1024, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=True, share_all_embeddings=True, decoder_output_dim=768, decoder_input_dim=768, no_scale_embedding=True, layernorm_embedding=True, activation_fn='gelu', pooler_activation_fn='tanh', pooler_dropout=0.0, _name='denoising'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-06-27 20:04:17 | INFO | fairseq.tasks.denoising | dictionary: 39984 types
encoder.embed_tokens.weight True torch.Size([39985, 768])
encoder.embed_positions.weight False torch.Size([1026, 768])
encoder.layernorm_embedding.weight False torch.Size([768])
encoder.layernorm_embedding.bias False torch.Size([768])
encoder.layers.0.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.0.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.0.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.0.fc1.weight False torch.Size([3072, 768])
encoder.layers.0.fc1.bias False torch.Size([3072])
encoder.layers.0.fc2.weight False torch.Size([768, 3072])
encoder.layers.0.fc2.bias False torch.Size([768])
encoder.layers.0.final_layer_norm.weight False torch.Size([768])
encoder.layers.0.final_layer_norm.bias False torch.Size([768])
encoder.layers.1.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.1.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.1.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.1.fc1.weight False torch.Size([3072, 768])
encoder.layers.1.fc1.bias False torch.Size([3072])
encoder.layers.1.fc2.weight False torch.Size([768, 3072])
encoder.layers.1.fc2.bias False torch.Size([768])
encoder.layers.1.final_layer_norm.weight False torch.Size([768])
encoder.layers.1.final_layer_norm.bias False torch.Size([768])
encoder.layers.2.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.2.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.2.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.2.fc1.weight False torch.Size([3072, 768])
encoder.layers.2.fc1.bias False torch.Size([3072])
encoder.layers.2.fc2.weight False torch.Size([768, 3072])
encoder.layers.2.fc2.bias False torch.Size([768])
encoder.layers.2.final_layer_norm.weight False torch.Size([768])
encoder.layers.2.final_layer_norm.bias False torch.Size([768])
encoder.layers.3.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.3.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.3.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.3.fc1.weight False torch.Size([3072, 768])
encoder.layers.3.fc1.bias False torch.Size([3072])
encoder.layers.3.fc2.weight False torch.Size([768, 3072])
encoder.layers.3.fc2.bias False torch.Size([768])
encoder.layers.3.final_layer_norm.weight False torch.Size([768])
encoder.layers.3.final_layer_norm.bias False torch.Size([768])
encoder.layers.4.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.4.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.4.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.4.fc1.weight False torch.Size([3072, 768])
encoder.layers.4.fc1.bias False torch.Size([3072])
encoder.layers.4.fc2.weight False torch.Size([768, 3072])
encoder.layers.4.fc2.bias False torch.Size([768])
encoder.layers.4.final_layer_norm.weight False torch.Size([768])
encoder.layers.4.final_layer_norm.bias False torch.Size([768])
encoder.layers.5.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.5.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.5.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.5.fc1.weight False torch.Size([3072, 768])
encoder.layers.5.fc1.bias False torch.Size([3072])
encoder.layers.5.fc2.weight False torch.Size([768, 3072])
encoder.layers.5.fc2.bias False torch.Size([768])
encoder.layers.5.final_layer_norm.weight False torch.Size([768])
encoder.layers.5.final_layer_norm.bias False torch.Size([768])
decoder.embed_positions.weight False torch.Size([1026, 768])
decoder.layernorm_embedding.weight False torch.Size([768])
decoder.layernorm_embedding.bias False torch.Size([768])
decoder.layers.0.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.0.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.0.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.0.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.0.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.0.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.0.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.0.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.0.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.0.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.0.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.0.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.0.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.0.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.0.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.0.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.0.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.0.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.0.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.0.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.0.fc1.weight False torch.Size([3072, 768])
decoder.layers.0.fc1.bias False torch.Size([3072])
decoder.layers.0.fc2.weight False torch.Size([768, 3072])
decoder.layers.0.fc2.bias False torch.Size([768])
decoder.layers.0.final_layer_norm.weight False torch.Size([768])
decoder.layers.0.final_layer_norm.bias False torch.Size([768])
decoder.layers.1.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.1.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.1.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.1.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.1.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.1.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.1.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.1.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.1.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.1.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.1.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.1.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.1.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.1.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.1.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.1.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.1.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.1.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.1.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.1.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.1.fc1.weight False torch.Size([3072, 768])
decoder.layers.1.fc1.bias False torch.Size([3072])
decoder.layers.1.fc2.weight False torch.Size([768, 3072])
decoder.layers.1.fc2.bias False torch.Size([768])
decoder.layers.1.final_layer_norm.weight False torch.Size([768])
decoder.layers.1.final_layer_norm.bias False torch.Size([768])
decoder.layers.2.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.2.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.2.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.2.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.2.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.2.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.2.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.2.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.2.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.2.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.2.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.2.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.2.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.2.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.2.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.2.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.2.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.2.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.2.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.2.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.2.fc1.weight False torch.Size([3072, 768])
decoder.layers.2.fc1.bias False torch.Size([3072])
decoder.layers.2.fc2.weight False torch.Size([768, 3072])
decoder.layers.2.fc2.bias False torch.Size([768])
decoder.layers.2.final_layer_norm.weight False torch.Size([768])
decoder.layers.2.final_layer_norm.bias False torch.Size([768])
decoder.layers.3.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.3.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.3.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.3.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.3.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.3.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.3.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.3.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.3.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.3.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.3.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.3.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.3.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.3.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.3.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.3.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.3.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.3.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.3.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.3.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.3.fc1.weight False torch.Size([3072, 768])
decoder.layers.3.fc1.bias False torch.Size([3072])
decoder.layers.3.fc2.weight False torch.Size([768, 3072])
decoder.layers.3.fc2.bias False torch.Size([768])
decoder.layers.3.final_layer_norm.weight False torch.Size([768])
decoder.layers.3.final_layer_norm.bias False torch.Size([768])
decoder.layers.4.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.4.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.4.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.4.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.4.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.4.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.4.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.4.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.4.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.4.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.4.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.4.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.4.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.4.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.4.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.4.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.4.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.4.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.4.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.4.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.4.fc1.weight False torch.Size([3072, 768])
decoder.layers.4.fc1.bias False torch.Size([3072])
decoder.layers.4.fc2.weight False torch.Size([768, 3072])
decoder.layers.4.fc2.bias False torch.Size([768])
decoder.layers.4.final_layer_norm.weight False torch.Size([768])
decoder.layers.4.final_layer_norm.bias False torch.Size([768])
decoder.layers.5.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.5.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.5.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.5.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.5.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.5.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.5.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.5.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.5.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.5.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.5.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.5.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.5.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.5.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.5.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.5.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.5.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.5.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.5.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.5.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.5.fc1.weight False torch.Size([3072, 768])
decoder.layers.5.fc1.bias False torch.Size([3072])
decoder.layers.5.fc2.weight False torch.Size([768, 3072])
decoder.layers.5.fc2.bias False torch.Size([768])
decoder.layers.5.final_layer_norm.weight False torch.Size([768])
decoder.layers.5.final_layer_norm.bias False torch.Size([768])
2022-06-27 20:04:22 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39985, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39985, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=39985, bias=False)
  )
  (classification_heads): ModuleDict()
)
2022-06-27 20:04:22 | INFO | fairseq_cli.train | task: DenoisingTask
2022-06-27 20:04:22 | INFO | fairseq_cli.train | model: BARTModel
2022-06-27 20:04:22 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-06-27 20:04:22 | INFO | fairseq_cli.train | num. shared model params: 131,525,376 (num. trained: 30,708,480)
2022-06-27 20:04:22 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-06-27 20:04:22 | INFO | fairseq.data.data_utils | loaded 10,000 examples from: /home/dahmanir/lisa/Datasets/10_percent/valid
2022-06-27 20:04:22 | INFO | fairseq.tasks.denoising | loaded 1445 blocks from: /home/dahmanir/lisa/Datasets/10_percent/valid
2022-06-27 20:04:22 | INFO | fairseq.tasks.denoising | Split: valid, Loaded 1445 samples of denoising_dataset
2022-06-27 20:04:26 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2022-06-27 20:04:26 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.
2022-06-27 20:04:26 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-06-27 20:04:26 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-06-27 20:04:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2022-06-27 20:04:26 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = NVIDIA GeForce GTX 1080 Ti              
2022-06-27 20:04:26 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = NVIDIA GeForce GTX 1080 Ti              
2022-06-27 20:04:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2022-06-27 20:04:26 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)
2022-06-27 20:04:26 | INFO | fairseq_cli.train | max tokens per device = 3200 and max sentences per device = None
2022-06-27 20:04:26 | INFO | fairseq.trainer | Preparing to load checkpoint /home/dahmanir/lisa/Models/bart.pt
2022-06-27 20:04:32 | INFO | fairseq.trainer | NOTE: your device does NOT support faster training with --fp16 or --amp, please switch to FP32 which is likely to be faster
2022-06-27 20:04:32 | INFO | fairseq.trainer | Loaded checkpoint /home/dahmanir/lisa/Models/bart.pt (epoch 14 @ 0 updates)
2022-06-27 20:04:32 | INFO | fairseq.trainer | loading train data for epoch 14
2022-06-27 20:04:34 | INFO | fairseq.data.data_utils | loaded 12,554,555 examples from: /home/dahmanir/lisa/Datasets/10_percent/train
2022-06-27 20:04:35 | INFO | fairseq.tasks.denoising | loaded 1788013 blocks from: /home/dahmanir/lisa/Datasets/10_percent/train
2022-06-27 20:04:35 | INFO | fairseq.tasks.denoising | Split: train, Loaded 1788013 samples of denoising_dataset
encoder.embed_tokens.weight True torch.Size([39985, 768])
encoder.embed_positions.weight False torch.Size([1026, 768])
encoder.layernorm_embedding.weight False torch.Size([768])
encoder.layernorm_embedding.bias False torch.Size([768])
encoder.layers.0.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.0.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.0.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.0.fc1.weight False torch.Size([3072, 768])
encoder.layers.0.fc1.bias False torch.Size([3072])
encoder.layers.0.fc2.weight False torch.Size([768, 3072])
encoder.layers.0.fc2.bias False torch.Size([768])
encoder.layers.0.final_layer_norm.weight False torch.Size([768])
encoder.layers.0.final_layer_norm.bias False torch.Size([768])
encoder.layers.1.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.1.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.1.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.1.fc1.weight False torch.Size([3072, 768])
encoder.layers.1.fc1.bias False torch.Size([3072])
encoder.layers.1.fc2.weight False torch.Size([768, 3072])
encoder.layers.1.fc2.bias False torch.Size([768])
encoder.layers.1.final_layer_norm.weight False torch.Size([768])
encoder.layers.1.final_layer_norm.bias False torch.Size([768])
encoder.layers.2.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.2.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.2.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.2.fc1.weight False torch.Size([3072, 768])
encoder.layers.2.fc1.bias False torch.Size([3072])
encoder.layers.2.fc2.weight False torch.Size([768, 3072])
encoder.layers.2.fc2.bias False torch.Size([768])
encoder.layers.2.final_layer_norm.weight False torch.Size([768])
encoder.layers.2.final_layer_norm.bias False torch.Size([768])
encoder.layers.3.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.3.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.3.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.3.fc1.weight False torch.Size([3072, 768])
encoder.layers.3.fc1.bias False torch.Size([3072])
encoder.layers.3.fc2.weight False torch.Size([768, 3072])
encoder.layers.3.fc2.bias False torch.Size([768])
encoder.layers.3.final_layer_norm.weight False torch.Size([768])
encoder.layers.3.final_layer_norm.bias False torch.Size([768])
encoder.layers.4.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.4.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.4.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.4.fc1.weight False torch.Size([3072, 768])
encoder.layers.4.fc1.bias False torch.Size([3072])
encoder.layers.4.fc2.weight False torch.Size([768, 3072])
encoder.layers.4.fc2.bias False torch.Size([768])
encoder.layers.4.final_layer_norm.weight False torch.Size([768])
encoder.layers.4.final_layer_norm.bias False torch.Size([768])
encoder.layers.5.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.5.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.5.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.5.fc1.weight False torch.Size([3072, 768])
encoder.layers.5.fc1.bias False torch.Size([3072])
encoder.layers.5.fc2.weight False torch.Size([768, 3072])
encoder.layers.5.fc2.bias False torch.Size([768])
encoder.layers.5.final_layer_norm.weight False torch.Size([768])
encoder.layers.5.final_layer_norm.bias False torch.Size([768])
decoder.embed_positions.weight False torch.Size([1026, 768])
decoder.layernorm_embedding.weight False torch.Size([768])
decoder.layernorm_embedding.bias False torch.Size([768])
decoder.layers.0.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.0.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.0.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.0.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.0.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.0.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.0.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.0.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.0.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.0.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.0.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.0.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.0.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.0.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.0.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.0.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.0.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.0.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.0.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.0.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.0.fc1.weight False torch.Size([3072, 768])
decoder.layers.0.fc1.bias False torch.Size([3072])
decoder.layers.0.fc2.weight False torch.Size([768, 3072])
decoder.layers.0.fc2.bias False torch.Size([768])
decoder.layers.0.final_layer_norm.weight False torch.Size([768])
decoder.layers.0.final_layer_norm.bias False torch.Size([768])
decoder.layers.1.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.1.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.1.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.1.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.1.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.1.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.1.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.1.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.1.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.1.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.1.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.1.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.1.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.1.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.1.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.1.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.1.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.1.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.1.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.1.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.1.fc1.weight False torch.Size([3072, 768])
decoder.layers.1.fc1.bias False torch.Size([3072])
decoder.layers.1.fc2.weight False torch.Size([768, 3072])
decoder.layers.1.fc2.bias False torch.Size([768])
decoder.layers.1.final_layer_norm.weight False torch.Size([768])
decoder.layers.1.final_layer_norm.bias False torch.Size([768])
decoder.layers.2.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.2.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.2.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.2.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.2.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.2.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.2.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.2.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.2.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.2.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.2.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.2.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.2.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.2.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.2.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.2.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.2.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.2.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.2.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.2.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.2.fc1.weight False torch.Size([3072, 768])
decoder.layers.2.fc1.bias False torch.Size([3072])
decoder.layers.2.fc2.weight False torch.Size([768, 3072])
decoder.layers.2.fc2.bias False torch.Size([768])
decoder.layers.2.final_layer_norm.weight False torch.Size([768])
decoder.layers.2.final_layer_norm.bias False torch.Size([768])
decoder.layers.3.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.3.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.3.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.3.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.3.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.3.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.3.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.3.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.3.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.3.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.3.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.3.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.3.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.3.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.3.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.3.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.3.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.3.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.3.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.3.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.3.fc1.weight False torch.Size([3072, 768])
decoder.layers.3.fc1.bias False torch.Size([3072])
decoder.layers.3.fc2.weight False torch.Size([768, 3072])
decoder.layers.3.fc2.bias False torch.Size([768])
decoder.layers.3.final_layer_norm.weight False torch.Size([768])
decoder.layers.3.final_layer_norm.bias False torch.Size([768])
decoder.layers.4.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.4.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.4.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.4.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.4.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.4.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.4.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.4.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.4.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.4.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.4.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.4.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.4.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.4.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.4.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.4.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.4.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.4.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.4.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.4.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.4.fc1.weight False torch.Size([3072, 768])
decoder.layers.4.fc1.bias False torch.Size([3072])
decoder.layers.4.fc2.weight False torch.Size([768, 3072])
decoder.layers.4.fc2.bias False torch.Size([768])
decoder.layers.4.final_layer_norm.weight False torch.Size([768])
decoder.layers.4.final_layer_norm.bias False torch.Size([768])
decoder.layers.5.self_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.5.self_attn.k_proj.bias False torch.Size([768])
decoder.layers.5.self_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.5.self_attn.v_proj.bias False torch.Size([768])
decoder.layers.5.self_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.5.self_attn.q_proj.bias False torch.Size([768])
decoder.layers.5.self_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.5.self_attn.out_proj.bias False torch.Size([768])
decoder.layers.5.self_attn_layer_norm.weight False torch.Size([768])
decoder.layers.5.self_attn_layer_norm.bias False torch.Size([768])
decoder.layers.5.encoder_attn.k_proj.weight False torch.Size([768, 768])
decoder.layers.5.encoder_attn.k_proj.bias False torch.Size([768])
decoder.layers.5.encoder_attn.v_proj.weight False torch.Size([768, 768])
decoder.layers.5.encoder_attn.v_proj.bias False torch.Size([768])
decoder.layers.5.encoder_attn.q_proj.weight False torch.Size([768, 768])
decoder.layers.5.encoder_attn.q_proj.bias False torch.Size([768])
decoder.layers.5.encoder_attn.out_proj.weight False torch.Size([768, 768])
decoder.layers.5.encoder_attn.out_proj.bias False torch.Size([768])
decoder.layers.5.encoder_attn_layer_norm.weight False torch.Size([768])
decoder.layers.5.encoder_attn_layer_norm.bias False torch.Size([768])
decoder.layers.5.fc1.weight False torch.Size([3072, 768])
decoder.layers.5.fc1.bias False torch.Size([3072])
decoder.layers.5.fc2.weight False torch.Size([768, 3072])
decoder.layers.5.fc2.bias False torch.Size([768])
decoder.layers.5.final_layer_norm.weight False torch.Size([768])
decoder.layers.5.final_layer_norm.bias False torch.Size([768])
2022-06-27 20:04:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4377
wandb: Currently logged in as: redredouane. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.12.19
wandb: Run data is saved locally in /home/dahmanir/wandb/run-20220627_200458-1g7cc3ob
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run checkpoints
wandb:  View project at https://wandb.ai/redredouane/BART
wandb:  View run at https://wandb.ai/redredouane/BART/runs/1g7cc3ob
2022-06-27 20:05:03 | INFO | fairseq.trainer | begin training epoch 14
2022-06-27 20:05:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-06-27 20:05:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-06-27 20:05:25 | INFO | root | Reducer buckets have been rebuilt in this iteration.
2022-06-27 20:05:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-06-27 20:06:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-27 20:06:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-27 20:06:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-27 20:07:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-27 20:12:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2022-06-27 20:21:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2022-06-27 20:24:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2022-06-27 20:45:46 | INFO | train_inner | {"epoch": 14, "update": 13.191, "loss": "12.778", "ppl": "7024.6", "wps": "14937.4", "ups": "0.08", "wpb": "185201", "bsz": "409.4", "num_updates": "200", "lr": "0.00016", "gnorm": "1.516", "clip": "100", "loss_scale": "0", "train_wall": "2344", "gb_free": "6.7", "wall": "0"}
2022-06-27 21:13:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2022-06-27 21:20:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2022-06-27 21:23:59 | INFO | train_inner | {"epoch": 14, "update": 13.238, "loss": "8.016", "ppl": "258.89", "wps": "16156.4", "ups": "0.09", "wpb": "185245", "bsz": "408.8", "num_updates": "400", "lr": "0.00032", "gnorm": "0.55", "clip": "100", "loss_scale": "0", "train_wall": "2202", "gb_free": "6.7", "wall": "0"}
2022-06-27 21:47:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2022-06-27 22:01:54 | INFO | train_inner | {"epoch": 14, "update": 13.284, "loss": "2.797", "ppl": "6.95", "wps": "16293.2", "ups": "0.09", "wpb": "185294", "bsz": "408.5", "num_updates": "600", "lr": "0.00039992", "gnorm": "0.129", "clip": "70", "loss_scale": "0", "train_wall": "2184", "gb_free": "6.8", "wall": "0"}
2022-06-27 22:39:31 | INFO | train_inner | {"epoch": 14, "update": 13.329, "loss": "2.365", "ppl": "5.15", "wps": "16399.6", "ups": "0.09", "wpb": "185132", "bsz": "408.6", "num_updates": "800", "lr": "0.00039976", "gnorm": "0.084", "clip": "1", "loss_scale": "0", "train_wall": "2168", "gb_free": "6.7", "wall": "0"}
2022-06-27 23:17:11 | INFO | train_inner | {"epoch": 14, "update": 13.375, "loss": "2.229", "ppl": "4.69", "wps": "16395.5", "ups": "0.09", "wpb": "185210", "bsz": "408.3", "num_updates": "1000", "lr": "0.0003996", "gnorm": "0.076", "clip": "1", "loss_scale": "0", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-06-27 23:54:51 | INFO | train_inner | {"epoch": 14, "update": 13.421, "loss": "2.144", "ppl": "4.42", "wps": "16388", "ups": "0.09", "wpb": "185235", "bsz": "408.8", "num_updates": "1200", "lr": "0.000399439", "gnorm": "0.071", "clip": "1", "loss_scale": "0", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-28 00:32:32 | INFO | train_inner | {"epoch": 14, "update": 13.466, "loss": "2.083", "ppl": "4.24", "wps": "16386.5", "ups": "0.09", "wpb": "185232", "bsz": "408.9", "num_updates": "1400", "lr": "0.000399279", "gnorm": "0.067", "clip": "0", "loss_scale": "0", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-28 01:10:14 | INFO | train_inner | {"epoch": 14, "update": 13.512, "loss": "2.033", "ppl": "4.09", "wps": "16390.6", "ups": "0.09", "wpb": "185379", "bsz": "408.6", "num_updates": "1600", "lr": "0.000399119", "gnorm": "0.065", "clip": "0", "loss_scale": "1", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-28 01:15:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2022-06-28 01:48:07 | INFO | train_inner | {"epoch": 14, "update": 13.558, "loss": "1.989", "ppl": "3.97", "wps": "16308.1", "ups": "0.09", "wpb": "185344", "bsz": "408.5", "num_updates": "1800", "lr": "0.000398959", "gnorm": "0.063", "clip": "0", "loss_scale": "0", "train_wall": "2183", "gb_free": "6.7", "wall": "0"}
2022-06-28 02:25:47 | INFO | train_inner | {"epoch": 14, "update": 13.604, "loss": "1.95", "ppl": "3.86", "wps": "16385.9", "ups": "0.09", "wpb": "185146", "bsz": "408.6", "num_updates": "2000", "lr": "0.000398799", "gnorm": "0.063", "clip": "0.5", "loss_scale": "1", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-06-28 03:03:29 | INFO | train_inner | {"epoch": 14, "update": 13.649, "loss": "1.912", "ppl": "3.76", "wps": "16380.2", "ups": "0.09", "wpb": "185294", "bsz": "408.9", "num_updates": "2200", "lr": "0.000398639", "gnorm": "0.06", "clip": "0", "loss_scale": "2", "train_wall": "2173", "gb_free": "6.7", "wall": "0"}
2022-06-28 03:41:11 | INFO | train_inner | {"epoch": 14, "update": 13.695, "loss": "1.875", "ppl": "3.67", "wps": "16372.7", "ups": "0.09", "wpb": "185121", "bsz": "408.5", "num_updates": "2400", "lr": "0.000398478", "gnorm": "0.059", "clip": "0", "loss_scale": "4", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-28 03:59:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-28 04:19:06 | INFO | train_inner | {"epoch": 14, "update": 13.741, "loss": "1.84", "ppl": "3.58", "wps": "16287", "ups": "0.09", "wpb": "185256", "bsz": "408.3", "num_updates": "2600", "lr": "0.000398318", "gnorm": "0.059", "clip": "1", "loss_scale": "2", "train_wall": "2185", "gb_free": "6.7", "wall": "0"}
2022-06-28 04:56:49 | INFO | train_inner | {"epoch": 14, "update": 13.787, "loss": "1.806", "ppl": "3.5", "wps": "16369.3", "ups": "0.09", "wpb": "185266", "bsz": "408.4", "num_updates": "2800", "lr": "0.000398158", "gnorm": "0.056", "clip": "0", "loss_scale": "4", "train_wall": "2174", "gb_free": "6.7", "wall": "0"}
2022-06-28 05:01:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-28 05:28:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2022-06-28 05:34:49 | INFO | train_inner | {"epoch": 14, "update": 13.833, "loss": "1.775", "ppl": "3.42", "wps": "16226.1", "ups": "0.09", "wpb": "184982", "bsz": "408.1", "num_updates": "3000", "lr": "0.000397998", "gnorm": "0.056", "clip": "1", "loss_scale": "1", "train_wall": "2189", "gb_free": "6.7", "wall": "0"}
2022-06-28 06:12:29 | INFO | train_inner | {"epoch": 14, "update": 13.878, "loss": "1.748", "ppl": "3.36", "wps": "16392.3", "ups": "0.09", "wpb": "185232", "bsz": "407.9", "num_updates": "3200", "lr": "0.000397838", "gnorm": "0.055", "clip": "0", "loss_scale": "1", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-06-28 06:50:10 | INFO | train_inner | {"epoch": 14, "update": 13.924, "loss": "1.722", "ppl": "3.3", "wps": "16386", "ups": "0.09", "wpb": "185255", "bsz": "408.9", "num_updates": "3400", "lr": "0.000397678", "gnorm": "0.054", "clip": "0.5", "loss_scale": "2", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-28 07:27:51 | INFO | train_inner | {"epoch": 14, "update": 13.97, "loss": "1.698", "ppl": "3.24", "wps": "16381.4", "ups": "0.09", "wpb": "185195", "bsz": "408.5", "num_updates": "3600", "lr": "0.000397518", "gnorm": "0.053", "clip": "0", "loss_scale": "4", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-28 07:52:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-28 07:53:05 | INFO | valid | {"epoch": 14, "valid_loss": "1.646", "valid_ppl": "3.13", "valid_wps": "36240.1", "valid_wpb": "5748.7", "valid_bsz": "12.7", "valid_num_updates": "3732", "valid_best_loss": "0.85"}
2022-06-28 07:53:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 3732 updates
2022-06-28 07:53:05 | INFO | fairseq.trainer | Saving checkpoint to /home/dahmanir/checkpoints/checkpoint14.pt
2022-06-28 07:53:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/dahmanir/checkpoints/checkpoint14.pt
2022-06-28 07:53:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint14.pt (epoch 14 @ 3732 updates, score 1.646) (writing took 3.099797297734767 seconds)
2022-06-28 07:53:08 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-06-28 07:53:09 | INFO | train | {"epoch": 14, "train_loss": "1.014", "train_ppl": "2.02", "train_wps": "523492", "train_ups": "0.56", "train_wpb": "933324", "train_bsz": "2020.7", "train_num_updates": "3732", "train_lr": "0.000397412", "train_gnorm": "0.082", "train_clip": "1.2", "train_loss_scale": "5", "train_train_wall": "88723", "train_gb_free": "6.7", "train_wall": "0"}
2022-06-28 07:53:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4377
2022-06-28 07:53:09 | INFO | fairseq.trainer | begin training epoch 15
2022-06-28 07:53:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-06-28 08:06:02 | INFO | train_inner | {"epoch": 15, "update": 14.016, "loss": "1.677", "ppl": "3.2", "wps": "16169.3", "ups": "0.09", "wpb": "185218", "bsz": "408.3", "num_updates": "3800", "lr": "0.000397357", "gnorm": "0.052", "clip": "0", "loss_scale": "8", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-28 08:43:48 | INFO | train_inner | {"epoch": 15, "update": 14.061, "loss": "1.658", "ppl": "3.16", "wps": "16347.4", "ups": "0.09", "wpb": "185193", "bsz": "409.4", "num_updates": "4000", "lr": "0.000397197", "gnorm": "0.052", "clip": "0", "loss_scale": "16", "train_wall": "2175", "gb_free": "6.7", "wall": "0"}
2022-06-28 09:02:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-28 09:21:45 | INFO | train_inner | {"epoch": 15, "update": 14.107, "loss": "1.641", "ppl": "3.12", "wps": "16271.6", "ups": "0.09", "wpb": "185216", "bsz": "408.4", "num_updates": "4200", "lr": "0.000397037", "gnorm": "0.052", "clip": "0.5", "loss_scale": "8", "train_wall": "2185", "gb_free": "6.7", "wall": "0"}
2022-06-28 09:53:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-28 09:54:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-28 09:59:50 | INFO | train_inner | {"epoch": 15, "update": 14.153, "loss": "1.629", "ppl": "3.09", "wps": "16217.2", "ups": "0.09", "wpb": "185274", "bsz": "408.6", "num_updates": "4400", "lr": "0.000396877", "gnorm": "0.051", "clip": "0", "loss_scale": "4", "train_wall": "2194", "gb_free": "6.7", "wall": "0"}
2022-06-28 10:37:33 | INFO | train_inner | {"epoch": 15, "update": 14.199, "loss": "1.615", "ppl": "3.06", "wps": "16367.4", "ups": "0.09", "wpb": "185224", "bsz": "408.4", "num_updates": "4600", "lr": "0.000396717", "gnorm": "0.051", "clip": "0", "loss_scale": "4", "train_wall": "2174", "gb_free": "6.7", "wall": "0"}
2022-06-28 10:43:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-28 10:55:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-28 11:15:38 | INFO | train_inner | {"epoch": 15, "update": 14.245, "loss": "1.603", "ppl": "3.04", "wps": "16222.3", "ups": "0.09", "wpb": "185368", "bsz": "409", "num_updates": "4800", "lr": "0.000396557", "gnorm": "0.051", "clip": "0", "loss_scale": "2", "train_wall": "2195", "gb_free": "6.7", "wall": "0"}
2022-06-28 11:53:18 | INFO | train_inner | {"epoch": 15, "update": 14.291, "loss": "1.595", "ppl": "3.02", "wps": "16400.6", "ups": "0.09", "wpb": "185338", "bsz": "408.8", "num_updates": "5000", "lr": "0.000396396", "gnorm": "0.053", "clip": "1", "loss_scale": "4", "train_wall": "2170", "gb_free": "6.6", "wall": "0"}
2022-06-28 12:31:00 | INFO | train_inner | {"epoch": 15, "update": 14.337, "loss": "1.583", "ppl": "3", "wps": "16370.9", "ups": "0.09", "wpb": "185138", "bsz": "407.8", "num_updates": "5200", "lr": "0.000396236", "gnorm": "0.05", "clip": "0", "loss_scale": "4", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-28 13:08:41 | INFO | train_inner | {"epoch": 15, "update": 14.382, "loss": "1.576", "ppl": "2.98", "wps": "16366", "ups": "0.09", "wpb": "185030", "bsz": "408.9", "num_updates": "5400", "lr": "0.000396076", "gnorm": "0.05", "clip": "0", "loss_scale": "8", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-28 13:36:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-28 13:41:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-28 13:44:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-28 13:46:57 | INFO | train_inner | {"epoch": 15, "update": 14.429, "loss": "1.568", "ppl": "2.97", "wps": "16130.5", "ups": "0.09", "wpb": "185143", "bsz": "408.7", "num_updates": "5600", "lr": "0.000395916", "gnorm": "0.049", "clip": "0", "loss_scale": "2", "train_wall": "2205", "gb_free": "6.7", "wall": "0"}
2022-06-28 14:24:37 | INFO | train_inner | {"epoch": 15, "update": 14.474, "loss": "1.56", "ppl": "2.95", "wps": "16398.2", "ups": "0.09", "wpb": "185280", "bsz": "408.9", "num_updates": "5800", "lr": "0.000395756", "gnorm": "0.05", "clip": "0", "loss_scale": "2", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-06-28 15:02:18 | INFO | train_inner | {"epoch": 15, "update": 14.52, "loss": "1.552", "ppl": "2.93", "wps": "16388.2", "ups": "0.09", "wpb": "185262", "bsz": "408.8", "num_updates": "6000", "lr": "0.000395596", "gnorm": "0.05", "clip": "0.5", "loss_scale": "4", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-28 15:13:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-28 15:40:08 | INFO | train_inner | {"epoch": 15, "update": 14.566, "loss": "1.548", "ppl": "2.92", "wps": "16307.1", "ups": "0.09", "wpb": "185131", "bsz": "408.4", "num_updates": "6200", "lr": "0.000395435", "gnorm": "0.049", "clip": "0", "loss_scale": "2", "train_wall": "2181", "gb_free": "6.7", "wall": "0"}
2022-06-28 16:17:47 | INFO | train_inner | {"epoch": 15, "update": 14.612, "loss": "1.541", "ppl": "2.91", "wps": "16406.4", "ups": "0.09", "wpb": "185296", "bsz": "408.6", "num_updates": "6400", "lr": "0.000395275", "gnorm": "0.049", "clip": "0", "loss_scale": "4", "train_wall": "2169", "gb_free": "6.7", "wall": "0"}
2022-06-28 16:55:27 | INFO | train_inner | {"epoch": 15, "update": 14.657, "loss": "1.538", "ppl": "2.9", "wps": "16384.5", "ups": "0.09", "wpb": "185127", "bsz": "408.4", "num_updates": "6600", "lr": "0.000395115", "gnorm": "0.049", "clip": "0", "loss_scale": "8", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-06-28 17:33:09 | INFO | train_inner | {"epoch": 15, "update": 14.703, "loss": "1.531", "ppl": "2.89", "wps": "16379.6", "ups": "0.09", "wpb": "185289", "bsz": "408.3", "num_updates": "6800", "lr": "0.000394955", "gnorm": "0.049", "clip": "0", "loss_scale": "8", "train_wall": "2173", "gb_free": "6.7", "wall": "0"}
2022-06-28 17:42:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-28 18:11:03 | INFO | train_inner | {"epoch": 15, "update": 14.749, "loss": "1.53", "ppl": "2.89", "wps": "16309.9", "ups": "0.09", "wpb": "185420", "bsz": "409", "num_updates": "7000", "lr": "0.000394795", "gnorm": "0.049", "clip": "0", "loss_scale": "8", "train_wall": "2184", "gb_free": "6.7", "wall": "0"}
2022-06-28 18:44:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-28 18:48:54 | INFO | train_inner | {"epoch": 15, "update": 14.795, "loss": "1.524", "ppl": "2.88", "wps": "16300.9", "ups": "0.09", "wpb": "185089", "bsz": "408", "num_updates": "7200", "lr": "0.000394635", "gnorm": "0.049", "clip": "0", "loss_scale": "8", "train_wall": "2181", "gb_free": "6.7", "wall": "0"}
2022-06-28 19:23:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-28 19:26:46 | INFO | train_inner | {"epoch": 15, "update": 14.841, "loss": "1.521", "ppl": "2.87", "wps": "16305.3", "ups": "0.09", "wpb": "185221", "bsz": "408", "num_updates": "7400", "lr": "0.000394474", "gnorm": "0.048", "clip": "0", "loss_scale": "4", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-06-28 19:34:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-28 20:04:38 | INFO | train_inner | {"epoch": 15, "update": 14.887, "loss": "1.518", "ppl": "2.86", "wps": "16303", "ups": "0.09", "wpb": "185260", "bsz": "408.5", "num_updates": "7600", "lr": "0.000394314", "gnorm": "0.048", "clip": "0", "loss_scale": "2", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-06-28 20:42:19 | INFO | train_inner | {"epoch": 15, "update": 14.932, "loss": "1.512", "ppl": "2.85", "wps": "16389.8", "ups": "0.09", "wpb": "185268", "bsz": "408.2", "num_updates": "7800", "lr": "0.000394154", "gnorm": "0.049", "clip": "0", "loss_scale": "4", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-28 20:50:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-28 21:20:08 | INFO | train_inner | {"epoch": 15, "update": 14.978, "loss": "1.509", "ppl": "2.85", "wps": "16322.3", "ups": "0.09", "wpb": "185134", "bsz": "408.5", "num_updates": "8000", "lr": "0.000393994", "gnorm": "0.049", "clip": "1", "loss_scale": "2", "train_wall": "2179", "gb_free": "6.7", "wall": "0"}
2022-06-28 21:38:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-28 21:38:22 | INFO | valid | {"epoch": 15, "valid_loss": "1.473", "valid_ppl": "2.78", "valid_wps": "36257.9", "valid_wpb": "5748.7", "valid_bsz": "12.7", "valid_num_updates": "8095", "valid_best_loss": "0.85"}
2022-06-28 21:38:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 8095 updates
2022-06-28 21:38:22 | INFO | fairseq.trainer | Saving checkpoint to /home/dahmanir/checkpoints/checkpoint15.pt
2022-06-28 21:38:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/dahmanir/checkpoints/checkpoint15.pt
2022-06-28 21:38:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint15.pt (epoch 15 @ 8095 updates, score 1.473) (writing took 3.289255728945136 seconds)
2022-06-28 21:38:25 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-06-28 21:38:25 | INFO | train | {"epoch": 15, "train_loss": "1.565", "train_ppl": "2.96", "train_wps": "16319.7", "train_ups": "0.09", "train_wpb": "185217", "train_bsz": "408.5", "train_num_updates": "8095", "train_lr": "0.000393918", "train_gnorm": "0.05", "train_clip": "0.1", "train_loss_scale": "2", "train_train_wall": "47524", "train_gb_free": "6.7", "train_wall": "0"}
2022-06-28 21:38:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4377
2022-06-28 21:38:26 | INFO | fairseq.trainer | begin training epoch 16
2022-06-28 21:38:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-06-28 21:58:19 | INFO | train_inner | {"epoch": 16, "update": 15.024, "loss": "1.505", "ppl": "2.84", "wps": "16155.5", "ups": "0.09", "wpb": "185077", "bsz": "408", "num_updates": "8200", "lr": "0.000393834", "gnorm": "0.048", "clip": "0", "loss_scale": "4", "train_wall": "2169", "gb_free": "6.7", "wall": "0"}
2022-06-28 22:35:59 | INFO | train_inner | {"epoch": 16, "update": 15.07, "loss": "1.502", "ppl": "2.83", "wps": "16375.1", "ups": "0.09", "wpb": "185072", "bsz": "408.6", "num_updates": "8400", "lr": "0.000393674", "gnorm": "0.048", "clip": "0", "loss_scale": "8", "train_wall": "2169", "gb_free": "6.7", "wall": "0"}
2022-06-28 23:13:40 | INFO | train_inner | {"epoch": 16, "update": 15.115, "loss": "1.5", "ppl": "2.83", "wps": "16388.4", "ups": "0.09", "wpb": "185265", "bsz": "408.8", "num_updates": "8600", "lr": "0.000393514", "gnorm": "0.048", "clip": "0", "loss_scale": "8", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-28 23:51:22 | INFO | train_inner | {"epoch": 16, "update": 15.161, "loss": "1.498", "ppl": "2.82", "wps": "16382.8", "ups": "0.09", "wpb": "185237", "bsz": "408.8", "num_updates": "8800", "lr": "0.000393353", "gnorm": "0.048", "clip": "0", "loss_scale": "16", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-29 00:08:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-29 00:16:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-29 00:29:24 | INFO | train_inner | {"epoch": 16, "update": 15.207, "loss": "1.493", "ppl": "2.81", "wps": "16215.5", "ups": "0.09", "wpb": "185060", "bsz": "408.7", "num_updates": "9000", "lr": "0.000393193", "gnorm": "0.048", "clip": "0", "loss_scale": "8", "train_wall": "2192", "gb_free": "6.7", "wall": "0"}
2022-06-29 01:07:06 | INFO | train_inner | {"epoch": 16, "update": 15.253, "loss": "1.493", "ppl": "2.82", "wps": "16375.6", "ups": "0.09", "wpb": "185222", "bsz": "407.9", "num_updates": "9200", "lr": "0.000393033", "gnorm": "0.048", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-29 01:20:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-29 01:44:58 | INFO | train_inner | {"epoch": 16, "update": 15.299, "loss": "1.493", "ppl": "2.81", "wps": "16299.6", "ups": "0.09", "wpb": "185146", "bsz": "408.9", "num_updates": "9400", "lr": "0.000392873", "gnorm": "0.048", "clip": "0", "loss_scale": "8", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-06-29 01:57:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-29 02:09:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-29 02:23:00 | INFO | train_inner | {"epoch": 16, "update": 15.345, "loss": "1.49", "ppl": "2.81", "wps": "16230.9", "ups": "0.09", "wpb": "185234", "bsz": "408.1", "num_updates": "9600", "lr": "0.000392713", "gnorm": "0.048", "clip": "0", "loss_scale": "2", "train_wall": "2192", "gb_free": "6.7", "wall": "0"}
2022-06-29 03:00:40 | INFO | train_inner | {"epoch": 16, "update": 15.391, "loss": "1.487", "ppl": "2.8", "wps": "16405.5", "ups": "0.09", "wpb": "185364", "bsz": "408.9", "num_updates": "9800", "lr": "0.000392553", "gnorm": "0.048", "clip": "0", "loss_scale": "4", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-06-29 03:38:22 | INFO | train_inner | {"epoch": 16, "update": 15.436, "loss": "1.485", "ppl": "2.8", "wps": "16385.3", "ups": "0.09", "wpb": "185308", "bsz": "407.6", "num_updates": "10000", "lr": "0.000392392", "gnorm": "0.048", "clip": "0", "loss_scale": "4", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-29 03:54:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-29 04:16:13 | INFO | train_inner | {"epoch": 16, "update": 15.482, "loss": "1.482", "ppl": "2.79", "wps": "16310.1", "ups": "0.09", "wpb": "185164", "bsz": "408.2", "num_updates": "10200", "lr": "0.000392232", "gnorm": "0.048", "clip": "0", "loss_scale": "4", "train_wall": "2180", "gb_free": "6.7", "wall": "0"}
2022-06-29 04:48:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-29 04:54:05 | INFO | train_inner | {"epoch": 16, "update": 15.528, "loss": "1.483", "ppl": "2.8", "wps": "16311.8", "ups": "0.09", "wpb": "185333", "bsz": "408.6", "num_updates": "10400", "lr": "0.000392072", "gnorm": "0.048", "clip": "0", "loss_scale": "4", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-06-29 05:31:45 | INFO | train_inner | {"epoch": 16, "update": 15.574, "loss": "1.482", "ppl": "2.79", "wps": "16393.2", "ups": "0.09", "wpb": "185230", "bsz": "408.9", "num_updates": "10600", "lr": "0.000391912", "gnorm": "0.048", "clip": "0", "loss_scale": "4", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-06-29 06:09:26 | INFO | train_inner | {"epoch": 16, "update": 15.62, "loss": "1.479", "ppl": "2.79", "wps": "16388.9", "ups": "0.09", "wpb": "185291", "bsz": "408.9", "num_updates": "10800", "lr": "0.000391752", "gnorm": "0.048", "clip": "0", "loss_scale": "8", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-29 06:32:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-29 06:47:20 | INFO | train_inner | {"epoch": 16, "update": 15.666, "loss": "1.478", "ppl": "2.79", "wps": "16290.3", "ups": "0.09", "wpb": "185236", "bsz": "408.8", "num_updates": "11000", "lr": "0.000391592", "gnorm": "0.048", "clip": "0", "loss_scale": "8", "train_wall": "2183", "gb_free": "6.7", "wall": "0"}
2022-06-29 07:25:02 | INFO | train_inner | {"epoch": 16, "update": 15.711, "loss": "1.477", "ppl": "2.78", "wps": "16385", "ups": "0.09", "wpb": "185275", "bsz": "408.5", "num_updates": "11200", "lr": "0.000391431", "gnorm": "0.048", "clip": "0", "loss_scale": "16", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-29 08:02:43 | INFO | train_inner | {"epoch": 16, "update": 15.757, "loss": "1.476", "ppl": "2.78", "wps": "16368.6", "ups": "0.09", "wpb": "185097", "bsz": "408.1", "num_updates": "11400", "lr": "0.000391271", "gnorm": "0.048", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-29 08:40:28 | INFO | train_inner | {"epoch": 16, "update": 15.803, "loss": "1.472", "ppl": "2.77", "wps": "16366.3", "ups": "0.09", "wpb": "185309", "bsz": "407.9", "num_updates": "11600", "lr": "0.000391111", "gnorm": "0.048", "clip": "0", "loss_scale": "32", "train_wall": "2175", "gb_free": "6.7", "wall": "0"}
2022-06-29 09:02:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-06-29 09:18:22 | INFO | train_inner | {"epoch": 16, "update": 15.849, "loss": "1.474", "ppl": "2.78", "wps": "16287.9", "ups": "0.09", "wpb": "185176", "bsz": "409.1", "num_updates": "11800", "lr": "0.000390951", "gnorm": "0.048", "clip": "0", "loss_scale": "32", "train_wall": "2183", "gb_free": "6.7", "wall": "0"}
2022-06-29 09:51:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-06-29 09:56:19 | INFO | train_inner | {"epoch": 16, "update": 15.894, "loss": "1.471", "ppl": "2.77", "wps": "16282.7", "ups": "0.09", "wpb": "185380", "bsz": "408.1", "num_updates": "12000", "lr": "0.000390791", "gnorm": "0.048", "clip": "0", "loss_scale": "32", "train_wall": "2187", "gb_free": "6.7", "wall": "0"}
2022-06-29 10:22:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-29 10:34:12 | INFO | train_inner | {"epoch": 16, "update": 15.94, "loss": "1.472", "ppl": "2.77", "wps": "16294.1", "ups": "0.09", "wpb": "185188", "bsz": "409.6", "num_updates": "12200", "lr": "0.000390631", "gnorm": "0.048", "clip": "0", "loss_scale": "16", "train_wall": "2183", "gb_free": "6.7", "wall": "0"}
2022-06-29 11:11:56 | INFO | train_inner | {"epoch": 16, "update": 15.986, "loss": "1.471", "ppl": "2.77", "wps": "16361.1", "ups": "0.09", "wpb": "185249", "bsz": "408.3", "num_updates": "12400", "lr": "0.00039047", "gnorm": "0.048", "clip": "0", "loss_scale": "32", "train_wall": "2174", "gb_free": "6.7", "wall": "0"}
2022-06-29 11:23:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-29 11:23:45 | INFO | valid | {"epoch": 16, "valid_loss": "1.44", "valid_ppl": "2.71", "valid_wps": "36110.4", "valid_wpb": "5748.7", "valid_bsz": "12.7", "valid_num_updates": "12461", "valid_best_loss": "0.85"}
2022-06-29 11:23:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 12461 updates
2022-06-29 11:23:45 | INFO | fairseq.trainer | Saving checkpoint to /home/dahmanir/checkpoints/checkpoint16.pt
2022-06-29 11:23:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/dahmanir/checkpoints/checkpoint16.pt
2022-06-29 11:23:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint16.pt (epoch 16 @ 12461 updates, score 1.44) (writing took 3.1410877439193428 seconds)
2022-06-29 11:23:48 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-06-29 11:23:48 | INFO | train | {"epoch": 16, "train_loss": "1.484", "train_ppl": "2.8", "train_wps": "16329.2", "train_ups": "0.09", "train_wpb": "185218", "train_bsz": "408.5", "train_num_updates": "12461", "train_lr": "0.000390422", "train_gnorm": "0.048", "train_clip": "0", "train_loss_scale": "32", "train_train_wall": "47520", "train_gb_free": "6.7", "train_wall": "0"}
2022-06-29 11:23:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4377
2022-06-29 11:23:48 | INFO | fairseq.trainer | begin training epoch 17
2022-06-29 11:23:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-06-29 11:37:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-29 11:50:18 | INFO | train_inner | {"epoch": 17, "update": 16.032, "loss": "1.468", "ppl": "2.77", "wps": "16069.6", "ups": "0.09", "wpb": "184936", "bsz": "408.2", "num_updates": "12600", "lr": "0.00039031", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-06-29 12:09:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-29 12:18:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-29 12:18:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-29 12:28:35 | INFO | train_inner | {"epoch": 17, "update": 16.078, "loss": "1.466", "ppl": "2.76", "wps": "16135.7", "ups": "0.09", "wpb": "185304", "bsz": "409.5", "num_updates": "12800", "lr": "0.00039015", "gnorm": "0.047", "clip": "0", "loss_scale": "2", "train_wall": "2206", "gb_free": "6.7", "wall": "0"}
2022-06-29 13:06:13 | INFO | train_inner | {"epoch": 17, "update": 16.124, "loss": "1.463", "ppl": "2.76", "wps": "16410.3", "ups": "0.09", "wpb": "185264", "bsz": "407.9", "num_updates": "13000", "lr": "0.00038999", "gnorm": "0.047", "clip": "0", "loss_scale": "2", "train_wall": "2169", "gb_free": "6.7", "wall": "0"}
2022-06-29 13:43:53 | INFO | train_inner | {"epoch": 17, "update": 16.17, "loss": "1.464", "ppl": "2.76", "wps": "16384.5", "ups": "0.09", "wpb": "185166", "bsz": "408.2", "num_updates": "13200", "lr": "0.00038983", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-29 14:21:34 | INFO | train_inner | {"epoch": 17, "update": 16.215, "loss": "1.463", "ppl": "2.76", "wps": "16395.9", "ups": "0.09", "wpb": "185321", "bsz": "408.3", "num_updates": "13400", "lr": "0.00038967", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-29 14:59:13 | INFO | train_inner | {"epoch": 17, "update": 16.261, "loss": "1.462", "ppl": "2.76", "wps": "16384.4", "ups": "0.09", "wpb": "185121", "bsz": "408.4", "num_updates": "13600", "lr": "0.00038951", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-29 15:22:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-29 15:37:07 | INFO | train_inner | {"epoch": 17, "update": 16.307, "loss": "1.461", "ppl": "2.75", "wps": "16303", "ups": "0.09", "wpb": "185320", "bsz": "408.6", "num_updates": "13800", "lr": "0.000389349", "gnorm": "0.048", "clip": "0", "loss_scale": "8", "train_wall": "2184", "gb_free": "6.7", "wall": "0"}
2022-06-29 16:04:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-29 16:14:58 | INFO | train_inner | {"epoch": 17, "update": 16.353, "loss": "1.462", "ppl": "2.75", "wps": "16310.1", "ups": "0.09", "wpb": "185231", "bsz": "409", "num_updates": "14000", "lr": "0.000389189", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-06-29 16:52:38 | INFO | train_inner | {"epoch": 17, "update": 16.399, "loss": "1.46", "ppl": "2.75", "wps": "16390.9", "ups": "0.09", "wpb": "185237", "bsz": "408.1", "num_updates": "14200", "lr": "0.000389029", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-29 17:30:20 | INFO | train_inner | {"epoch": 17, "update": 16.444, "loss": "1.462", "ppl": "2.76", "wps": "16377.7", "ups": "0.09", "wpb": "185192", "bsz": "408", "num_updates": "14400", "lr": "0.000388869", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2173", "gb_free": "6.7", "wall": "0"}
2022-06-29 18:08:01 | INFO | train_inner | {"epoch": 17, "update": 16.49, "loss": "1.461", "ppl": "2.75", "wps": "16382.1", "ups": "0.09", "wpb": "185229", "bsz": "408.9", "num_updates": "14600", "lr": "0.000388709", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-29 18:39:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-29 18:45:55 | INFO | train_inner | {"epoch": 17, "update": 16.536, "loss": "1.46", "ppl": "2.75", "wps": "16293.8", "ups": "0.09", "wpb": "185208", "bsz": "408.8", "num_updates": "14800", "lr": "0.000388549", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2184", "gb_free": "6.7", "wall": "0"}
2022-06-29 19:23:37 | INFO | train_inner | {"epoch": 17, "update": 16.582, "loss": "1.461", "ppl": "2.75", "wps": "16374.3", "ups": "0.09", "wpb": "185238", "bsz": "408.9", "num_updates": "15000", "lr": "0.000388388", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2174", "gb_free": "6.7", "wall": "0"}
2022-06-29 20:01:21 | INFO | train_inner | {"epoch": 17, "update": 16.627, "loss": "1.46", "ppl": "2.75", "wps": "16364.6", "ups": "0.09", "wpb": "185233", "bsz": "408.7", "num_updates": "15200", "lr": "0.000388228", "gnorm": "0.047", "clip": "0", "loss_scale": "32", "train_wall": "2175", "gb_free": "6.7", "wall": "0"}
2022-06-29 20:11:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-29 20:17:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-29 20:30:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-29 20:39:36 | INFO | train_inner | {"epoch": 17, "update": 16.674, "loss": "1.459", "ppl": "2.75", "wps": "16143.5", "ups": "0.09", "wpb": "185215", "bsz": "407.8", "num_updates": "15400", "lr": "0.000388068", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2205", "gb_free": "6.7", "wall": "0"}
2022-06-29 20:49:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-29 20:50:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2022-06-29 21:12:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2022-06-29 21:17:46 | INFO | train_inner | {"epoch": 17, "update": 16.72, "loss": "1.455", "ppl": "2.74", "wps": "16170.5", "ups": "0.09", "wpb": "185190", "bsz": "408", "num_updates": "15600", "lr": "0.000387908", "gnorm": "0.047", "clip": "0.5", "loss_scale": "0", "train_wall": "2201", "gb_free": "6.7", "wall": "0"}
2022-06-29 21:55:21 | INFO | train_inner | {"epoch": 17, "update": 16.766, "loss": "1.456", "ppl": "2.74", "wps": "16426.8", "ups": "0.09", "wpb": "185229", "bsz": "408.3", "num_updates": "15800", "lr": "0.000387748", "gnorm": "0.047", "clip": "0", "loss_scale": "0", "train_wall": "2167", "gb_free": "6.7", "wall": "0"}
2022-06-29 22:32:57 | INFO | train_inner | {"epoch": 17, "update": 16.812, "loss": "1.457", "ppl": "2.75", "wps": "16421", "ups": "0.09", "wpb": "185200", "bsz": "408.8", "num_updates": "16000", "lr": "0.000387588", "gnorm": "0.047", "clip": "0", "loss_scale": "1", "train_wall": "2167", "gb_free": "6.7", "wall": "0"}
2022-06-29 23:10:34 | INFO | train_inner | {"epoch": 17, "update": 16.857, "loss": "1.456", "ppl": "2.74", "wps": "16410.6", "ups": "0.09", "wpb": "185206", "bsz": "408.4", "num_updates": "16200", "lr": "0.000387427", "gnorm": "0.048", "clip": "0.5", "loss_scale": "2", "train_wall": "2168", "gb_free": "6.7", "wall": "0"}
2022-06-29 23:39:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-06-29 23:48:24 | INFO | train_inner | {"epoch": 17, "update": 16.903, "loss": "1.453", "ppl": "2.74", "wps": "16322.5", "ups": "0.09", "wpb": "185271", "bsz": "408.2", "num_updates": "16400", "lr": "0.000387267", "gnorm": "0.047", "clip": "0", "loss_scale": "2", "train_wall": "2181", "gb_free": "6.7", "wall": "0"}
2022-06-30 00:26:01 | INFO | train_inner | {"epoch": 17, "update": 16.949, "loss": "1.453", "ppl": "2.74", "wps": "16411.9", "ups": "0.09", "wpb": "185185", "bsz": "408.3", "num_updates": "16600", "lr": "0.000387107", "gnorm": "0.048", "clip": "0.5", "loss_scale": "2", "train_wall": "2168", "gb_free": "6.7", "wall": "0"}
2022-06-30 01:03:38 | INFO | train_inner | {"epoch": 17, "update": 16.995, "loss": "1.454", "ppl": "2.74", "wps": "16414.3", "ups": "0.09", "wpb": "185226", "bsz": "409.2", "num_updates": "16800", "lr": "0.000386947", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2168", "gb_free": "6.7", "wall": "0"}
2022-06-30 01:08:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 01:08:27 | INFO | valid | {"epoch": 17, "valid_loss": "1.426", "valid_ppl": "2.69", "valid_wps": "36299.2", "valid_wpb": "5748.7", "valid_bsz": "12.7", "valid_num_updates": "16824", "valid_best_loss": "0.85"}
2022-06-30 01:08:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 16824 updates
2022-06-30 01:08:27 | INFO | fairseq.trainer | Saving checkpoint to /home/dahmanir/checkpoints/checkpoint17.pt
2022-06-30 01:08:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/dahmanir/checkpoints/checkpoint17.pt
2022-06-30 01:08:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint17.pt (epoch 17 @ 16824 updates, score 1.426) (writing took 3.063674666918814 seconds)
2022-06-30 01:08:30 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-06-30 01:08:30 | INFO | train | {"epoch": 17, "train_loss": "1.46", "train_ppl": "2.75", "train_wps": "16331", "train_ups": "0.09", "train_wpb": "185217", "train_bsz": "408.5", "train_num_updates": "16824", "train_lr": "0.000386928", "train_gnorm": "0.047", "train_clip": "0.1", "train_loss_scale": "4", "train_train_wall": "47507", "train_gb_free": "6.7", "train_wall": "0"}
2022-06-30 01:08:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4377
2022-06-30 01:08:31 | INFO | fairseq.trainer | begin training epoch 18
2022-06-30 01:08:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-06-30 01:41:45 | INFO | train_inner | {"epoch": 18, "update": 17.04, "loss": "1.451", "ppl": "2.73", "wps": "16179.8", "ups": "0.09", "wpb": "185042", "bsz": "408", "num_updates": "17000", "lr": "0.000386787", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2168", "gb_free": "6.7", "wall": "0"}
2022-06-30 02:19:27 | INFO | train_inner | {"epoch": 18, "update": 17.086, "loss": "1.45", "ppl": "2.73", "wps": "16375.7", "ups": "0.09", "wpb": "185178", "bsz": "408.9", "num_updates": "17200", "lr": "0.000386627", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-30 02:27:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-30 02:35:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-30 02:57:30 | INFO | train_inner | {"epoch": 18, "update": 17.132, "loss": "1.449", "ppl": "2.73", "wps": "16233.5", "ups": "0.09", "wpb": "185315", "bsz": "407.6", "num_updates": "17400", "lr": "0.000386466", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2194", "gb_free": "6.7", "wall": "0"}
2022-06-30 03:35:09 | INFO | train_inner | {"epoch": 18, "update": 17.178, "loss": "1.45", "ppl": "2.73", "wps": "16399.4", "ups": "0.09", "wpb": "185223", "bsz": "408.8", "num_updates": "17600", "lr": "0.000386306", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-06-30 04:12:50 | INFO | train_inner | {"epoch": 18, "update": 17.223, "loss": "1.45", "ppl": "2.73", "wps": "16384.7", "ups": "0.09", "wpb": "185232", "bsz": "409.3", "num_updates": "17800", "lr": "0.000386146", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-30 04:14:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-30 04:50:41 | INFO | train_inner | {"epoch": 18, "update": 17.269, "loss": "1.45", "ppl": "2.73", "wps": "16306.6", "ups": "0.09", "wpb": "185214", "bsz": "408.8", "num_updates": "18000", "lr": "0.000385986", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-06-30 05:08:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-30 05:28:32 | INFO | train_inner | {"epoch": 18, "update": 17.315, "loss": "1.448", "ppl": "2.73", "wps": "16317.1", "ups": "0.09", "wpb": "185249", "bsz": "408.4", "num_updates": "18200", "lr": "0.000385826", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2181", "gb_free": "6.8", "wall": "0"}
2022-06-30 06:06:13 | INFO | train_inner | {"epoch": 18, "update": 17.361, "loss": "1.45", "ppl": "2.73", "wps": "16385.8", "ups": "0.09", "wpb": "185242", "bsz": "407.6", "num_updates": "18400", "lr": "0.000385666", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-30 06:43:54 | INFO | train_inner | {"epoch": 18, "update": 17.407, "loss": "1.449", "ppl": "2.73", "wps": "16381.4", "ups": "0.09", "wpb": "185221", "bsz": "407.7", "num_updates": "18600", "lr": "0.000385506", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2173", "gb_free": "6.7", "wall": "0"}
2022-06-30 07:21:38 | INFO | train_inner | {"epoch": 18, "update": 17.452, "loss": "1.45", "ppl": "2.73", "wps": "16364.8", "ups": "0.09", "wpb": "185186", "bsz": "408.5", "num_updates": "18800", "lr": "0.000385345", "gnorm": "0.047", "clip": "0", "loss_scale": "32", "train_wall": "2174", "gb_free": "6.7", "wall": "0"}
2022-06-30 07:30:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-30 07:59:30 | INFO | train_inner | {"epoch": 18, "update": 17.498, "loss": "1.45", "ppl": "2.73", "wps": "16299.6", "ups": "0.09", "wpb": "185204", "bsz": "408.9", "num_updates": "19000", "lr": "0.000385185", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2183", "gb_free": "6.8", "wall": "0"}
2022-06-30 08:37:11 | INFO | train_inner | {"epoch": 18, "update": 17.544, "loss": "1.448", "ppl": "2.73", "wps": "16380.6", "ups": "0.09", "wpb": "185200", "bsz": "408.4", "num_updates": "19200", "lr": "0.000385025", "gnorm": "0.047", "clip": "0", "loss_scale": "32", "train_wall": "2172", "gb_free": "6.8", "wall": "0"}
2022-06-30 08:50:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-30 08:56:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-30 09:15:13 | INFO | train_inner | {"epoch": 18, "update": 17.59, "loss": "1.448", "ppl": "2.73", "wps": "16229.7", "ups": "0.09", "wpb": "185174", "bsz": "408.4", "num_updates": "19400", "lr": "0.000384865", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2192", "gb_free": "6.7", "wall": "0"}
2022-06-30 09:52:55 | INFO | train_inner | {"epoch": 18, "update": 17.636, "loss": "1.448", "ppl": "2.73", "wps": "16380.8", "ups": "0.09", "wpb": "185214", "bsz": "408.1", "num_updates": "19600", "lr": "0.000384705", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-30 10:30:35 | INFO | train_inner | {"epoch": 18, "update": 17.682, "loss": "1.447", "ppl": "2.73", "wps": "16391.4", "ups": "0.09", "wpb": "185255", "bsz": "408.5", "num_updates": "19800", "lr": "0.000384545", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-06-30 10:47:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-30 11:08:29 | INFO | train_inner | {"epoch": 18, "update": 17.727, "loss": "1.447", "ppl": "2.73", "wps": "16306.6", "ups": "0.09", "wpb": "185374", "bsz": "408.2", "num_updates": "20000", "lr": "0.000384384", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2184", "gb_free": "6.7", "wall": "0"}
2022-06-30 11:42:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-30 11:44:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-30 11:46:33 | INFO | train_inner | {"epoch": 18, "update": 17.774, "loss": "1.446", "ppl": "2.72", "wps": "16215.6", "ups": "0.09", "wpb": "185182", "bsz": "409.7", "num_updates": "20200", "lr": "0.000384224", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2194", "gb_free": "6.7", "wall": "0"}
2022-06-30 12:24:12 | INFO | train_inner | {"epoch": 18, "update": 17.819, "loss": "1.446", "ppl": "2.73", "wps": "16386.7", "ups": "0.09", "wpb": "185122", "bsz": "408.5", "num_updates": "20400", "lr": "0.000384064", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-30 12:47:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-30 13:02:06 | INFO | train_inner | {"epoch": 18, "update": 17.865, "loss": "1.446", "ppl": "2.73", "wps": "16307.2", "ups": "0.09", "wpb": "185393", "bsz": "409.1", "num_updates": "20600", "lr": "0.000383904", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2185", "gb_free": "6.7", "wall": "0"}
2022-06-30 13:39:45 | INFO | train_inner | {"epoch": 18, "update": 17.911, "loss": "1.446", "ppl": "2.72", "wps": "16391.2", "ups": "0.09", "wpb": "185153", "bsz": "408.6", "num_updates": "20800", "lr": "0.000383744", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-30 13:46:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-30 14:17:34 | INFO | train_inner | {"epoch": 18, "update": 17.957, "loss": "1.445", "ppl": "2.72", "wps": "16319.2", "ups": "0.09", "wpb": "185178", "bsz": "408.3", "num_updates": "21000", "lr": "0.000383584", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2180", "gb_free": "6.7", "wall": "0"}
2022-06-30 14:53:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 14:53:29 | INFO | valid | {"epoch": 18, "valid_loss": "1.415", "valid_ppl": "2.67", "valid_wps": "36083.5", "valid_wpb": "5748.7", "valid_bsz": "12.7", "valid_num_updates": "21189", "valid_best_loss": "0.85"}
2022-06-30 14:53:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 21189 updates
2022-06-30 14:53:29 | INFO | fairseq.trainer | Saving checkpoint to /home/dahmanir/checkpoints/checkpoint18.pt
2022-06-30 14:53:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/dahmanir/checkpoints/checkpoint18.pt
2022-06-30 14:53:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint18.pt (epoch 18 @ 21189 updates, score 1.415) (writing took 3.155256835743785 seconds)
2022-06-30 14:53:32 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-06-30 14:53:32 | INFO | train | {"epoch": 18, "train_loss": "1.448", "train_ppl": "2.73", "train_wps": "16332.3", "train_ups": "0.09", "train_wpb": "185217", "train_bsz": "408.5", "train_num_updates": "21189", "train_lr": "0.000383432", "train_gnorm": "0.047", "train_clip": "0", "train_loss_scale": "16", "train_train_wall": "47528", "train_gb_free": "6.7", "train_wall": "0"}
2022-06-30 14:53:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4377
2022-06-30 14:53:32 | INFO | fairseq.trainer | begin training epoch 19
2022-06-30 14:53:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-06-30 14:55:40 | INFO | train_inner | {"epoch": 19, "update": 18.003, "loss": "1.446", "ppl": "2.72", "wps": "16187.5", "ups": "0.09", "wpb": "185012", "bsz": "408.7", "num_updates": "21200", "lr": "0.000383423", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2168", "gb_free": "6.7", "wall": "0"}
2022-06-30 15:00:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-30 15:33:30 | INFO | train_inner | {"epoch": 19, "update": 18.048, "loss": "1.442", "ppl": "2.72", "wps": "16307.7", "ups": "0.09", "wpb": "185074", "bsz": "408.3", "num_updates": "21400", "lr": "0.000383263", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2180", "gb_free": "6.7", "wall": "0"}
2022-06-30 16:11:09 | INFO | train_inner | {"epoch": 19, "update": 18.094, "loss": "1.441", "ppl": "2.72", "wps": "16389.6", "ups": "0.09", "wpb": "185094", "bsz": "409.2", "num_updates": "21600", "lr": "0.000383103", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-06-30 16:46:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-30 16:49:00 | INFO | train_inner | {"epoch": 19, "update": 18.14, "loss": "1.442", "ppl": "2.72", "wps": "16300.8", "ups": "0.09", "wpb": "185135", "bsz": "408.5", "num_updates": "21800", "lr": "0.000382943", "gnorm": "0.047", "clip": "0.5", "loss_scale": "16", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-06-30 17:11:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-30 17:26:52 | INFO | train_inner | {"epoch": 19, "update": 18.186, "loss": "1.443", "ppl": "2.72", "wps": "16312.6", "ups": "0.09", "wpb": "185257", "bsz": "408.5", "num_updates": "22000", "lr": "0.000382783", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-06-30 18:04:31 | INFO | train_inner | {"epoch": 19, "update": 18.232, "loss": "1.442", "ppl": "2.72", "wps": "16387.2", "ups": "0.09", "wpb": "185102", "bsz": "408.5", "num_updates": "22200", "lr": "0.000382623", "gnorm": "0.047", "clip": "0.5", "loss_scale": "16", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-06-30 18:42:13 | INFO | train_inner | {"epoch": 19, "update": 18.277, "loss": "1.444", "ppl": "2.72", "wps": "16382.9", "ups": "0.09", "wpb": "185289", "bsz": "409.2", "num_updates": "22400", "lr": "0.000382462", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2173", "gb_free": "6.7", "wall": "0"}
2022-06-30 19:19:56 | INFO | train_inner | {"epoch": 19, "update": 18.323, "loss": "1.441", "ppl": "2.72", "wps": "16374.7", "ups": "0.09", "wpb": "185303", "bsz": "407.8", "num_updates": "22600", "lr": "0.000382302", "gnorm": "0.047", "clip": "0", "loss_scale": "32", "train_wall": "2174", "gb_free": "6.7", "wall": "0"}
2022-06-30 19:21:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-30 19:57:48 | INFO | train_inner | {"epoch": 19, "update": 18.369, "loss": "1.443", "ppl": "2.72", "wps": "16311.9", "ups": "0.09", "wpb": "185333", "bsz": "408.9", "num_updates": "22800", "lr": "0.000382142", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2183", "gb_free": "6.7", "wall": "0"}
2022-06-30 20:20:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-06-30 20:28:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-30 20:35:54 | INFO | train_inner | {"epoch": 19, "update": 18.415, "loss": "1.442", "ppl": "2.72", "wps": "16212.9", "ups": "0.09", "wpb": "185269", "bsz": "409.1", "num_updates": "23000", "lr": "0.000381982", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2195", "gb_free": "6.7", "wall": "0"}
2022-06-30 20:56:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-30 21:13:42 | INFO | train_inner | {"epoch": 19, "update": 18.461, "loss": "1.44", "ppl": "2.71", "wps": "16324.5", "ups": "0.09", "wpb": "185169", "bsz": "408.5", "num_updates": "23200", "lr": "0.000381822", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2179", "gb_free": "6.7", "wall": "0"}
2022-06-30 21:51:19 | INFO | train_inner | {"epoch": 19, "update": 18.507, "loss": "1.441", "ppl": "2.72", "wps": "16410.7", "ups": "0.09", "wpb": "185177", "bsz": "408.7", "num_updates": "23400", "lr": "0.000381662", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2168", "gb_free": "6.7", "wall": "0"}
2022-06-30 22:28:59 | INFO | train_inner | {"epoch": 19, "update": 18.552, "loss": "1.441", "ppl": "2.72", "wps": "16385.7", "ups": "0.09", "wpb": "185178", "bsz": "408.5", "num_updates": "23600", "lr": "0.000381502", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-06-30 23:04:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-06-30 23:06:51 | INFO | train_inner | {"epoch": 19, "update": 18.598, "loss": "1.443", "ppl": "2.72", "wps": "16308.4", "ups": "0.09", "wpb": "185248", "bsz": "408.3", "num_updates": "23800", "lr": "0.000381341", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-06-30 23:23:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-06-30 23:44:42 | INFO | train_inner | {"epoch": 19, "update": 18.644, "loss": "1.441", "ppl": "2.72", "wps": "16324.6", "ups": "0.09", "wpb": "185337", "bsz": "408.8", "num_updates": "24000", "lr": "0.000381181", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2181", "gb_free": "6.7", "wall": "0"}
2022-07-01 00:22:22 | INFO | train_inner | {"epoch": 19, "update": 18.69, "loss": "1.442", "ppl": "2.72", "wps": "16391.3", "ups": "0.09", "wpb": "185206", "bsz": "408.1", "num_updates": "24200", "lr": "0.000381021", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-07-01 01:00:01 | INFO | train_inner | {"epoch": 19, "update": 18.736, "loss": "1.44", "ppl": "2.71", "wps": "16385.6", "ups": "0.09", "wpb": "185131", "bsz": "408.8", "num_updates": "24400", "lr": "0.000380861", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-07-01 01:37:43 | INFO | train_inner | {"epoch": 19, "update": 18.781, "loss": "1.44", "ppl": "2.71", "wps": "16388.9", "ups": "0.09", "wpb": "185290", "bsz": "409.2", "num_updates": "24600", "lr": "0.000380701", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-07-01 01:56:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-01 02:15:35 | INFO | train_inner | {"epoch": 19, "update": 18.827, "loss": "1.438", "ppl": "2.71", "wps": "16300.8", "ups": "0.09", "wpb": "185231", "bsz": "408", "num_updates": "24800", "lr": "0.000380541", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2183", "gb_free": "6.7", "wall": "0"}
2022-07-01 02:53:16 | INFO | train_inner | {"epoch": 19, "update": 18.873, "loss": "1.44", "ppl": "2.71", "wps": "16386.3", "ups": "0.09", "wpb": "185247", "bsz": "408.4", "num_updates": "25000", "lr": "0.00038038", "gnorm": "0.047", "clip": "0", "loss_scale": "32", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-07-01 03:31:00 | INFO | train_inner | {"epoch": 19, "update": 18.919, "loss": "1.441", "ppl": "2.71", "wps": "16370.7", "ups": "0.09", "wpb": "185298", "bsz": "408.3", "num_updates": "25200", "lr": "0.00038022", "gnorm": "0.047", "clip": "0", "loss_scale": "32", "train_wall": "2174", "gb_free": "6.7", "wall": "0"}
2022-07-01 03:37:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-01 04:08:56 | INFO | train_inner | {"epoch": 19, "update": 18.965, "loss": "1.441", "ppl": "2.71", "wps": "16287.7", "ups": "0.09", "wpb": "185337", "bsz": "407.9", "num_updates": "25400", "lr": "0.00038006", "gnorm": "0.047", "clip": "0", "loss_scale": "32", "train_wall": "2186", "gb_free": "6.7", "wall": "0"}
2022-07-01 04:38:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:38:29 | INFO | valid | {"epoch": 19, "valid_loss": "1.411", "valid_ppl": "2.66", "valid_wps": "36282", "valid_wpb": "5748.7", "valid_bsz": "12.7", "valid_num_updates": "25555", "valid_best_loss": "0.85"}
2022-07-01 04:38:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 25555 updates
2022-07-01 04:38:29 | INFO | fairseq.trainer | Saving checkpoint to /home/dahmanir/checkpoints/checkpoint19.pt
2022-07-01 04:38:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/dahmanir/checkpoints/checkpoint19.pt
2022-07-01 04:38:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint19.pt (epoch 19 @ 25555 updates, score 1.411) (writing took 3.108712033368647 seconds)
2022-07-01 04:38:32 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-07-01 04:38:32 | INFO | train | {"epoch": 19, "train_loss": "1.441", "train_ppl": "2.72", "train_wps": "16336.6", "train_ups": "0.09", "train_wpb": "185219", "train_bsz": "408.5", "train_num_updates": "25555", "train_lr": "0.000379936", "train_gnorm": "0.047", "train_clip": "0", "train_loss_scale": "64", "train_train_wall": "47523", "train_gb_free": "6.7", "train_wall": "0"}
2022-07-01 04:38:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4377
2022-07-01 04:38:33 | INFO | fairseq.trainer | begin training epoch 20
2022-07-01 04:38:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-07-01 04:47:06 | INFO | train_inner | {"epoch": 20, "update": 19.01, "loss": "1.441", "ppl": "2.71", "wps": "16163.6", "ups": "0.09", "wpb": "185114", "bsz": "408.2", "num_updates": "25600", "lr": "0.0003799", "gnorm": "0.047", "clip": "0", "loss_scale": "64", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-07-01 05:07:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-01 05:25:02 | INFO | train_inner | {"epoch": 20, "update": 19.056, "loss": "1.437", "ppl": "2.71", "wps": "16271.8", "ups": "0.09", "wpb": "185184", "bsz": "408.7", "num_updates": "25800", "lr": "0.00037974", "gnorm": "0.047", "clip": "0", "loss_scale": "32", "train_wall": "2183", "gb_free": "6.7", "wall": "0"}
2022-07-01 05:28:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-01 05:38:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-01 05:56:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-07-01 06:03:16 | INFO | train_inner | {"epoch": 20, "update": 19.103, "loss": "1.437", "ppl": "2.71", "wps": "16146.3", "ups": "0.09", "wpb": "185184", "bsz": "408.6", "num_updates": "26000", "lr": "0.00037958", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2201", "gb_free": "6.7", "wall": "0"}
2022-07-01 06:40:55 | INFO | train_inner | {"epoch": 20, "update": 19.148, "loss": "1.438", "ppl": "2.71", "wps": "16399.4", "ups": "0.09", "wpb": "185198", "bsz": "407.5", "num_updates": "26200", "lr": "0.000379419", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2168", "gb_free": "6.7", "wall": "0"}
2022-07-01 07:18:33 | INFO | train_inner | {"epoch": 20, "update": 19.194, "loss": "1.435", "ppl": "2.7", "wps": "16394.4", "ups": "0.09", "wpb": "185117", "bsz": "409.3", "num_updates": "26400", "lr": "0.000379259", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2168", "gb_free": "6.7", "wall": "0"}
2022-07-01 07:56:16 | INFO | train_inner | {"epoch": 20, "update": 19.24, "loss": "1.439", "ppl": "2.71", "wps": "16380.8", "ups": "0.09", "wpb": "185346", "bsz": "408.5", "num_updates": "26600", "lr": "0.000379099", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-07-01 08:17:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-01 08:34:08 | INFO | train_inner | {"epoch": 20, "update": 19.286, "loss": "1.436", "ppl": "2.71", "wps": "16301.8", "ups": "0.09", "wpb": "185166", "bsz": "408.2", "num_updates": "26800", "lr": "0.000378939", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2180", "gb_free": "6.7", "wall": "0"}
2022-07-01 09:11:48 | INFO | train_inner | {"epoch": 20, "update": 19.331, "loss": "1.439", "ppl": "2.71", "wps": "16403", "ups": "0.09", "wpb": "185332", "bsz": "408.4", "num_updates": "27000", "lr": "0.000378779", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2169", "gb_free": "6.7", "wall": "0"}
2022-07-01 09:49:29 | INFO | train_inner | {"epoch": 20, "update": 19.377, "loss": "1.437", "ppl": "2.71", "wps": "16380.2", "ups": "0.09", "wpb": "185204", "bsz": "408.6", "num_updates": "27200", "lr": "0.000378619", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-07-01 10:27:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-01 10:27:21 | INFO | train_inner | {"epoch": 20, "update": 19.423, "loss": "1.436", "ppl": "2.71", "wps": "16303.5", "ups": "0.09", "wpb": "185240", "bsz": "407.8", "num_updates": "27400", "lr": "0.000378458", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2181", "gb_free": "6.7", "wall": "0"}
2022-07-01 11:05:03 | INFO | train_inner | {"epoch": 20, "update": 19.469, "loss": "1.438", "ppl": "2.71", "wps": "16388.6", "ups": "0.09", "wpb": "185297", "bsz": "408.3", "num_updates": "27600", "lr": "0.000378298", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-07-01 11:32:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-01 11:42:55 | INFO | train_inner | {"epoch": 20, "update": 19.515, "loss": "1.437", "ppl": "2.71", "wps": "16295.9", "ups": "0.09", "wpb": "185192", "bsz": "408.7", "num_updates": "27800", "lr": "0.000378138", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-07-01 11:44:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-01 12:20:47 | INFO | train_inner | {"epoch": 20, "update": 19.56, "loss": "1.437", "ppl": "2.71", "wps": "16309.7", "ups": "0.09", "wpb": "185243", "bsz": "408.6", "num_updates": "28000", "lr": "0.000377978", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2180", "gb_free": "6.7", "wall": "0"}
2022-07-01 12:58:28 | INFO | train_inner | {"epoch": 20, "update": 19.606, "loss": "1.437", "ppl": "2.71", "wps": "16386.2", "ups": "0.09", "wpb": "185236", "bsz": "408.6", "num_updates": "28200", "lr": "0.000377818", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-07-01 13:00:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-01 13:36:15 | INFO | train_inner | {"epoch": 20, "update": 19.652, "loss": "1.436", "ppl": "2.71", "wps": "16329", "ups": "0.09", "wpb": "185109", "bsz": "409.1", "num_updates": "28400", "lr": "0.000377658", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2176", "gb_free": "6.7", "wall": "0"}
2022-07-01 14:13:56 | INFO | train_inner | {"epoch": 20, "update": 19.698, "loss": "1.437", "ppl": "2.71", "wps": "16389.9", "ups": "0.09", "wpb": "185310", "bsz": "408.4", "num_updates": "28600", "lr": "0.000377497", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-07-01 14:51:38 | INFO | train_inner | {"epoch": 20, "update": 19.743, "loss": "1.437", "ppl": "2.71", "wps": "16387", "ups": "0.09", "wpb": "185263", "bsz": "408.6", "num_updates": "28800", "lr": "0.000377337", "gnorm": "0.047", "clip": "0", "loss_scale": "32", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-07-01 15:24:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-01 15:29:29 | INFO | train_inner | {"epoch": 20, "update": 19.789, "loss": "1.436", "ppl": "2.71", "wps": "16312.8", "ups": "0.09", "wpb": "185249", "bsz": "408.1", "num_updates": "29000", "lr": "0.000377177", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2181", "gb_free": "6.7", "wall": "0"}
2022-07-01 15:30:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-01 16:07:18 | INFO | train_inner | {"epoch": 20, "update": 19.835, "loss": "1.435", "ppl": "2.7", "wps": "16325.9", "ups": "0.09", "wpb": "185247", "bsz": "408.9", "num_updates": "29200", "lr": "0.000377017", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2179", "gb_free": "6.7", "wall": "0"}
2022-07-01 16:44:59 | INFO | train_inner | {"epoch": 20, "update": 19.881, "loss": "1.436", "ppl": "2.71", "wps": "16390.5", "ups": "0.09", "wpb": "185300", "bsz": "408.5", "num_updates": "29400", "lr": "0.000376857", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-07-01 17:05:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-01 17:22:50 | INFO | train_inner | {"epoch": 20, "update": 19.927, "loss": "1.437", "ppl": "2.71", "wps": "16317.4", "ups": "0.09", "wpb": "185265", "bsz": "408.2", "num_updates": "29600", "lr": "0.000376697", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2181", "gb_free": "6.7", "wall": "0"}
2022-07-01 18:00:27 | INFO | train_inner | {"epoch": 20, "update": 19.973, "loss": "1.436", "ppl": "2.71", "wps": "16403.3", "ups": "0.09", "wpb": "185108", "bsz": "408.9", "num_updates": "29800", "lr": "0.000376537", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2167", "gb_free": "6.7", "wall": "0"}
2022-07-01 18:23:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 18:23:21 | INFO | valid | {"epoch": 20, "valid_loss": "1.399", "valid_ppl": "2.64", "valid_wps": "36255.3", "valid_wpb": "5748.7", "valid_bsz": "12.7", "valid_num_updates": "29920", "valid_best_loss": "0.85"}
2022-07-01 18:23:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29920 updates
2022-07-01 18:23:21 | INFO | fairseq.trainer | Saving checkpoint to /home/dahmanir/checkpoints/checkpoint20.pt
2022-07-01 18:23:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/dahmanir/checkpoints/checkpoint20.pt
2022-07-01 18:23:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint20.pt (epoch 20 @ 29920 updates, score 1.399) (writing took 3.0598235856741667 seconds)
2022-07-01 18:23:25 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-07-01 18:23:25 | INFO | train | {"epoch": 20, "train_loss": "1.437", "train_ppl": "2.71", "train_wps": "16335.4", "train_ups": "0.09", "train_wpb": "185218", "train_bsz": "408.5", "train_num_updates": "29920", "train_lr": "0.00037644", "train_gnorm": "0.047", "train_clip": "0", "train_loss_scale": "16", "train_train_wall": "47480", "train_gb_free": "6.7", "train_wall": "0"}
2022-07-01 18:23:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4377
2022-07-01 18:23:25 | INFO | fairseq.trainer | begin training epoch 21
2022-07-01 18:23:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-07-01 18:38:32 | INFO | train_inner | {"epoch": 21, "update": 20.018, "loss": "1.436", "ppl": "2.71", "wps": "16195.1", "ups": "0.09", "wpb": "185071", "bsz": "407.8", "num_updates": "30000", "lr": "0.000376376", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2166", "gb_free": "6.6", "wall": "0"}
2022-07-01 18:54:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-01 19:16:23 | INFO | train_inner | {"epoch": 21, "update": 20.064, "loss": "1.433", "ppl": "2.7", "wps": "16308.9", "ups": "0.09", "wpb": "185135", "bsz": "409.3", "num_updates": "30200", "lr": "0.000376216", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2181", "gb_free": "6.7", "wall": "0"}
2022-07-01 19:54:03 | INFO | train_inner | {"epoch": 21, "update": 20.11, "loss": "1.433", "ppl": "2.7", "wps": "16385.3", "ups": "0.09", "wpb": "185138", "bsz": "408.5", "num_updates": "30400", "lr": "0.000376056", "gnorm": "0.047", "clip": "0", "loss_scale": "32", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-07-01 20:31:45 | INFO | train_inner | {"epoch": 21, "update": 20.156, "loss": "1.433", "ppl": "2.7", "wps": "16378.4", "ups": "0.09", "wpb": "185262", "bsz": "408.2", "num_updates": "30600", "lr": "0.000375896", "gnorm": "0.047", "clip": "0", "loss_scale": "64", "train_wall": "2174", "gb_free": "6.7", "wall": "0"}
2022-07-01 20:32:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-01 21:03:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-01 21:09:48 | INFO | train_inner | {"epoch": 21, "update": 20.202, "loss": "1.433", "ppl": "2.7", "wps": "16216.7", "ups": "0.09", "wpb": "185134", "bsz": "409.2", "num_updates": "30800", "lr": "0.000375736", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2194", "gb_free": "6.7", "wall": "0"}
2022-07-01 21:47:29 | INFO | train_inner | {"epoch": 21, "update": 20.247, "loss": "1.434", "ppl": "2.7", "wps": "16393.7", "ups": "0.09", "wpb": "185287", "bsz": "408.9", "num_updates": "31000", "lr": "0.000375576", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-07-01 21:59:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-01 22:02:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-01 22:25:30 | INFO | train_inner | {"epoch": 21, "update": 20.294, "loss": "1.433", "ppl": "2.7", "wps": "16233.5", "ups": "0.09", "wpb": "185142", "bsz": "408.5", "num_updates": "31200", "lr": "0.000375415", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2192", "gb_free": "6.7", "wall": "0"}
2022-07-01 23:03:07 | INFO | train_inner | {"epoch": 21, "update": 20.339, "loss": "1.435", "ppl": "2.7", "wps": "16395", "ups": "0.09", "wpb": "185033", "bsz": "409", "num_updates": "31400", "lr": "0.000375255", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2169", "gb_free": "6.7", "wall": "0"}
2022-07-01 23:40:47 | INFO | train_inner | {"epoch": 21, "update": 20.385, "loss": "1.433", "ppl": "2.7", "wps": "16392.6", "ups": "0.09", "wpb": "185269", "bsz": "408", "num_updates": "31600", "lr": "0.000375095", "gnorm": "0.047", "clip": "0", "loss_scale": "32", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-07-01 23:57:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-02 00:12:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-02 00:18:50 | INFO | train_inner | {"epoch": 21, "update": 20.431, "loss": "1.435", "ppl": "2.7", "wps": "16235.9", "ups": "0.09", "wpb": "185305", "bsz": "408.4", "num_updates": "31800", "lr": "0.000374935", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2193", "gb_free": "6.7", "wall": "0"}
2022-07-02 00:22:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-07-02 00:56:40 | INFO | train_inner | {"epoch": 21, "update": 20.477, "loss": "1.434", "ppl": "2.7", "wps": "16331.6", "ups": "0.09", "wpb": "185338", "bsz": "408.7", "num_updates": "32000", "lr": "0.000374775", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2181", "gb_free": "6.7", "wall": "0"}
2022-07-02 01:34:20 | INFO | train_inner | {"epoch": 21, "update": 20.523, "loss": "1.434", "ppl": "2.7", "wps": "16400.8", "ups": "0.09", "wpb": "185336", "bsz": "408.2", "num_updates": "32200", "lr": "0.000374615", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-07-02 02:11:58 | INFO | train_inner | {"epoch": 21, "update": 20.568, "loss": "1.434", "ppl": "2.7", "wps": "16407", "ups": "0.09", "wpb": "185299", "bsz": "407.6", "num_updates": "32400", "lr": "0.000374454", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-07-02 02:29:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-02 02:49:51 | INFO | train_inner | {"epoch": 21, "update": 20.614, "loss": "1.434", "ppl": "2.7", "wps": "16308.3", "ups": "0.09", "wpb": "185285", "bsz": "407.8", "num_updates": "32600", "lr": "0.000374294", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2184", "gb_free": "6.7", "wall": "0"}
2022-07-02 03:03:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-07-02 03:27:38 | INFO | train_inner | {"epoch": 21, "update": 20.66, "loss": "1.433", "ppl": "2.7", "wps": "16322", "ups": "0.09", "wpb": "185044", "bsz": "408.3", "num_updates": "32800", "lr": "0.000374134", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2179", "gb_free": "6.7", "wall": "0"}
2022-07-02 04:05:14 | INFO | train_inner | {"epoch": 21, "update": 20.706, "loss": "1.435", "ppl": "2.7", "wps": "16412.3", "ups": "0.09", "wpb": "185083", "bsz": "408.9", "num_updates": "33000", "lr": "0.000373974", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2167", "gb_free": "6.7", "wall": "0"}
2022-07-02 04:42:53 | INFO | train_inner | {"epoch": 21, "update": 20.752, "loss": "1.434", "ppl": "2.7", "wps": "16406.7", "ups": "0.09", "wpb": "185340", "bsz": "408.4", "num_updates": "33200", "lr": "0.000373814", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-07-02 05:12:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-02 05:20:46 | INFO | train_inner | {"epoch": 21, "update": 20.798, "loss": "1.433", "ppl": "2.7", "wps": "16311.6", "ups": "0.09", "wpb": "185368", "bsz": "408.8", "num_updates": "33400", "lr": "0.000373654", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2184", "gb_free": "6.7", "wall": "0"}
2022-07-02 05:29:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-07-02 05:58:33 | INFO | train_inner | {"epoch": 21, "update": 20.844, "loss": "1.435", "ppl": "2.7", "wps": "16337.5", "ups": "0.09", "wpb": "185170", "bsz": "408.7", "num_updates": "33600", "lr": "0.000373493", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2178", "gb_free": "6.7", "wall": "0"}
2022-07-02 06:36:10 | INFO | train_inner | {"epoch": 21, "update": 20.889, "loss": "1.434", "ppl": "2.7", "wps": "16408.3", "ups": "0.09", "wpb": "185214", "bsz": "408", "num_updates": "33800", "lr": "0.000373333", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2169", "gb_free": "6.7", "wall": "0"}
2022-07-02 07:13:50 | INFO | train_inner | {"epoch": 21, "update": 20.935, "loss": "1.434", "ppl": "2.7", "wps": "16396.5", "ups": "0.09", "wpb": "185256", "bsz": "408.7", "num_updates": "34000", "lr": "0.000373173", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-07-02 07:20:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-02 07:51:42 | INFO | train_inner | {"epoch": 21, "update": 20.981, "loss": "1.434", "ppl": "2.7", "wps": "16313.5", "ups": "0.09", "wpb": "185304", "bsz": "408.6", "num_updates": "34200", "lr": "0.000373013", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2183", "gb_free": "6.7", "wall": "0"}
2022-07-02 08:07:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-02 08:07:49 | INFO | valid | {"epoch": 21, "valid_loss": "1.4", "valid_ppl": "2.64", "valid_wps": "36284.8", "valid_wpb": "5748.7", "valid_bsz": "12.7", "valid_num_updates": "34284", "valid_best_loss": "0.85"}
2022-07-02 08:07:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 34284 updates
2022-07-02 08:07:49 | INFO | fairseq.trainer | Saving checkpoint to /home/dahmanir/checkpoints/checkpoint21.pt
2022-07-02 08:07:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/dahmanir/checkpoints/checkpoint21.pt
2022-07-02 08:07:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint21.pt (epoch 21 @ 34284 updates, score 1.4) (writing took 3.0906860288232565 seconds)
2022-07-02 08:07:53 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-07-02 08:07:53 | INFO | train | {"epoch": 21, "train_loss": "1.434", "train_ppl": "2.7", "train_wps": "16339.7", "train_ups": "0.09", "train_wpb": "185218", "train_bsz": "408.5", "train_num_updates": "34284", "train_lr": "0.000372946", "train_gnorm": "0.047", "train_clip": "0", "train_loss_scale": "8", "train_train_wall": "47507", "train_gb_free": "6.7", "train_wall": "0"}
2022-07-02 08:07:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4377
2022-07-02 08:07:53 | INFO | fairseq.trainer | begin training epoch 22
2022-07-02 08:07:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-07-02 08:29:50 | INFO | train_inner | {"epoch": 22, "update": 21.027, "loss": "1.43", "ppl": "2.69", "wps": "16176.9", "ups": "0.09", "wpb": "185067", "bsz": "408.3", "num_updates": "34400", "lr": "0.000372853", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2169", "gb_free": "6.7", "wall": "0"}
2022-07-02 08:39:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-02 09:07:42 | INFO | train_inner | {"epoch": 22, "update": 21.072, "loss": "1.43", "ppl": "2.69", "wps": "16305.1", "ups": "0.09", "wpb": "185278", "bsz": "408.5", "num_updates": "34600", "lr": "0.000372693", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2182", "gb_free": "6.7", "wall": "0"}
2022-07-02 09:45:24 | INFO | train_inner | {"epoch": 22, "update": 21.118, "loss": "1.431", "ppl": "2.7", "wps": "16392.7", "ups": "0.09", "wpb": "185369", "bsz": "408.7", "num_updates": "34800", "lr": "0.000372533", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-07-02 10:17:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-02 10:23:16 | INFO | train_inner | {"epoch": 22, "update": 21.164, "loss": "1.433", "ppl": "2.7", "wps": "16299.8", "ups": "0.09", "wpb": "185198", "bsz": "408.6", "num_updates": "35000", "lr": "0.000372372", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2183", "gb_free": "6.7", "wall": "0"}
2022-07-02 11:00:56 | INFO | train_inner | {"epoch": 22, "update": 21.21, "loss": "1.429", "ppl": "2.69", "wps": "16387.6", "ups": "0.09", "wpb": "185153", "bsz": "408.8", "num_updates": "35200", "lr": "0.000372212", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-07-02 11:13:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-02 11:31:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-02 11:39:00 | INFO | train_inner | {"epoch": 22, "update": 21.256, "loss": "1.433", "ppl": "2.7", "wps": "16218.6", "ups": "0.09", "wpb": "185186", "bsz": "408.4", "num_updates": "35400", "lr": "0.000372052", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2193", "gb_free": "6.7", "wall": "0"}
2022-07-02 12:05:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-07-02 12:16:49 | INFO | train_inner | {"epoch": 22, "update": 21.302, "loss": "1.431", "ppl": "2.7", "wps": "16319.9", "ups": "0.09", "wpb": "185147", "bsz": "409.5", "num_updates": "35600", "lr": "0.000371892", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2179", "gb_free": "6.7", "wall": "0"}
2022-07-02 12:54:28 | INFO | train_inner | {"epoch": 22, "update": 21.347, "loss": "1.432", "ppl": "2.7", "wps": "16404", "ups": "0.09", "wpb": "185359", "bsz": "408.1", "num_updates": "35800", "lr": "0.000371732", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2170", "gb_free": "6.8", "wall": "0"}
2022-07-02 13:13:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-07-02 13:32:17 | INFO | train_inner | {"epoch": 22, "update": 21.393, "loss": "1.43", "ppl": "2.69", "wps": "16328", "ups": "0.09", "wpb": "185218", "bsz": "408.6", "num_updates": "36000", "lr": "0.000371572", "gnorm": "0.047", "clip": "0", "loss_scale": "4", "train_wall": "2179", "gb_free": "6.8", "wall": "0"}
2022-07-02 14:09:54 | INFO | train_inner | {"epoch": 22, "update": 21.439, "loss": "1.432", "ppl": "2.7", "wps": "16414", "ups": "0.09", "wpb": "185188", "bsz": "409", "num_updates": "36200", "lr": "0.000371411", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2167", "gb_free": "6.7", "wall": "0"}
2022-07-02 14:47:32 | INFO | train_inner | {"epoch": 22, "update": 21.485, "loss": "1.432", "ppl": "2.7", "wps": "16400.6", "ups": "0.09", "wpb": "185152", "bsz": "408.6", "num_updates": "36400", "lr": "0.000371251", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2169", "gb_free": "6.7", "wall": "0"}
2022-07-02 14:48:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-07-02 15:25:19 | INFO | train_inner | {"epoch": 22, "update": 21.531, "loss": "1.432", "ppl": "2.7", "wps": "16325.8", "ups": "0.09", "wpb": "185105", "bsz": "409.2", "num_updates": "36600", "lr": "0.000371091", "gnorm": "0.047", "clip": "0.5", "loss_scale": "4", "train_wall": "2178", "gb_free": "6.7", "wall": "0"}
2022-07-02 16:02:58 | INFO | train_inner | {"epoch": 22, "update": 21.576, "loss": "1.431", "ppl": "2.7", "wps": "16401.8", "ups": "0.09", "wpb": "185213", "bsz": "408.6", "num_updates": "36800", "lr": "0.000370931", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2169", "gb_free": "6.7", "wall": "0"}
2022-07-02 16:40:37 | INFO | train_inner | {"epoch": 22, "update": 21.622, "loss": "1.431", "ppl": "2.7", "wps": "16395.2", "ups": "0.09", "wpb": "185241", "bsz": "409.1", "num_updates": "37000", "lr": "0.000370771", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2170", "gb_free": "6.7", "wall": "0"}
2022-07-02 16:50:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-02 17:18:30 | INFO | train_inner | {"epoch": 22, "update": 21.668, "loss": "1.432", "ppl": "2.7", "wps": "16311.6", "ups": "0.09", "wpb": "185371", "bsz": "408.5", "num_updates": "37200", "lr": "0.000370611", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2183", "gb_free": "6.7", "wall": "0"}
2022-07-02 17:46:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-07-02 17:56:21 | INFO | train_inner | {"epoch": 22, "update": 21.714, "loss": "1.432", "ppl": "2.7", "wps": "16314.6", "ups": "0.09", "wpb": "185268", "bsz": "407.4", "num_updates": "37400", "lr": "0.00037045", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2181", "gb_free": "6.7", "wall": "0"}
2022-07-02 18:34:02 | INFO | train_inner | {"epoch": 22, "update": 21.76, "loss": "1.431", "ppl": "2.7", "wps": "16381", "ups": "0.09", "wpb": "185142", "bsz": "408", "num_updates": "37600", "lr": "0.00037029", "gnorm": "0.047", "clip": "0", "loss_scale": "8", "train_wall": "2171", "gb_free": "6.7", "wall": "0"}
2022-07-02 19:11:43 | INFO | train_inner | {"epoch": 22, "update": 21.805, "loss": "1.431", "ppl": "2.7", "wps": "16379.2", "ups": "0.09", "wpb": "185205", "bsz": "408.6", "num_updates": "37800", "lr": "0.00037013", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2172", "gb_free": "6.7", "wall": "0"}
2022-07-02 19:23:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-07-02 19:49:36 | INFO | train_inner | {"epoch": 22, "update": 21.851, "loss": "1.433", "ppl": "2.7", "wps": "16295.4", "ups": "0.09", "wpb": "185207", "bsz": "407.1", "num_updates": "38000", "lr": "0.00036997", "gnorm": "0.047", "clip": "0", "loss_scale": "16", "train_wall": "2184", "gb_free": "6.7", "wall": "0"}
slurmstepd: error: *** JOB 9649326 ON r32n5 CANCELLED AT 2022-07-02T20:04:26 DUE TO TIME LIMIT ***
