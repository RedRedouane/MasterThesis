[2022-07-18 11:28:43,450][HYDRA] Launching 1 jobs locally
[2022-07-18 11:28:43,451][HYDRA] 	#0 : 
2022-07-18 11:28:50 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:18747
2022-07-18 11:28:50 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:18747
2022-07-18 11:28:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-18 11:28:50 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-18 11:28:50 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2022-07-18 11:28:50 | INFO | fairseq.distributed.utils | initialized host r31n3.lisa.surfsara.nl as rank 0
2022-07-18 11:28:50 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2022-07-18 11:28:50 | INFO | fairseq.distributed.utils | initialized host r31n3.lisa.surfsara.nl as rank 1
[2022-07-18 11:28:56,766][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'RoBERTa', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18747', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': True, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 125000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': '/home/dahmanir/lisa/Models/RoBERTa_50_finetune.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'roberta', 'max_positions': 512, 'dropout': 0.1, 'attention_dropout': 0.1}, 'task': {'_name': 'masked_lm', 'data': '/home/dahmanir/lisa/Datasets/second_50_percent', 'sample_break_mode': complete, 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': none, 'shorten_data_split_list': '', 'seed': 1, 'include_target_tokens': False}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 125000.0, 'lr': [1e-05]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2022-07-18 11:28:56,864][fairseq.tasks.masked_lm][INFO] - dictionary: 39984 types
[2022-07-18 11:29:02,111][fairseq_cli.train][INFO] - RobertaModel(
  (encoder): RobertaEncoder(
    (sentence_encoder): TransformerEncoder(
      (dropout_module): FairseqDropout()
      (embed_tokens): Embedding(39985, 768, padding_idx=1)
      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)
      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (lm_head): RobertaLMHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (classification_heads): ModuleDict()
)
[2022-07-18 11:29:02,115][fairseq_cli.train][INFO] - task: MaskedLMTask
[2022-07-18 11:29:02,116][fairseq_cli.train][INFO] - model: RobertaModel
[2022-07-18 11:29:02,116][fairseq_cli.train][INFO] - criterion: MaskedLmLoss
[2022-07-18 11:29:02,118][fairseq_cli.train][INFO] - num. shared model params: 116,791,345 (num. trained: 116,791,345)
[2022-07-18 11:29:02,119][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2022-07-18 11:29:02,143][fairseq.data.data_utils][INFO] - loaded 10,000 examples from: /home/dahmanir/lisa/Datasets/second_50_percent/valid
[2022-07-18 11:29:02,157][fairseq.tasks.masked_lm][INFO] - loaded 1441 blocks from: /home/dahmanir/lisa/Datasets/second_50_percent/valid
[2022-07-18 11:29:04,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:2 to store for rank: 0
[2022-07-18 11:29:05,478][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.
[2022-07-18 11:29:05,478][fairseq.trainer][INFO] - detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight
[2022-07-18 11:29:05,604][fairseq.utils][INFO] - ***********************CUDA enviroments for all 2 workers***********************
[2022-07-18 11:29:05,605][fairseq.utils][INFO] - rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = NVIDIA GeForce GTX 1080 Ti              
[2022-07-18 11:29:05,605][fairseq.utils][INFO] - rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = NVIDIA GeForce GTX 1080 Ti              
[2022-07-18 11:29:05,605][fairseq.utils][INFO] - ***********************CUDA enviroments for all 2 workers***********************
[2022-07-18 11:29:05,605][fairseq_cli.train][INFO] - training on 2 devices (GPUs/TPUs)
[2022-07-18 11:29:05,605][fairseq_cli.train][INFO] - max tokens per device = None and max sentences per device = 8
[2022-07-18 11:29:05,607][fairseq.trainer][INFO] - Preparing to load checkpoint /home/dahmanir/lisa/Models/RoBERTa_50_finetune.pt
[2022-07-18 11:29:11,437][fairseq.trainer][INFO] - NOTE: your device does NOT support faster training with --fp16 or --amp, please switch to FP32 which is likely to be faster
[2022-07-18 11:29:12,006][fairseq.trainer][INFO] - Loaded checkpoint /home/dahmanir/lisa/Models/RoBERTa_50_finetune.pt (epoch 2 @ 17748 updates)
[2022-07-18 11:29:12,007][fairseq.trainer][INFO] - loading train data for epoch 2
[2022-07-18 11:29:20,033][fairseq.data.data_utils][INFO] - loaded 62,772,775 examples from: /home/dahmanir/lisa/Datasets/second_50_percent/train
[2022-07-18 11:29:27,899][fairseq.tasks.masked_lm][INFO] - loaded 9340602 blocks from: /home/dahmanir/lisa/Datasets/second_50_percent/train
2022-07-18 11:29:30 | WARNING | fairseq.tasks.fairseq_task | 2,809 samples have invalid sizes and will be skipped, max_positions=512, first few sample ids=[6964576, 8576312, 1154077, 3948181, 3959698, 8854795, 6584129, 6184654, 2746599, 1419102]
[2022-07-18 11:29:32,565][fairseq.tasks.fairseq_task][WARNING] - 2,809 samples have invalid sizes and will be skipped, max_positions=512, first few sample ids=[6964576, 8576312, 1154077, 3948181, 3959698, 8854795, 6584129, 6184654, 2746599, 1419102]
[2022-07-18 11:29:49,998][fairseq.data.iterators][INFO] - grouped total_num_itrs = 18238
wandb: Currently logged in as: redredouane. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.21 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.19
wandb: Run data is saved locally in /home/dahmanir/multirun/2022-07-18/11-28-41/0/wandb/run-20220718_112958-3uie729i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run checkpoints
wandb:  View project at https://wandb.ai/redredouane/RoBERTa
wandb:  View run at https://wandb.ai/redredouane/RoBERTa/runs/3uie729i
[2022-07-18 11:30:06,428][fairseq.trainer][INFO] - begin training epoch 2
[2022-07-18 11:30:06,457][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-18 11:30:35,966][root][INFO] - Reducer buckets have been rebuilt in this iteration.
[2022-07-18 11:42:29,783][train_inner][INFO] - {"epoch": 2, "update": 1.003, "loss": "2.526", "ppl": "5.76", "wps": "16726.8", "ups": "0.07", "wpb": "234284", "bsz": "512", "num_updates": "17800", "lr": "8.576e-06", "gnorm": "1.512", "loss_scale": "2", "train_wall": "692", "gb_free": "5.4", "wall": "804"}
[2022-07-18 12:28:33,480][train_inner][INFO] - {"epoch": 2, "update": 1.014, "loss": "2.53", "ppl": "5.77", "wps": "16945.2", "ups": "0.07", "wpb": "234156", "bsz": "512", "num_updates": "18000", "lr": "8.56e-06", "gnorm": "1.427", "loss_scale": "2", "train_wall": "2631", "gb_free": "5.4", "wall": "3568"}
[2022-07-18 13:02:17,572][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-18 13:15:12,347][train_inner][INFO] - {"epoch": 2, "update": 1.025, "loss": "2.534", "ppl": "5.79", "wps": "16748.3", "ups": "0.07", "wpb": "234380", "bsz": "512", "num_updates": "18200", "lr": "8.544e-06", "gnorm": "1.437", "loss_scale": "2", "train_wall": "2662", "gb_free": "5.4", "wall": "6367"}
[2022-07-18 14:01:39,331][train_inner][INFO] - {"epoch": 2, "update": 1.036, "loss": "2.526", "ppl": "5.76", "wps": "16825.9", "ups": "0.07", "wpb": "234466", "bsz": "512", "num_updates": "18400", "lr": "8.528e-06", "gnorm": "1.46", "loss_scale": "2", "train_wall": "2651", "gb_free": "5.4", "wall": "9154"}
[2022-07-18 14:17:08,498][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-18 14:48:17,612][train_inner][INFO] - {"epoch": 2, "update": 1.047, "loss": "2.529", "ppl": "5.77", "wps": "16770.9", "ups": "0.07", "wpb": "234648", "bsz": "512", "num_updates": "18600", "lr": "8.512e-06", "gnorm": "1.408", "loss_scale": "2", "train_wall": "2668", "gb_free": "5.4", "wall": "11952"}
[2022-07-18 15:34:19,143][train_inner][INFO] - {"epoch": 2, "update": 1.058, "loss": "2.526", "ppl": "5.76", "wps": "16950.9", "ups": "0.07", "wpb": "234052", "bsz": "512", "num_updates": "18800", "lr": "8.496e-06", "gnorm": "1.44", "loss_scale": "4", "train_wall": "2634", "gb_free": "5.4", "wall": "14714"}
[2022-07-18 15:39:45,740][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-18 16:02:03,866][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-18 16:21:01,725][train_inner][INFO] - {"epoch": 2, "update": 1.069, "loss": "2.526", "ppl": "5.76", "wps": "16749.1", "ups": "0.07", "wpb": "234703", "bsz": "512", "num_updates": "19000", "lr": "8.48e-06", "gnorm": "1.447", "loss_scale": "1", "train_wall": "2672", "gb_free": "5.4", "wall": "17516"}
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-18 17:07:12,313][train_inner][INFO] - {"epoch": 2, "update": 1.08, "loss": "2.524", "ppl": "5.75", "wps": "16936.3", "ups": "0.07", "wpb": "234617", "bsz": "512", "num_updates": "19200", "lr": "8.464e-06", "gnorm": "1.456", "loss_scale": "2", "train_wall": "2642", "gb_free": "5.4", "wall": "20287"}
[2022-07-18 17:53:14,016][train_inner][INFO] - {"epoch": 2, "update": 1.091, "loss": "2.522", "ppl": "5.74", "wps": "17020", "ups": "0.07", "wpb": "235020", "bsz": "512", "num_updates": "19400", "lr": "8.448e-06", "gnorm": "1.401", "loss_scale": "2", "train_wall": "2632", "gb_free": "5.4", "wall": "23048"}
[2022-07-18 18:10:36,076][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-18 18:39:46,238][train_inner][INFO] - {"epoch": 2, "update": 1.102, "loss": "2.52", "ppl": "5.73", "wps": "16787.7", "ups": "0.07", "wpb": "234373", "bsz": "512", "num_updates": "19600", "lr": "8.432e-06", "gnorm": "1.432", "loss_scale": "2", "train_wall": "2654", "gb_free": "5.4", "wall": "25841"}
[2022-07-18 19:26:00,487][train_inner][INFO] - {"epoch": 2, "update": 1.113, "loss": "2.513", "ppl": "5.71", "wps": "16888.7", "ups": "0.07", "wpb": "234265", "bsz": "512", "num_updates": "19800", "lr": "8.416e-06", "gnorm": "1.446", "loss_scale": "4", "train_wall": "2645", "gb_free": "5.4", "wall": "28615"}
[2022-07-18 20:09:36,882][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-18 20:12:20,272][train_inner][INFO] - {"epoch": 2, "update": 1.124, "loss": "2.52", "ppl": "5.73", "wps": "16857.1", "ups": "0.07", "wpb": "234291", "bsz": "512", "num_updates": "20000", "lr": "8.4e-06", "gnorm": "1.416", "loss_scale": "4", "train_wall": "2650", "gb_free": "5.3", "wall": "31395"}
[2022-07-18 20:34:23,356][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-18 20:58:47,146][train_inner][INFO] - {"epoch": 2, "update": 1.135, "loss": "2.517", "ppl": "5.72", "wps": "16825.2", "ups": "0.07", "wpb": "234448", "bsz": "512", "num_updates": "20200", "lr": "8.384e-06", "gnorm": "1.397", "loss_scale": "2", "train_wall": "2657", "gb_free": "5.5", "wall": "34182"}
[2022-07-18 21:04:26,627][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-18 21:45:21,177][train_inner][INFO] - {"epoch": 2, "update": 1.146, "loss": "2.512", "ppl": "5.7", "wps": "16798", "ups": "0.07", "wpb": "234669", "bsz": "512", "num_updates": "20400", "lr": "8.368e-06", "gnorm": "1.415", "loss_scale": "1", "train_wall": "2662", "gb_free": "5.4", "wall": "36976"}
[2022-07-18 22:31:31,447][train_inner][INFO] - {"epoch": 2, "update": 1.157, "loss": "2.515", "ppl": "5.72", "wps": "16942.2", "ups": "0.07", "wpb": "234672", "bsz": "512", "num_updates": "20600", "lr": "8.352e-06", "gnorm": "1.452", "loss_scale": "2", "train_wall": "2644", "gb_free": "5.4", "wall": "39746"}
[2022-07-18 23:15:39,248][train_inner][INFO] - {"epoch": 2, "update": 1.168, "loss": "2.512", "ppl": "5.7", "wps": "17701.7", "ups": "0.08", "wpb": "234352", "bsz": "512", "num_updates": "20800", "lr": "8.336e-06", "gnorm": "1.429", "loss_scale": "4", "train_wall": "2538", "gb_free": "5.4", "wall": "42394"}
[2022-07-19 00:00:15,584][train_inner][INFO] - {"epoch": 2, "update": 1.179, "loss": "2.511", "ppl": "5.7", "wps": "17515.4", "ups": "0.07", "wpb": "234384", "bsz": "512", "num_updates": "21000", "lr": "8.32e-06", "gnorm": "1.425", "loss_scale": "8", "train_wall": "2557", "gb_free": "5.4", "wall": "45070"}
[2022-07-19 00:08:10,167][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 00:08:50,245][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 00:50:27,294][train_inner][INFO] - {"epoch": 2, "update": 1.19, "loss": "2.508", "ppl": "5.69", "wps": "16756.8", "ups": "0.07", "wpb": "234789", "bsz": "512", "num_updates": "21200", "lr": "8.304e-06", "gnorm": "1.443", "loss_scale": "2", "train_wall": "2673", "gb_free": "5.4", "wall": "48082"}
[2022-07-19 01:36:12,295][train_inner][INFO] - {"epoch": 2, "update": 1.201, "loss": "2.51", "ppl": "5.7", "wps": "17105.4", "ups": "0.07", "wpb": "234769", "bsz": "512", "num_updates": "21400", "lr": "8.288e-06", "gnorm": "1.427", "loss_scale": "4", "train_wall": "2620", "gb_free": "5.4", "wall": "50827"}
[2022-07-19 02:06:48,611][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 02:19:45,779][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 02:22:31,576][train_inner][INFO] - {"epoch": 2, "update": 1.212, "loss": "2.506", "ppl": "5.68", "wps": "16887.6", "ups": "0.07", "wpb": "234676", "bsz": "512", "num_updates": "21600", "lr": "8.272e-06", "gnorm": "1.419", "loss_scale": "2", "train_wall": "2644", "gb_free": "5.4", "wall": "53606"}
[2022-07-19 03:08:25,641][train_inner][INFO] - {"epoch": 2, "update": 1.223, "loss": "2.5", "ppl": "5.66", "wps": "17024.4", "ups": "0.07", "wpb": "234431", "bsz": "512", "num_updates": "21800", "lr": "8.256e-06", "gnorm": "1.448", "loss_scale": "2", "train_wall": "2623", "gb_free": "5.4", "wall": "56360"}
[2022-07-19 03:29:21,148][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 03:54:50,587][train_inner][INFO] - {"epoch": 2, "update": 1.234, "loss": "2.506", "ppl": "5.68", "wps": "16828.8", "ups": "0.07", "wpb": "234336", "bsz": "512", "num_updates": "22000", "lr": "8.24e-06", "gnorm": "1.453", "loss_scale": "2", "train_wall": "2651", "gb_free": "5.4", "wall": "59145"}
[2022-07-19 04:41:03,932][train_inner][INFO] - {"epoch": 2, "update": 1.245, "loss": "2.499", "ppl": "5.65", "wps": "16909.3", "ups": "0.07", "wpb": "234477", "bsz": "512", "num_updates": "22200", "lr": "8.224e-06", "gnorm": "1.404", "loss_scale": "4", "train_wall": "2643", "gb_free": "5.4", "wall": "61918"}
[2022-07-19 05:00:18,720][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 05:27:34,058][train_inner][INFO] - {"epoch": 2, "update": 1.256, "loss": "2.508", "ppl": "5.69", "wps": "16813.3", "ups": "0.07", "wpb": "234555", "bsz": "512", "num_updates": "22400", "lr": "8.208e-06", "gnorm": "1.374", "loss_scale": "2", "train_wall": "2658", "gb_free": "5.4", "wall": "64708"}
[2022-07-19 06:14:03,017][train_inner][INFO] - {"epoch": 2, "update": 1.267, "loss": "2.501", "ppl": "5.66", "wps": "16820.3", "ups": "0.07", "wpb": "234554", "bsz": "512", "num_updates": "22600", "lr": "8.192e-06", "gnorm": "1.442", "loss_scale": "4", "train_wall": "2657", "gb_free": "5.4", "wall": "67497"}
[2022-07-19 07:00:13,080][train_inner][INFO] - {"epoch": 2, "update": 1.278, "loss": "2.502", "ppl": "5.66", "wps": "16944.4", "ups": "0.07", "wpb": "234685", "bsz": "512", "num_updates": "22800", "lr": "8.176e-06", "gnorm": "1.411", "loss_scale": "8", "train_wall": "2638", "gb_free": "5.4", "wall": "70267"}
[2022-07-19 07:02:44,762][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 07:47:03,557][train_inner][INFO] - {"epoch": 2, "update": 1.289, "loss": "2.504", "ppl": "5.67", "wps": "16673.3", "ups": "0.07", "wpb": "234298", "bsz": "512", "num_updates": "23000", "lr": "8.16e-06", "gnorm": "1.415", "loss_scale": "4", "train_wall": "2675", "gb_free": "5.4", "wall": "73078"}
[2022-07-19 08:04:31,489][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 08:29:09,244][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 08:33:31,330][train_inner][INFO] - {"epoch": 2, "update": 1.3, "loss": "2.498", "ppl": "5.65", "wps": "16812.9", "ups": "0.07", "wpb": "234353", "bsz": "512", "num_updates": "23200", "lr": "8.144e-06", "gnorm": "1.39", "loss_scale": "2", "train_wall": "2653", "gb_free": "5.4", "wall": "75866"}
[2022-07-19 09:19:48,201][train_inner][INFO] - {"epoch": 2, "update": 1.311, "loss": "2.494", "ppl": "5.63", "wps": "16864.2", "ups": "0.07", "wpb": "234148", "bsz": "512", "num_updates": "23400", "lr": "8.128e-06", "gnorm": "1.493", "loss_scale": "2", "train_wall": "2644", "gb_free": "5.4", "wall": "78643"}
[2022-07-19 09:36:01,612][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-19 10:06:19,532][train_inner][INFO] - {"epoch": 2, "update": 1.322, "loss": "2.493", "ppl": "5.63", "wps": "16808.6", "ups": "0.07", "wpb": "234590", "bsz": "512", "num_updates": "23600", "lr": "8.112e-06", "gnorm": "1.419", "loss_scale": "2", "train_wall": "2661", "gb_free": "5.4", "wall": "81434"}
[2022-07-19 10:35:26,626][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 10:52:33,327][train_inner][INFO] - {"epoch": 2, "update": 1.333, "loss": "2.493", "ppl": "5.63", "wps": "16919", "ups": "0.07", "wpb": "234648", "bsz": "512", "num_updates": "23800", "lr": "8.096e-06", "gnorm": "1.408", "loss_scale": "2", "train_wall": "2643", "gb_free": "5.4", "wall": "84208"}
[2022-07-19 11:38:32,925][train_inner][INFO] - {"epoch": 2, "update": 1.344, "loss": "2.498", "ppl": "5.65", "wps": "17008.1", "ups": "0.07", "wpb": "234675", "bsz": "512", "num_updates": "24000", "lr": "8.08e-06", "gnorm": "1.427", "loss_scale": "4", "train_wall": "2630", "gb_free": "5.4", "wall": "86967"}
[2022-07-19 11:54:31,496][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 11:59:03,698][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-19 12:24:50,883][train_inner][INFO] - {"epoch": 2, "update": 1.355, "loss": "2.497", "ppl": "5.65", "wps": "16919.7", "ups": "0.07", "wpb": "235010", "bsz": "512", "num_updates": "24200", "lr": "8.064e-06", "gnorm": "1.459", "loss_scale": "1", "train_wall": "2646", "gb_free": "5.4", "wall": "89745"}
[2022-07-19 13:11:24,443][train_inner][INFO] - {"epoch": 2, "update": 1.366, "loss": "2.492", "ppl": "5.63", "wps": "16802.4", "ups": "0.07", "wpb": "234691", "bsz": "512", "num_updates": "24400", "lr": "8.048e-06", "gnorm": "1.459", "loss_scale": "2", "train_wall": "2658", "gb_free": "5.4", "wall": "92539"}
[2022-07-19 13:57:51,205][train_inner][INFO] - {"epoch": 2, "update": 1.377, "loss": "2.491", "ppl": "5.62", "wps": "16808.4", "ups": "0.07", "wpb": "234204", "bsz": "512", "num_updates": "24600", "lr": "8.032e-06", "gnorm": "1.406", "loss_scale": "4", "train_wall": "2656", "gb_free": "5.4", "wall": "95326"}
[2022-07-19 14:16:51,428][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 14:44:28,311][train_inner][INFO] - {"epoch": 2, "update": 1.388, "loss": "2.488", "ppl": "5.61", "wps": "16766.9", "ups": "0.07", "wpb": "234494", "bsz": "512", "num_updates": "24800", "lr": "8.016e-06", "gnorm": "1.407", "loss_scale": "2", "train_wall": "2667", "gb_free": "5.4", "wall": "98123"}
[2022-07-19 15:30:22,651][train_inner][INFO] - {"epoch": 2, "update": 1.399, "loss": "2.486", "ppl": "5.6", "wps": "17000.5", "ups": "0.07", "wpb": "234125", "bsz": "512", "num_updates": "25000", "lr": "8e-06", "gnorm": "1.519", "loss_scale": "4", "train_wall": "2624", "gb_free": "5.4", "wall": "100877"}
[2022-07-19 15:36:39,145][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 16:17:04,771][train_inner][INFO] - {"epoch": 2, "update": 1.41, "loss": "2.483", "ppl": "5.59", "wps": "16763.2", "ups": "0.07", "wpb": "234861", "bsz": "512", "num_updates": "25200", "lr": "7.984e-06", "gnorm": "1.435", "loss_scale": "2", "train_wall": "2672", "gb_free": "5.4", "wall": "103679"}
[2022-07-19 17:03:54,528][train_inner][INFO] - {"epoch": 2, "update": 1.421, "loss": "2.482", "ppl": "5.59", "wps": "16691.4", "ups": "0.07", "wpb": "234494", "bsz": "512", "num_updates": "25400", "lr": "7.968e-06", "gnorm": "1.448", "loss_scale": "4", "train_wall": "2678", "gb_free": "5.4", "wall": "106489"}
[2022-07-19 17:17:32,272][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-19 17:50:22,699][train_inner][INFO] - {"epoch": 2, "update": 1.432, "loss": "2.483", "ppl": "5.59", "wps": "16812.7", "ups": "0.07", "wpb": "234383", "bsz": "512", "num_updates": "25600", "lr": "7.952e-06", "gnorm": "1.414", "loss_scale": "2", "train_wall": "2657", "gb_free": "5.4", "wall": "109277"}
[2022-07-19 18:37:02,285][train_inner][INFO] - {"epoch": 2, "update": 1.443, "loss": "2.481", "ppl": "5.58", "wps": "16799.1", "ups": "0.07", "wpb": "235152", "bsz": "512", "num_updates": "25800", "lr": "7.936e-06", "gnorm": "1.42", "loss_scale": "4", "train_wall": "2665", "gb_free": "5.4", "wall": "112077"}
[2022-07-19 19:22:51,165][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 19:23:34,185][train_inner][INFO] - {"epoch": 2, "update": 1.454, "loss": "2.483", "ppl": "5.59", "wps": "16778", "ups": "0.07", "wpb": "234210", "bsz": "512", "num_updates": "26000", "lr": "7.92e-06", "gnorm": "1.385", "loss_scale": "4", "train_wall": "2658", "gb_free": "5.4", "wall": "114869"}
[2022-07-19 20:00:31,697][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 20:09:45,253][train_inner][INFO] - {"epoch": 2, "update": 1.465, "loss": "2.478", "ppl": "5.57", "wps": "16915.3", "ups": "0.07", "wpb": "234366", "bsz": "512", "num_updates": "26200", "lr": "7.904e-06", "gnorm": "1.396", "loss_scale": "2", "train_wall": "2642", "gb_free": "5.4", "wall": "117640"}
[2022-07-19 20:56:26,699][train_inner][INFO] - {"epoch": 2, "update": 1.476, "loss": "2.479", "ppl": "5.58", "wps": "16741.9", "ups": "0.07", "wpb": "234506", "bsz": "512", "num_updates": "26400", "lr": "7.888e-06", "gnorm": "1.489", "loss_scale": "2", "train_wall": "2666", "gb_free": "5.4", "wall": "120441"}
[2022-07-19 20:59:59,843][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-19 21:43:20,519][train_inner][INFO] - {"epoch": 2, "update": 1.487, "loss": "2.479", "ppl": "5.58", "wps": "16677.1", "ups": "0.07", "wpb": "234630", "bsz": "512", "num_updates": "26600", "lr": "7.872e-06", "gnorm": "1.489", "loss_scale": "1", "train_wall": "2680", "gb_free": "5.4", "wall": "123255"}
[2022-07-19 22:29:59,101][train_inner][INFO] - {"epoch": 2, "update": 1.498, "loss": "2.477", "ppl": "5.57", "wps": "16791.8", "ups": "0.07", "wpb": "234966", "bsz": "512", "num_updates": "26800", "lr": "7.856e-06", "gnorm": "1.429", "loss_scale": "2", "train_wall": "2661", "gb_free": "5.4", "wall": "126053"}
[2022-07-19 22:59:52,440][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 23:15:39,749][train_inner][INFO] - {"epoch": 2, "update": 1.509, "loss": "2.481", "ppl": "5.58", "wps": "17107.4", "ups": "0.07", "wpb": "234426", "bsz": "512", "num_updates": "27000", "lr": "7.84e-06", "gnorm": "1.412", "loss_scale": "2", "train_wall": "2612", "gb_free": "5.4", "wall": "128794"}
[2022-07-20 00:00:51,923][train_inner][INFO] - {"epoch": 2, "update": 1.52, "loss": "2.483", "ppl": "5.59", "wps": "17276.6", "ups": "0.07", "wpb": "234285", "bsz": "512", "num_updates": "27200", "lr": "7.824e-06", "gnorm": "1.418", "loss_scale": "4", "train_wall": "2589", "gb_free": "5.4", "wall": "131506"}
[2022-07-20 00:12:02,920][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 00:46:55,622][train_inner][INFO] - {"epoch": 2, "update": 1.531, "loss": "2.479", "ppl": "5.57", "wps": "16951.9", "ups": "0.07", "wpb": "234249", "bsz": "512", "num_updates": "27400", "lr": "7.808e-06", "gnorm": "1.412", "loss_scale": "2", "train_wall": "2633", "gb_free": "5.4", "wall": "134270"}
[2022-07-20 01:15:23,729][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 01:33:00,290][train_inner][INFO] - {"epoch": 2, "update": 1.542, "loss": "2.48", "ppl": "5.58", "wps": "16963.8", "ups": "0.07", "wpb": "234496", "bsz": "512", "num_updates": "27600", "lr": "7.792e-06", "gnorm": "1.42", "loss_scale": "2", "train_wall": "2637", "gb_free": "5.4", "wall": "137035"}
[2022-07-20 02:19:09,378][train_inner][INFO] - {"epoch": 2, "update": 1.553, "loss": "2.47", "ppl": "5.54", "wps": "16941.4", "ups": "0.07", "wpb": "234558", "bsz": "512", "num_updates": "27800", "lr": "7.776e-06", "gnorm": "1.378", "loss_scale": "4", "train_wall": "2639", "gb_free": "5.4", "wall": "139804"}
[2022-07-20 02:46:08,935][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 03:06:08,815][train_inner][INFO] - {"epoch": 2, "update": 1.564, "loss": "2.473", "ppl": "5.55", "wps": "16661.8", "ups": "0.07", "wpb": "234884", "bsz": "512", "num_updates": "28000", "lr": "7.76e-06", "gnorm": "1.356", "loss_scale": "2", "train_wall": "2683", "gb_free": "5.4", "wall": "142623"}
[2022-07-20 03:52:15,356][train_inner][INFO] - {"epoch": 2, "update": 1.575, "loss": "2.466", "ppl": "5.52", "wps": "16970.4", "ups": "0.07", "wpb": "234746", "bsz": "512", "num_updates": "28200", "lr": "7.744e-06", "gnorm": "1.436", "loss_scale": "4", "train_wall": "2635", "gb_free": "5.4", "wall": "145390"}
[2022-07-20 04:38:20,229][train_inner][INFO] - {"epoch": 2, "update": 1.586, "loss": "2.471", "ppl": "5.54", "wps": "16966.1", "ups": "0.07", "wpb": "234545", "bsz": "512", "num_updates": "28400", "lr": "7.728e-06", "gnorm": "1.386", "loss_scale": "4", "train_wall": "2631", "gb_free": "5.4", "wall": "148155"}
[2022-07-20 04:49:07,601][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-20 04:53:38,293][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 05:25:12,203][train_inner][INFO] - {"epoch": 2, "update": 1.597, "loss": "2.472", "ppl": "5.55", "wps": "16672.5", "ups": "0.07", "wpb": "234413", "bsz": "512", "num_updates": "28600", "lr": "7.712e-06", "gnorm": "1.454", "loss_scale": "2", "train_wall": "2679", "gb_free": "5.4", "wall": "150967"}
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-20 06:06:28,437][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 06:11:49,556][train_inner][INFO] - {"epoch": 2, "update": 1.608, "loss": "2.463", "ppl": "5.51", "wps": "16795.7", "ups": "0.07", "wpb": "234917", "bsz": "512", "num_updates": "28800", "lr": "7.696e-06", "gnorm": "1.367", "loss_scale": "2", "train_wall": "2663", "gb_free": "5.4", "wall": "153764"}
[2022-07-20 06:57:40,766][train_inner][INFO] - {"epoch": 2, "update": 1.619, "loss": "2.466", "ppl": "5.53", "wps": "17063.5", "ups": "0.07", "wpb": "234726", "bsz": "512", "num_updates": "29000", "lr": "7.68e-06", "gnorm": "1.399", "loss_scale": "2", "train_wall": "2621", "gb_free": "5.4", "wall": "156515"}
[2022-07-20 07:44:09,420][train_inner][INFO] - {"epoch": 2, "update": 1.63, "loss": "2.464", "ppl": "5.52", "wps": "16826.8", "ups": "0.07", "wpb": "234620", "bsz": "512", "num_updates": "29200", "lr": "7.664e-06", "gnorm": "1.403", "loss_scale": "4", "train_wall": "2658", "gb_free": "5.4", "wall": "159304"}
[2022-07-20 08:30:15,420][train_inner][INFO] - {"epoch": 2, "update": 1.641, "loss": "2.461", "ppl": "5.51", "wps": "16953", "ups": "0.07", "wpb": "234459", "bsz": "512", "num_updates": "29400", "lr": "7.648e-06", "gnorm": "1.379", "loss_scale": "8", "train_wall": "2638", "gb_free": "5.4", "wall": "162070"}
[2022-07-20 08:42:54,454][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-20 09:04:18,924][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 09:16:42,441][train_inner][INFO] - {"epoch": 2, "update": 1.652, "loss": "2.467", "ppl": "5.53", "wps": "16842.2", "ups": "0.07", "wpb": "234697", "bsz": "512", "num_updates": "29600", "lr": "7.632e-06", "gnorm": "1.434", "loss_scale": "2", "train_wall": "2656", "gb_free": "5.4", "wall": "164857"}
[2022-07-20 09:20:45,549][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-20 10:02:57,265][train_inner][INFO] - {"epoch": 2, "update": 1.663, "loss": "2.462", "ppl": "5.51", "wps": "16909.6", "ups": "0.07", "wpb": "234605", "bsz": "512", "num_updates": "29800", "lr": "7.616e-06", "gnorm": "1.424", "loss_scale": "1", "train_wall": "2642", "gb_free": "5.4", "wall": "167632"}
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-20 10:45:13,256][train_inner][INFO] - {"epoch": 2, "update": 1.674, "loss": "2.455", "ppl": "5.48", "wps": "18517.5", "ups": "0.08", "wpb": "234801", "bsz": "512", "num_updates": "30000", "lr": "7.6e-06", "gnorm": "1.427", "loss_scale": "2", "train_wall": "2443", "gb_free": "5.4", "wall": "170168"}
[2022-07-20 11:18:43,051][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 11:27:32,359][train_inner][INFO] - {"epoch": 2, "update": 1.685, "loss": "2.459", "ppl": "5.5", "wps": "18499.2", "ups": "0.08", "wpb": "234857", "bsz": "512", "num_updates": "30200", "lr": "7.584e-06", "gnorm": "1.373", "loss_scale": "2", "train_wall": "2448", "gb_free": "5.4", "wall": "172707"}
[2022-07-20 12:09:34,505][train_inner][INFO] - {"epoch": 2, "update": 1.696, "loss": "2.465", "ppl": "5.52", "wps": "18589.8", "ups": "0.08", "wpb": "234430", "bsz": "512", "num_updates": "30400", "lr": "7.568e-06", "gnorm": "1.391", "loss_scale": "2", "train_wall": "2431", "gb_free": "5.4", "wall": "175229"}
[2022-07-20 12:51:34,670][train_inner][INFO] - {"epoch": 2, "update": 1.707, "loss": "2.465", "ppl": "5.52", "wps": "18609", "ups": "0.08", "wpb": "234488", "bsz": "512", "num_updates": "30600", "lr": "7.552e-06", "gnorm": "1.412", "loss_scale": "4", "train_wall": "2429", "gb_free": "5.4", "wall": "177749"}
[2022-07-20 13:24:20,525][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-20 13:29:47,647][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 13:33:58,745][train_inner][INFO] - {"epoch": 2, "update": 1.718, "loss": "2.467", "ppl": "5.53", "wps": "18422.9", "ups": "0.08", "wpb": "234346", "bsz": "512", "num_updates": "30800", "lr": "7.536e-06", "gnorm": "1.398", "loss_scale": "2", "train_wall": "2452", "gb_free": "5.4", "wall": "180293"}
[2022-07-20 13:52:17,892][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-20 14:16:12,288][train_inner][INFO] - {"epoch": 2, "update": 1.729, "loss": "2.458", "ppl": "5.5", "wps": "18516.6", "ups": "0.08", "wpb": "234563", "bsz": "512", "num_updates": "31000", "lr": "7.52e-06", "gnorm": "1.42", "loss_scale": "1", "train_wall": "2442", "gb_free": "5.4", "wall": "182827"}
[2022-07-20 14:58:14,807][train_inner][INFO] - {"epoch": 2, "update": 1.74, "loss": "2.457", "ppl": "5.49", "wps": "18610.7", "ups": "0.08", "wpb": "234729", "bsz": "512", "num_updates": "31200", "lr": "7.504e-06", "gnorm": "1.427", "loss_scale": "2", "train_wall": "2432", "gb_free": "5.4", "wall": "185349"}
[2022-07-20 15:40:16,983][train_inner][INFO] - {"epoch": 2, "update": 1.751, "loss": "2.456", "ppl": "5.49", "wps": "18605.5", "ups": "0.08", "wpb": "234631", "bsz": "512", "num_updates": "31400", "lr": "7.488e-06", "gnorm": "1.427", "loss_scale": "4", "train_wall": "2432", "gb_free": "5.4", "wall": "187871"}
[2022-07-20 15:57:57,732][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 16:22:39,355][train_inner][INFO] - {"epoch": 2, "update": 1.762, "loss": "2.454", "ppl": "5.48", "wps": "18490.8", "ups": "0.08", "wpb": "235053", "bsz": "512", "num_updates": "31600", "lr": "7.472e-06", "gnorm": "1.402", "loss_scale": "2", "train_wall": "2450", "gb_free": "5.4", "wall": "190414"}
[2022-07-20 17:04:33,750][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 17:04:59,235][train_inner][INFO] - {"epoch": 2, "update": 1.773, "loss": "2.459", "ppl": "5.5", "wps": "18443.9", "ups": "0.08", "wpb": "234226", "bsz": "512", "num_updates": "31800", "lr": "7.456e-06", "gnorm": "1.377", "loss_scale": "2", "train_wall": "2446", "gb_free": "5.4", "wall": "192954"}
[2022-07-20 17:47:01,692][train_inner][INFO] - {"epoch": 2, "update": 1.784, "loss": "2.459", "ppl": "5.5", "wps": "18549.2", "ups": "0.08", "wpb": "233948", "bsz": "512", "num_updates": "32000", "lr": "7.44e-06", "gnorm": "1.407", "loss_scale": "2", "train_wall": "2430", "gb_free": "5.4", "wall": "195476"}
[2022-07-20 17:47:39,361][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-20 18:29:20,317][train_inner][INFO] - {"epoch": 2, "update": 1.795, "loss": "2.451", "ppl": "5.47", "wps": "18448.6", "ups": "0.08", "wpb": "234170", "bsz": "512", "num_updates": "32200", "lr": "7.424e-06", "gnorm": "1.386", "loss_scale": "1", "train_wall": "2445", "gb_free": "5.4", "wall": "198015"}
[2022-07-20 19:11:32,356][train_inner][INFO] - {"epoch": 2, "update": 1.806, "loss": "2.452", "ppl": "5.47", "wps": "18533.4", "ups": "0.08", "wpb": "234636", "bsz": "512", "num_updates": "32400", "lr": "7.408e-06", "gnorm": "1.417", "loss_scale": "2", "train_wall": "2438", "gb_free": "5.4", "wall": "200547"}
[2022-07-20 19:50:37,244][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 19:53:47,174][train_inner][INFO] - {"epoch": 2, "update": 1.817, "loss": "2.45", "ppl": "5.46", "wps": "18509.5", "ups": "0.08", "wpb": "234591", "bsz": "512", "num_updates": "32600", "lr": "7.392e-06", "gnorm": "1.407", "loss_scale": "2", "train_wall": "2443", "gb_free": "5.4", "wall": "203082"}
[2022-07-20 20:35:49,651][train_inner][INFO] - {"epoch": 2, "update": 1.828, "loss": "2.453", "ppl": "5.47", "wps": "18616.3", "ups": "0.08", "wpb": "234796", "bsz": "512", "num_updates": "32800", "lr": "7.376e-06", "gnorm": "1.424", "loss_scale": "2", "train_wall": "2432", "gb_free": "5.4", "wall": "205604"}
[2022-07-20 21:17:49,897][train_inner][INFO] - {"epoch": 2, "update": 1.839, "loss": "2.45", "ppl": "5.47", "wps": "18614.3", "ups": "0.08", "wpb": "234563", "bsz": "512", "num_updates": "33000", "lr": "7.36e-06", "gnorm": "1.425", "loss_scale": "4", "train_wall": "2430", "gb_free": "5.4", "wall": "208124"}
[2022-07-20 21:43:54,641][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-20 21:56:33,694][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 22:00:22,469][train_inner][INFO] - {"epoch": 2, "update": 1.85, "loss": "2.451", "ppl": "5.47", "wps": "18397.6", "ups": "0.08", "wpb": "234805", "bsz": "512", "num_updates": "33200", "lr": "7.344e-06", "gnorm": "1.396", "loss_scale": "2", "train_wall": "2459", "gb_free": "5.4", "wall": "210677"}
[2022-07-20 22:30:37,569][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-20 22:42:37,605][train_inner][INFO] - {"epoch": 2, "update": 1.861, "loss": "2.441", "ppl": "5.43", "wps": "18493.7", "ups": "0.08", "wpb": "234419", "bsz": "512", "num_updates": "33400", "lr": "7.328e-06", "gnorm": "1.438", "loss_scale": "1", "train_wall": "2444", "gb_free": "5.4", "wall": "213212"}
[2022-07-20 23:24:37,394][train_inner][INFO] - {"epoch": 2, "update": 1.872, "loss": "2.447", "ppl": "5.45", "wps": "18619.3", "ups": "0.08", "wpb": "234584", "bsz": "512", "num_updates": "33600", "lr": "7.312e-06", "gnorm": "1.405", "loss_scale": "2", "train_wall": "2429", "gb_free": "5.4", "wall": "215732"}
[2022-07-21 00:06:37,909][train_inner][INFO] - {"epoch": 2, "update": 1.883, "loss": "2.447", "ppl": "5.45", "wps": "18612.7", "ups": "0.08", "wpb": "234568", "bsz": "512", "num_updates": "33800", "lr": "7.296e-06", "gnorm": "1.393", "loss_scale": "2", "train_wall": "2430", "gb_free": "5.4", "wall": "218252"}
[2022-07-21 00:48:31,848][train_inner][INFO] - {"epoch": 2, "update": 1.894, "loss": "2.445", "ppl": "5.45", "wps": "18644.8", "ups": "0.08", "wpb": "234359", "bsz": "512", "num_updates": "34000", "lr": "7.28e-06", "gnorm": "1.4", "loss_scale": "4", "train_wall": "2424", "gb_free": "5.3", "wall": "220766"}
[2022-07-21 00:48:56,933][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-21 01:30:41,776][train_inner][INFO] - {"epoch": 2, "update": 1.905, "loss": "2.444", "ppl": "5.44", "wps": "18545.3", "ups": "0.08", "wpb": "234591", "bsz": "512", "num_updates": "34200", "lr": "7.264e-06", "gnorm": "1.392", "loss_scale": "2", "train_wall": "2439", "gb_free": "5.4", "wall": "223296"}
[2022-07-21 02:12:45,148][train_inner][INFO] - {"epoch": 2, "update": 1.916, "loss": "2.443", "ppl": "5.44", "wps": "18610.2", "ups": "0.08", "wpb": "234802", "bsz": "512", "num_updates": "34400", "lr": "7.248e-06", "gnorm": "1.391", "loss_scale": "4", "train_wall": "2432", "gb_free": "5.4", "wall": "225820"}
[2022-07-21 02:54:27,585][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-21 02:55:05,351][train_inner][INFO] - {"epoch": 2, "update": 1.927, "loss": "2.447", "ppl": "5.45", "wps": "18475.1", "ups": "0.08", "wpb": "234653", "bsz": "512", "num_updates": "34600", "lr": "7.232e-06", "gnorm": "1.39", "loss_scale": "4", "train_wall": "2447", "gb_free": "5.4", "wall": "228360"}
[2022-07-21 03:15:17,693][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-21 03:37:24,417][train_inner][INFO] - {"epoch": 2, "update": 1.938, "loss": "2.445", "ppl": "5.44", "wps": "18486.9", "ups": "0.08", "wpb": "234697", "bsz": "512", "num_updates": "34800", "lr": "7.216e-06", "gnorm": "1.417", "loss_scale": "2", "train_wall": "2446", "gb_free": "5.4", "wall": "230899"}
[2022-07-21 04:19:26,724][train_inner][INFO] - {"epoch": 2, "update": 1.949, "loss": "2.442", "ppl": "5.43", "wps": "18569", "ups": "0.08", "wpb": "234183", "bsz": "512", "num_updates": "35000", "lr": "7.2e-06", "gnorm": "1.405", "loss_scale": "4", "train_wall": "2429", "gb_free": "5.4", "wall": "233421"}
[2022-07-21 04:24:16,872][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-21 05:01:39,323][train_inner][INFO] - {"epoch": 2, "update": 1.96, "loss": "2.441", "ppl": "5.43", "wps": "18498.1", "ups": "0.08", "wpb": "234241", "bsz": "512", "num_updates": "35200", "lr": "7.184e-06", "gnorm": "1.376", "loss_scale": "2", "train_wall": "2439", "gb_free": "5.4", "wall": "235954"}
[2022-07-21 05:32:47,738][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-21 05:43:57,711][train_inner][INFO] - {"epoch": 2, "update": 1.971, "loss": "2.441", "ppl": "5.43", "wps": "18468.7", "ups": "0.08", "wpb": "234403", "bsz": "512", "num_updates": "35400", "lr": "7.168e-06", "gnorm": "1.424", "loss_scale": "2", "train_wall": "2445", "gb_free": "5.4", "wall": "238492"}
[2022-07-21 06:26:07,972][train_inner][INFO] - {"epoch": 2, "update": 1.982, "loss": "2.438", "ppl": "5.42", "wps": "18547.5", "ups": "0.08", "wpb": "234650", "bsz": "512", "num_updates": "35600", "lr": "7.152e-06", "gnorm": "1.434", "loss_scale": "2", "train_wall": "2437", "gb_free": "5.4", "wall": "241022"}
[2022-07-21 07:08:14,723][train_inner][INFO] - {"epoch": 2, "update": 1.993, "loss": "2.44", "ppl": "5.42", "wps": "18554", "ups": "0.08", "wpb": "234406", "bsz": "512", "num_updates": "35800", "lr": "7.136e-06", "gnorm": "1.398", "loss_scale": "4", "train_wall": "2434", "gb_free": "5.4", "wall": "243549"}
[2022-07-21 07:16:13,625][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-21 07:24:25,646][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-21 07:36:08,506][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-21 07:36:25,983][valid][INFO] - {"epoch": 2, "valid_loss": "2.345", "valid_ppl": "5.08", "valid_wps": "51021.4", "valid_wpb": "7295.6", "valid_bsz": "15.8", "valid_num_updates": "35931", "valid_best_loss": "2.345"}
[2022-07-21 07:36:25,986][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 35931 updates
[2022-07-21 07:36:25,988][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-18/11-28-41/0/checkpoints/checkpoint2.pt
[2022-07-21 07:36:30,475][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-18/11-28-41/0/checkpoints/checkpoint2.pt
[2022-07-21 07:36:36,092][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 35931 updates, score 2.345) (writing took 10.105371422949247 seconds)
[2022-07-21 07:36:36,093][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2022-07-21 07:36:36,098][train][INFO] - {"epoch": 2, "train_loss": "2.48", "train_ppl": "5.58", "train_wps": "17393.3", "train_ups": "0.07", "train_wpb": "234526", "train_bsz": "512", "train_num_updates": "35931", "train_lr": "7.12552e-06", "train_gnorm": "1.419", "train_loss_scale": "1", "train_train_wall": "234215", "train_gb_free": "5.3", "train_wall": "245250"}
[2022-07-21 07:36:36,120][fairseq_cli.train][INFO] - done training in 245206.9 seconds
slurmstepd: error: *** JOB 9752892 ON r31n3 CANCELLED AT 2022-07-21T09:48:52 ***
