[2022-07-18 10:16:10,830][HYDRA] Launching 1 jobs locally
[2022-07-18 10:16:10,831][HYDRA] 	#0 : 
2022-07-18 10:16:15 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13027
2022-07-18 10:16:15 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13027
2022-07-18 10:16:15 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-18 10:16:15 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-18 10:16:15 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2022-07-18 10:16:15 | INFO | fairseq.distributed.utils | initialized host r32n2.lisa.surfsara.nl as rank 1
2022-07-18 10:16:15 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2022-07-18 10:16:15 | INFO | fairseq.distributed.utils | initialized host r32n2.lisa.surfsara.nl as rank 0
[2022-07-18 10:16:19,933][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'RoBERTa', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13027', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': True, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 125000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': '/home/dahmanir/lisa/Models/RoBERTa_50_finetune.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'roberta', 'max_positions': 512, 'dropout': 0.1, 'attention_dropout': 0.1}, 'task': {'_name': 'masked_lm', 'data': '/home/dahmanir/lisa/Datasets/50_percent', 'sample_break_mode': complete, 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': none, 'shorten_data_split_list': '', 'seed': 1, 'include_target_tokens': False}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 125000.0, 'lr': [1e-05]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2022-07-18 10:16:20,019][fairseq.tasks.masked_lm][INFO] - dictionary: 39984 types
[2022-07-18 10:16:24,985][fairseq_cli.train][INFO] - RobertaModel(
  (encoder): RobertaEncoder(
    (sentence_encoder): TransformerEncoder(
      (dropout_module): FairseqDropout()
      (embed_tokens): Embedding(39985, 768, padding_idx=1)
      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)
      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (lm_head): RobertaLMHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (classification_heads): ModuleDict()
)
[2022-07-18 10:16:24,987][fairseq_cli.train][INFO] - task: MaskedLMTask
[2022-07-18 10:16:24,988][fairseq_cli.train][INFO] - model: RobertaModel
[2022-07-18 10:16:24,988][fairseq_cli.train][INFO] - criterion: MaskedLmLoss
[2022-07-18 10:16:24,989][fairseq_cli.train][INFO] - num. shared model params: 116,791,345 (num. trained: 116,791,345)
[2022-07-18 10:16:24,991][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2022-07-18 10:16:24,997][fairseq.data.data_utils][INFO] - loaded 10,000 examples from: /home/dahmanir/lisa/Datasets/50_percent/valid
[2022-07-18 10:16:25,002][fairseq.tasks.masked_lm][INFO] - loaded 1441 blocks from: /home/dahmanir/lisa/Datasets/50_percent/valid
[2022-07-18 10:16:27,825][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:2 to store for rank: 0
[2022-07-18 10:16:27,826][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.
[2022-07-18 10:16:27,826][fairseq.trainer][INFO] - detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight
[2022-07-18 10:16:27,905][fairseq.utils][INFO] - ***********************CUDA enviroments for all 2 workers***********************
[2022-07-18 10:16:27,906][fairseq.utils][INFO] - rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = NVIDIA GeForce GTX 1080 Ti              
[2022-07-18 10:16:27,906][fairseq.utils][INFO] - rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = NVIDIA GeForce GTX 1080 Ti              
[2022-07-18 10:16:27,906][fairseq.utils][INFO] - ***********************CUDA enviroments for all 2 workers***********************
[2022-07-18 10:16:27,906][fairseq_cli.train][INFO] - training on 2 devices (GPUs/TPUs)
[2022-07-18 10:16:27,907][fairseq_cli.train][INFO] - max tokens per device = None and max sentences per device = 8
[2022-07-18 10:16:27,908][fairseq.trainer][INFO] - Preparing to load checkpoint /home/dahmanir/lisa/Models/RoBERTa_50_finetune.pt
[2022-07-18 10:16:32,987][fairseq.trainer][INFO] - NOTE: your device does NOT support faster training with --fp16 or --amp, please switch to FP32 which is likely to be faster
[2022-07-18 10:16:33,656][fairseq.trainer][INFO] - Loaded checkpoint /home/dahmanir/lisa/Models/RoBERTa_50_finetune.pt (epoch 2 @ 17748 updates)
[2022-07-18 10:16:33,656][fairseq.trainer][INFO] - loading train data for epoch 2
[2022-07-18 10:16:40,458][fairseq.data.data_utils][INFO] - loaded 62,772,775 examples from: /home/dahmanir/lisa/Datasets/50_percent/train
[2022-07-18 10:16:46,799][fairseq.tasks.masked_lm][INFO] - loaded 9118291 blocks from: /home/dahmanir/lisa/Datasets/50_percent/train
2022-07-18 10:16:50 | WARNING | fairseq.tasks.fairseq_task | 2,479 samples have invalid sizes and will be skipped, max_positions=512, first few sample ids=[7528646, 3259585, 6671452, 186122, 7950041, 4587179, 5755290, 6240826, 5879766, 938078]
[2022-07-18 10:16:50,504][fairseq.tasks.fairseq_task][WARNING] - 2,479 samples have invalid sizes and will be skipped, max_positions=512, first few sample ids=[7528646, 3259585, 6671452, 186122, 7950041, 4587179, 5755290, 6240826, 5879766, 938078]
[2022-07-18 10:17:04,723][fairseq.data.iterators][INFO] - grouped total_num_itrs = 17805
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.21 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.19
wandb: Run data is saved locally in /home/dahmanir/multirun/2022-07-18/10-16-09/0/wandb/run-20220718_101716-2flq78v1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run checkpoints
wandb:  View project at https://wandb.ai/redredouane/RoBERTa
wandb:  View run at https://wandb.ai/redredouane/RoBERTa/runs/2flq78v1
[2022-07-18 10:17:22,122][fairseq.trainer][INFO] - begin training epoch 2
[2022-07-18 10:17:22,124][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-18 10:17:43,384][root][INFO] - Reducer buckets have been rebuilt in this iteration.
[2022-07-18 10:28:23,388][train_inner][INFO] - {"epoch": 2, "update": 1.003, "loss": "2.51", "ppl": "5.7", "wps": "18759.4", "ups": "0.08", "wpb": "235655", "bsz": "512", "num_updates": "17800", "lr": "8.576e-06", "gnorm": "1.39", "loss_scale": "2", "train_wall": "629", "gb_free": "5.4", "wall": "715"}
[2022-07-18 11:10:14,325][train_inner][INFO] - {"epoch": 2, "update": 1.014, "loss": "2.515", "ppl": "5.72", "wps": "18737.6", "ups": "0.08", "wpb": "235245", "bsz": "512", "num_updates": "18000", "lr": "8.56e-06", "gnorm": "1.459", "loss_scale": "2", "train_wall": "2419", "gb_free": "5.4", "wall": "3226"}
[2022-07-18 11:52:09,348][train_inner][INFO] - {"epoch": 2, "update": 1.025, "loss": "2.516", "ppl": "5.72", "wps": "18746", "ups": "0.08", "wpb": "235733", "bsz": "512", "num_updates": "18200", "lr": "8.544e-06", "gnorm": "1.459", "loss_scale": "4", "train_wall": "2423", "gb_free": "5.3", "wall": "5741"}
[2022-07-18 12:09:45,946][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-18 12:14:11,007][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-18 12:34:26,015][train_inner][INFO] - {"epoch": 2, "update": 1.037, "loss": "2.508", "ppl": "5.69", "wps": "18569", "ups": "0.08", "wpb": "235517", "bsz": "512", "num_updates": "18400", "lr": "8.528e-06", "gnorm": "1.386", "loss_scale": "2", "train_wall": "2444", "gb_free": "5.4", "wall": "8278"}
[2022-07-18 13:16:09,188][train_inner][INFO] - {"epoch": 2, "update": 1.048, "loss": "2.511", "ppl": "5.7", "wps": "18740.8", "ups": "0.08", "wpb": "234556", "bsz": "512", "num_updates": "18600", "lr": "8.512e-06", "gnorm": "1.408", "loss_scale": "4", "train_wall": "2411", "gb_free": "5.3", "wall": "10781"}
[2022-07-18 13:57:58,337][train_inner][INFO] - {"epoch": 2, "update": 1.059, "loss": "2.51", "ppl": "5.7", "wps": "18755.7", "ups": "0.08", "wpb": "235304", "bsz": "512", "num_updates": "18800", "lr": "8.496e-06", "gnorm": "1.42", "loss_scale": "4", "train_wall": "2417", "gb_free": "5.4", "wall": "13290"}
[2022-07-18 14:20:09,641][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-18 14:39:58,622][train_inner][INFO] - {"epoch": 2, "update": 1.07, "loss": "2.502", "ppl": "5.67", "wps": "18656", "ups": "0.08", "wpb": "235092", "bsz": "512", "num_updates": "19000", "lr": "8.48e-06", "gnorm": "1.385", "loss_scale": "4", "train_wall": "2427", "gb_free": "5.4", "wall": "15811"}
[2022-07-18 15:18:00,418][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-18 15:21:59,180][train_inner][INFO] - {"epoch": 2, "update": 1.082, "loss": "2.509", "ppl": "5.69", "wps": "18649.7", "ups": "0.08", "wpb": "235038", "bsz": "512", "num_updates": "19200", "lr": "8.464e-06", "gnorm": "1.437", "loss_scale": "4", "train_wall": "2428", "gb_free": "5.4", "wall": "18331"}
[2022-07-18 16:03:43,574][train_inner][INFO] - {"epoch": 2, "update": 1.093, "loss": "2.505", "ppl": "5.68", "wps": "18727.4", "ups": "0.08", "wpb": "234504", "bsz": "512", "num_updates": "19400", "lr": "8.448e-06", "gnorm": "1.433", "loss_scale": "4", "train_wall": "2412", "gb_free": "5.4", "wall": "20836"}
[2022-07-18 16:16:52,433][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-18 16:45:43,734][train_inner][INFO] - {"epoch": 2, "update": 1.104, "loss": "2.494", "ppl": "5.63", "wps": "18645.4", "ups": "0.08", "wpb": "234946", "bsz": "512", "num_updates": "19600", "lr": "8.432e-06", "gnorm": "1.418", "loss_scale": "4", "train_wall": "2428", "gb_free": "5.4", "wall": "23356"}
[2022-07-18 17:05:21,875][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-18 17:11:01,337][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-18 17:27:58,637][train_inner][INFO] - {"epoch": 2, "update": 1.116, "loss": "2.502", "ppl": "5.67", "wps": "18561.9", "ups": "0.08", "wpb": "235263", "bsz": "512", "num_updates": "19800", "lr": "8.416e-06", "gnorm": "1.438", "loss_scale": "1", "train_wall": "2442", "gb_free": "5.4", "wall": "25891"}
[2022-07-18 18:09:51,816][train_inner][INFO] - {"epoch": 2, "update": 1.127, "loss": "2.506", "ppl": "5.68", "wps": "18754.3", "ups": "0.08", "wpb": "235665", "bsz": "512", "num_updates": "20000", "lr": "8.4e-06", "gnorm": "1.404", "loss_scale": "2", "train_wall": "2420", "gb_free": "5.4", "wall": "28404"}
[2022-07-18 18:51:38,864][train_inner][INFO] - {"epoch": 2, "update": 1.138, "loss": "2.499", "ppl": "5.65", "wps": "18737.8", "ups": "0.08", "wpb": "234883", "bsz": "512", "num_updates": "20200", "lr": "8.384e-06", "gnorm": "1.449", "loss_scale": "2", "train_wall": "2415", "gb_free": "5.4", "wall": "30911"}
[2022-07-18 19:33:27,011][train_inner][INFO] - {"epoch": 2, "update": 1.149, "loss": "2.498", "ppl": "5.65", "wps": "18731.4", "ups": "0.08", "wpb": "234905", "bsz": "512", "num_updates": "20400", "lr": "8.368e-06", "gnorm": "1.398", "loss_scale": "4", "train_wall": "2416", "gb_free": "5.4", "wall": "33419"}
[2022-07-18 19:56:38,394][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-18 20:15:27,842][train_inner][INFO] - {"epoch": 2, "update": 1.161, "loss": "2.495", "ppl": "5.64", "wps": "18669", "ups": "0.08", "wpb": "235307", "bsz": "512", "num_updates": "20600", "lr": "8.352e-06", "gnorm": "1.395", "loss_scale": "4", "train_wall": "2428", "gb_free": "5.4", "wall": "35940"}
[2022-07-18 20:30:19,217][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-18 20:57:27,403][train_inner][INFO] - {"epoch": 2, "update": 1.172, "loss": "2.491", "ppl": "5.62", "wps": "18665.1", "ups": "0.08", "wpb": "235139", "bsz": "512", "num_updates": "20800", "lr": "8.336e-06", "gnorm": "1.444", "loss_scale": "2", "train_wall": "2427", "gb_free": "5.4", "wall": "38459"}
[2022-07-18 21:39:15,176][train_inner][INFO] - {"epoch": 2, "update": 1.183, "loss": "2.494", "ppl": "5.63", "wps": "18754.8", "ups": "0.08", "wpb": "235164", "bsz": "512", "num_updates": "21000", "lr": "8.32e-06", "gnorm": "1.422", "loss_scale": "4", "train_wall": "2415", "gb_free": "5.3", "wall": "40967"}
[2022-07-18 22:07:39,787][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-18 22:20:28,203][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-18 22:21:31,261][train_inner][INFO] - {"epoch": 2, "update": 1.194, "loss": "2.491", "ppl": "5.62", "wps": "18551.5", "ups": "0.08", "wpb": "235241", "bsz": "512", "num_updates": "21200", "lr": "8.304e-06", "gnorm": "1.436", "loss_scale": "1", "train_wall": "2443", "gb_free": "5.3", "wall": "43503"}
[2022-07-18 23:03:15,228][train_inner][INFO] - {"epoch": 2, "update": 1.206, "loss": "2.491", "ppl": "5.62", "wps": "18765.5", "ups": "0.08", "wpb": "234941", "bsz": "512", "num_updates": "21400", "lr": "8.288e-06", "gnorm": "1.404", "loss_scale": "1", "train_wall": "2412", "gb_free": "5.4", "wall": "46007"}
[2022-07-18 23:44:58,735][train_inner][INFO] - {"epoch": 2, "update": 1.217, "loss": "2.492", "ppl": "5.63", "wps": "18770.6", "ups": "0.08", "wpb": "234962", "bsz": "512", "num_updates": "21600", "lr": "8.272e-06", "gnorm": "1.436", "loss_scale": "2", "train_wall": "2412", "gb_free": "5.4", "wall": "48511"}
[2022-07-19 00:26:47,644][train_inner][INFO] - {"epoch": 2, "update": 1.228, "loss": "2.487", "ppl": "5.61", "wps": "18766.7", "ups": "0.08", "wpb": "235420", "bsz": "512", "num_updates": "21800", "lr": "8.256e-06", "gnorm": "1.407", "loss_scale": "4", "train_wall": "2417", "gb_free": "5.4", "wall": "51020"}
[2022-07-19 01:07:37,248][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 01:08:52,364][train_inner][INFO] - {"epoch": 2, "update": 1.239, "loss": "2.479", "ppl": "5.57", "wps": "18639", "ups": "0.08", "wpb": "235290", "bsz": "512", "num_updates": "22000", "lr": "8.24e-06", "gnorm": "1.423", "loss_scale": "4", "train_wall": "2432", "gb_free": "5.4", "wall": "53544"}
[2022-07-19 01:50:44,952][train_inner][INFO] - {"epoch": 2, "update": 1.251, "loss": "2.475", "ppl": "5.56", "wps": "18747.6", "ups": "0.08", "wpb": "235525", "bsz": "512", "num_updates": "22200", "lr": "8.224e-06", "gnorm": "1.428", "loss_scale": "4", "train_wall": "2420", "gb_free": "5.4", "wall": "56057"}
[2022-07-19 01:57:25,016][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 02:32:39,530][train_inner][INFO] - {"epoch": 2, "update": 1.262, "loss": "2.484", "ppl": "5.59", "wps": "18675.7", "ups": "0.08", "wpb": "234808", "bsz": "512", "num_updates": "22400", "lr": "8.208e-06", "gnorm": "1.412", "loss_scale": "2", "train_wall": "2422", "gb_free": "5.4", "wall": "58572"}
[2022-07-19 03:14:20,610][train_inner][INFO] - {"epoch": 2, "update": 1.273, "loss": "2.481", "ppl": "5.58", "wps": "18776.9", "ups": "0.08", "wpb": "234812", "bsz": "512", "num_updates": "22600", "lr": "8.192e-06", "gnorm": "1.401", "loss_scale": "4", "train_wall": "2409", "gb_free": "5.4", "wall": "61073"}
[2022-07-19 03:56:07,519][train_inner][INFO] - {"epoch": 2, "update": 1.284, "loss": "2.472", "ppl": "5.55", "wps": "18737.8", "ups": "0.08", "wpb": "234870", "bsz": "512", "num_updates": "22800", "lr": "8.176e-06", "gnorm": "1.402", "loss_scale": "8", "train_wall": "2415", "gb_free": "5.4", "wall": "63580"}
[2022-07-19 04:03:27,360][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 04:21:51,485][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 04:38:24,181][train_inner][INFO] - {"epoch": 2, "update": 1.296, "loss": "2.486", "ppl": "5.6", "wps": "18554.5", "ups": "0.08", "wpb": "235332", "bsz": "512", "num_updates": "23000", "lr": "8.16e-06", "gnorm": "1.392", "loss_scale": "2", "train_wall": "2444", "gb_free": "5.3", "wall": "66116"}
[2022-07-19 05:20:10,081][train_inner][INFO] - {"epoch": 2, "update": 1.307, "loss": "2.485", "ppl": "5.6", "wps": "18767.6", "ups": "0.08", "wpb": "235148", "bsz": "512", "num_updates": "23200", "lr": "8.144e-06", "gnorm": "1.438", "loss_scale": "4", "train_wall": "2414", "gb_free": "5.4", "wall": "68622"}
[2022-07-19 05:33:44,049][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 06:02:09,017][train_inner][INFO] - {"epoch": 2, "update": 1.318, "loss": "2.48", "ppl": "5.58", "wps": "18663", "ups": "0.08", "wpb": "235054", "bsz": "512", "num_updates": "23400", "lr": "8.128e-06", "gnorm": "1.446", "loss_scale": "2", "train_wall": "2427", "gb_free": "5.3", "wall": "71141"}
[2022-07-19 06:44:05,525][train_inner][INFO] - {"epoch": 2, "update": 1.33, "loss": "2.477", "ppl": "5.57", "wps": "18743.9", "ups": "0.08", "wpb": "235846", "bsz": "512", "num_updates": "23600", "lr": "8.112e-06", "gnorm": "1.37", "loss_scale": "4", "train_wall": "2424", "gb_free": "5.4", "wall": "73658"}
[2022-07-19 06:54:56,743][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 07:26:09,628][train_inner][INFO] - {"epoch": 2, "update": 1.341, "loss": "2.474", "ppl": "5.56", "wps": "18642.4", "ups": "0.08", "wpb": "235276", "bsz": "512", "num_updates": "23800", "lr": "8.096e-06", "gnorm": "1.424", "loss_scale": "2", "train_wall": "2432", "gb_free": "5.4", "wall": "76182"}
[2022-07-19 08:07:51,013][train_inner][INFO] - {"epoch": 2, "update": 1.352, "loss": "2.473", "ppl": "5.55", "wps": "18774", "ups": "0.08", "wpb": "234804", "bsz": "512", "num_updates": "24000", "lr": "8.08e-06", "gnorm": "1.439", "loss_scale": "4", "train_wall": "2409", "gb_free": "5.4", "wall": "78683"}
[2022-07-19 08:48:06,627][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 08:49:46,634][train_inner][INFO] - {"epoch": 2, "update": 1.363, "loss": "2.474", "ppl": "5.56", "wps": "18684.7", "ups": "0.08", "wpb": "235018", "bsz": "512", "num_updates": "24200", "lr": "8.064e-06", "gnorm": "1.375", "loss_scale": "4", "train_wall": "2423", "gb_free": "5.4", "wall": "81199"}
[2022-07-19 09:31:30,888][train_inner][INFO] - {"epoch": 2, "update": 1.375, "loss": "2.469", "ppl": "5.54", "wps": "18777", "ups": "0.08", "wpb": "235112", "bsz": "512", "num_updates": "24400", "lr": "8.048e-06", "gnorm": "1.413", "loss_scale": "4", "train_wall": "2412", "gb_free": "5.4", "wall": "83703"}
[2022-07-19 09:35:41,416][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 10:13:30,795][train_inner][INFO] - {"epoch": 2, "update": 1.386, "loss": "2.473", "ppl": "5.55", "wps": "18669", "ups": "0.08", "wpb": "235221", "bsz": "512", "num_updates": "24600", "lr": "8.032e-06", "gnorm": "1.418", "loss_scale": "2", "train_wall": "2427", "gb_free": "5.4", "wall": "86223"}
[2022-07-19 10:55:20,093][train_inner][INFO] - {"epoch": 2, "update": 1.397, "loss": "2.468", "ppl": "5.53", "wps": "18753.7", "ups": "0.08", "wpb": "235292", "bsz": "512", "num_updates": "24800", "lr": "8.016e-06", "gnorm": "1.381", "loss_scale": "4", "train_wall": "2417", "gb_free": "5.4", "wall": "88732"}
[2022-07-19 11:34:57,114][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 11:37:28,802][train_inner][INFO] - {"epoch": 2, "update": 1.408, "loss": "2.466", "ppl": "5.53", "wps": "18637.8", "ups": "0.08", "wpb": "235648", "bsz": "512", "num_updates": "25000", "lr": "8e-06", "gnorm": "1.382", "loss_scale": "4", "train_wall": "2436", "gb_free": "5.4", "wall": "91261"}
[2022-07-19 12:19:15,933][train_inner][INFO] - {"epoch": 2, "update": 1.42, "loss": "2.468", "ppl": "5.53", "wps": "18735.6", "ups": "0.08", "wpb": "234863", "bsz": "512", "num_updates": "25200", "lr": "7.984e-06", "gnorm": "1.396", "loss_scale": "4", "train_wall": "2415", "gb_free": "5.4", "wall": "93768"}
[2022-07-19 13:01:09,992][train_inner][INFO] - {"epoch": 2, "update": 1.431, "loss": "2.467", "ppl": "5.53", "wps": "18720.6", "ups": "0.08", "wpb": "235323", "bsz": "512", "num_updates": "25400", "lr": "7.968e-06", "gnorm": "1.384", "loss_scale": "8", "train_wall": "2422", "gb_free": "5.3", "wall": "96282"}
[2022-07-19 13:03:27,038][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 13:43:15,474][train_inner][INFO] - {"epoch": 2, "update": 1.442, "loss": "2.467", "ppl": "5.53", "wps": "18629.1", "ups": "0.08", "wpb": "235237", "bsz": "512", "num_updates": "25600", "lr": "7.952e-06", "gnorm": "1.391", "loss_scale": "4", "train_wall": "2434", "gb_free": "5.4", "wall": "98808"}
[2022-07-19 14:14:43,559][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 14:25:23,790][train_inner][INFO] - {"epoch": 2, "update": 1.453, "loss": "2.459", "ppl": "5.5", "wps": "18640.5", "ups": "0.08", "wpb": "235645", "bsz": "512", "num_updates": "25800", "lr": "7.936e-06", "gnorm": "1.376", "loss_scale": "4", "train_wall": "2436", "gb_free": "5.4", "wall": "101336"}
[2022-07-19 15:07:12,492][train_inner][INFO] - {"epoch": 2, "update": 1.465, "loss": "2.458", "ppl": "5.5", "wps": "18726.8", "ups": "0.08", "wpb": "234900", "bsz": "512", "num_updates": "26000", "lr": "7.92e-06", "gnorm": "1.395", "loss_scale": "4", "train_wall": "2417", "gb_free": "5.4", "wall": "103845"}
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-19 15:20:23,004][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 15:49:15,758][train_inner][INFO] - {"epoch": 2, "update": 1.476, "loss": "2.456", "ppl": "5.49", "wps": "18649.7", "ups": "0.08", "wpb": "235290", "bsz": "512", "num_updates": "26200", "lr": "7.904e-06", "gnorm": "1.387", "loss_scale": "4", "train_wall": "2431", "gb_free": "5.4", "wall": "106368"}
[2022-07-19 15:56:35,911][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 16:31:26,071][train_inner][INFO] - {"epoch": 2, "update": 1.487, "loss": "2.464", "ppl": "5.52", "wps": "18635.4", "ups": "0.08", "wpb": "235767", "bsz": "512", "num_updates": "26400", "lr": "7.888e-06", "gnorm": "1.398", "loss_scale": "2", "train_wall": "2436", "gb_free": "5.4", "wall": "108898"}
[2022-07-19 16:52:19,821][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 17:13:26,971][train_inner][INFO] - {"epoch": 2, "update": 1.499, "loss": "2.464", "ppl": "5.52", "wps": "18651.7", "ups": "0.08", "wpb": "235095", "bsz": "512", "num_updates": "26600", "lr": "7.872e-06", "gnorm": "1.391", "loss_scale": "2", "train_wall": "2425", "gb_free": "5.4", "wall": "111419"}
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-19 17:47:55,393][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 17:55:26,784][train_inner][INFO] - {"epoch": 2, "update": 1.51, "loss": "2.463", "ppl": "5.51", "wps": "18648.9", "ups": "0.08", "wpb": "234957", "bsz": "512", "num_updates": "26800", "lr": "7.856e-06", "gnorm": "1.383", "loss_scale": "2", "train_wall": "2424", "gb_free": "5.4", "wall": "113939"}
[2022-07-19 18:00:53,099][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-19 18:37:26,604][train_inner][INFO] - {"epoch": 2, "update": 1.521, "loss": "2.454", "ppl": "5.48", "wps": "18647.3", "ups": "0.08", "wpb": "234938", "bsz": "512", "num_updates": "27000", "lr": "7.84e-06", "gnorm": "1.392", "loss_scale": "1", "train_wall": "2423", "gb_free": "5.4", "wall": "116459"}
[2022-07-19 19:19:12,387][train_inner][INFO] - {"epoch": 2, "update": 1.532, "loss": "2.459", "ppl": "5.5", "wps": "18751.5", "ups": "0.08", "wpb": "234935", "bsz": "512", "num_updates": "27200", "lr": "7.824e-06", "gnorm": "1.397", "loss_scale": "2", "train_wall": "2410", "gb_free": "5.4", "wall": "118964"}
[2022-07-19 20:01:05,419][train_inner][INFO] - {"epoch": 2, "update": 1.544, "loss": "2.453", "ppl": "5.48", "wps": "18735.9", "ups": "0.08", "wpb": "235420", "bsz": "512", "num_updates": "27400", "lr": "7.808e-06", "gnorm": "1.384", "loss_scale": "4", "train_wall": "2419", "gb_free": "5.4", "wall": "121478"}
[2022-07-19 20:42:53,598][train_inner][INFO] - {"epoch": 2, "update": 1.555, "loss": "2.454", "ppl": "5.48", "wps": "18780.8", "ups": "0.08", "wpb": "235527", "bsz": "512", "num_updates": "27600", "lr": "7.792e-06", "gnorm": "1.376", "loss_scale": "8", "train_wall": "2416", "gb_free": "5.4", "wall": "123986"}
[2022-07-19 21:02:20,329][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-19 21:06:45,519][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 21:25:15,976][train_inner][INFO] - {"epoch": 2, "update": 1.566, "loss": "2.447", "ppl": "5.45", "wps": "18512.3", "ups": "0.08", "wpb": "235326", "bsz": "512", "num_updates": "27800", "lr": "7.776e-06", "gnorm": "1.408", "loss_scale": "2", "train_wall": "2448", "gb_free": "5.4", "wall": "126528"}
[2022-07-19 22:07:12,579][train_inner][INFO] - {"epoch": 2, "update": 1.577, "loss": "2.447", "ppl": "5.45", "wps": "18675.9", "ups": "0.08", "wpb": "234999", "bsz": "512", "num_updates": "28000", "lr": "7.76e-06", "gnorm": "1.408", "loss_scale": "4", "train_wall": "2421", "gb_free": "5.4", "wall": "129045"}
[2022-07-19 22:44:40,410][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-19 22:49:16,033][train_inner][INFO] - {"epoch": 2, "update": 1.589, "loss": "2.449", "ppl": "5.46", "wps": "18657.4", "ups": "0.08", "wpb": "235405", "bsz": "512", "num_updates": "28200", "lr": "7.744e-06", "gnorm": "1.409", "loss_scale": "2", "train_wall": "2432", "gb_free": "5.4", "wall": "131568"}
[2022-07-19 23:35:55,445][train_inner][INFO] - {"epoch": 2, "update": 1.6, "loss": "2.452", "ppl": "5.47", "wps": "18755.6", "ups": "0.08", "wpb": "235271", "bsz": "512", "num_updates": "28400", "lr": "7.728e-06", "gnorm": "1.4", "loss_scale": "2", "train_wall": "2417", "gb_free": "5.4", "wall": "134368"}
[2022-07-20 00:17:52,716][train_inner][INFO] - {"epoch": 2, "update": 1.611, "loss": "2.447", "ppl": "5.45", "wps": "18724.4", "ups": "0.08", "wpb": "235671", "bsz": "512", "num_updates": "28600", "lr": "7.712e-06", "gnorm": "1.396", "loss_scale": "4", "train_wall": "2423", "gb_free": "5.4", "wall": "136885"}
[2022-07-20 00:36:52,726][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-20 00:59:53,323][train_inner][INFO] - {"epoch": 2, "update": 1.622, "loss": "2.448", "ppl": "5.46", "wps": "18651.6", "ups": "0.08", "wpb": "235066", "bsz": "512", "num_updates": "28800", "lr": "7.696e-06", "gnorm": "1.4", "loss_scale": "4", "train_wall": "2429", "gb_free": "5.4", "wall": "139405"}
[2022-07-20 01:06:34,397][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 01:41:52,125][train_inner][INFO] - {"epoch": 2, "update": 1.634, "loss": "2.448", "ppl": "5.46", "wps": "18668", "ups": "0.08", "wpb": "235104", "bsz": "512", "num_updates": "29000", "lr": "7.68e-06", "gnorm": "1.421", "loss_scale": "2", "train_wall": "2426", "gb_free": "5.4", "wall": "141924"}
[2022-07-20 02:23:44,811][train_inner][INFO] - {"epoch": 2, "update": 1.645, "loss": "2.448", "ppl": "5.46", "wps": "18747.6", "ups": "0.08", "wpb": "235533", "bsz": "512", "num_updates": "29200", "lr": "7.664e-06", "gnorm": "1.379", "loss_scale": "4", "train_wall": "2421", "gb_free": "5.3", "wall": "144437"}
[2022-07-20 03:05:39,724][train_inner][INFO] - {"epoch": 2, "update": 1.656, "loss": "2.439", "ppl": "5.42", "wps": "18717.7", "ups": "0.08", "wpb": "235367", "bsz": "512", "num_updates": "29400", "lr": "7.648e-06", "gnorm": "1.393", "loss_scale": "8", "train_wall": "2423", "gb_free": "5.4", "wall": "146952"}
[2022-07-20 03:06:17,826][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-20 03:47:48,501][train_inner][INFO] - {"epoch": 2, "update": 1.668, "loss": "2.442", "ppl": "5.43", "wps": "18625.8", "ups": "0.08", "wpb": "235502", "bsz": "512", "num_updates": "29600", "lr": "7.632e-06", "gnorm": "1.393", "loss_scale": "4", "train_wall": "2436", "gb_free": "5.4", "wall": "149481"}
[2022-07-20 04:03:03,836][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-20 04:29:52,145][train_inner][INFO] - {"epoch": 2, "update": 1.679, "loss": "2.441", "ppl": "5.43", "wps": "18636.7", "ups": "0.08", "wpb": "235162", "bsz": "512", "num_updates": "29800", "lr": "7.616e-06", "gnorm": "1.403", "loss_scale": "4", "train_wall": "2431", "gb_free": "5.4", "wall": "152004"}
[2022-07-20 04:59:42,856][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-20 05:11:52,713][train_inner][INFO] - {"epoch": 2, "update": 1.69, "loss": "2.44", "ppl": "5.43", "wps": "18646.2", "ups": "0.08", "wpb": "234995", "bsz": "512", "num_updates": "30000", "lr": "7.6e-06", "gnorm": "1.416", "loss_scale": "4", "train_wall": "2428", "gb_free": "5.4", "wall": "154525"}
[2022-07-20 05:23:09,650][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 05:53:52,627][train_inner][INFO] - {"epoch": 2, "update": 1.701, "loss": "2.442", "ppl": "5.43", "wps": "18654", "ups": "0.08", "wpb": "235032", "bsz": "512", "num_updates": "30200", "lr": "7.584e-06", "gnorm": "1.432", "loss_scale": "2", "train_wall": "2427", "gb_free": "5.4", "wall": "157045"}
wandb: Network error (ReadTimeout), entering retry loop.
wandb: ERROR Error while calling W&B API: internal database error (<Response [500]>)
wandb: Network error (ReadTimeout), entering retry loop.
wandb: ERROR Error while calling W&B API: internal database error (<Response [500]>)
[2022-07-20 06:07:42,754][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-20 06:35:57,211][train_inner][INFO] - {"epoch": 2, "update": 1.713, "loss": "2.438", "ppl": "5.42", "wps": "18649.1", "ups": "0.08", "wpb": "235406", "bsz": "512", "num_updates": "30400", "lr": "7.568e-06", "gnorm": "1.409", "loss_scale": "1", "train_wall": "2432", "gb_free": "5.4", "wall": "159569"}
[2022-07-20 07:17:48,103][train_inner][INFO] - {"epoch": 2, "update": 1.724, "loss": "2.431", "ppl": "5.39", "wps": "18760.1", "ups": "0.08", "wpb": "235523", "bsz": "512", "num_updates": "30600", "lr": "7.552e-06", "gnorm": "1.388", "loss_scale": "2", "train_wall": "2419", "gb_free": "5.4", "wall": "162080"}
[2022-07-20 07:59:36,820][train_inner][INFO] - {"epoch": 2, "update": 1.735, "loss": "2.434", "ppl": "5.4", "wps": "18740", "ups": "0.08", "wpb": "235067", "bsz": "512", "num_updates": "30800", "lr": "7.536e-06", "gnorm": "1.394", "loss_scale": "4", "train_wall": "2416", "gb_free": "5.4", "wall": "164589"}
[2022-07-20 08:41:34,157][train_inner][INFO] - {"epoch": 2, "update": 1.746, "loss": "2.436", "ppl": "5.41", "wps": "18736.8", "ups": "0.08", "wpb": "235834", "bsz": "512", "num_updates": "31000", "lr": "7.52e-06", "gnorm": "1.378", "loss_scale": "4", "train_wall": "2425", "gb_free": "5.4", "wall": "167106"}
[2022-07-20 08:47:23,522][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 09:23:32,862][train_inner][INFO] - {"epoch": 2, "update": 1.758, "loss": "2.43", "ppl": "5.39", "wps": "18632.9", "ups": "0.08", "wpb": "234654", "bsz": "512", "num_updates": "31200", "lr": "7.504e-06", "gnorm": "1.39", "loss_scale": "2", "train_wall": "2426", "gb_free": "5.4", "wall": "169625"}
[2022-07-20 09:50:17,044][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-20 10:05:31,476][train_inner][INFO] - {"epoch": 2, "update": 1.769, "loss": "2.429", "ppl": "5.39", "wps": "18639.8", "ups": "0.08", "wpb": "234732", "bsz": "512", "num_updates": "31400", "lr": "7.488e-06", "gnorm": "1.429", "loss_scale": "2", "train_wall": "2426", "gb_free": "5.4", "wall": "172144"}
[2022-07-20 10:47:20,699][train_inner][INFO] - {"epoch": 2, "update": 1.78, "loss": "2.429", "ppl": "5.38", "wps": "18729.3", "ups": "0.08", "wpb": "234979", "bsz": "512", "num_updates": "31600", "lr": "7.472e-06", "gnorm": "1.422", "loss_scale": "4", "train_wall": "2417", "gb_free": "5.4", "wall": "174653"}
[2022-07-20 11:04:16,016][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 11:29:19,661][train_inner][INFO] - {"epoch": 2, "update": 1.791, "loss": "2.434", "ppl": "5.41", "wps": "18692.4", "ups": "0.08", "wpb": "235427", "bsz": "512", "num_updates": "31800", "lr": "7.456e-06", "gnorm": "1.407", "loss_scale": "2", "train_wall": "2427", "gb_free": "5.4", "wall": "177172"}
[2022-07-20 12:11:08,449][train_inner][INFO] - {"epoch": 2, "update": 1.803, "loss": "2.428", "ppl": "5.38", "wps": "18745.3", "ups": "0.08", "wpb": "235140", "bsz": "512", "num_updates": "32000", "lr": "7.44e-06", "gnorm": "1.395", "loss_scale": "4", "train_wall": "2415", "gb_free": "5.4", "wall": "179681"}
[2022-07-20 12:12:49,044][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 12:53:22,484][train_inner][INFO] - {"epoch": 2, "update": 1.814, "loss": "2.427", "ppl": "5.38", "wps": "18569.1", "ups": "0.08", "wpb": "235271", "bsz": "512", "num_updates": "32200", "lr": "7.424e-06", "gnorm": "1.415", "loss_scale": "2", "train_wall": "2435", "gb_free": "5.3", "wall": "182215"}
[2022-07-20 13:19:02,887][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 13:28:28,151][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-20 13:35:46,539][train_inner][INFO] - {"epoch": 2, "update": 1.825, "loss": "2.416", "ppl": "5.34", "wps": "18521.4", "ups": "0.08", "wpb": "235597", "bsz": "512", "num_updates": "32400", "lr": "7.408e-06", "gnorm": "1.389", "loss_scale": "1", "train_wall": "2445", "gb_free": "5.4", "wall": "184759"}
[2022-07-20 14:17:35,716][train_inner][INFO] - {"epoch": 2, "update": 1.837, "loss": "2.429", "ppl": "5.39", "wps": "18743.8", "ups": "0.08", "wpb": "235158", "bsz": "512", "num_updates": "32600", "lr": "7.392e-06", "gnorm": "1.463", "loss_scale": "1", "train_wall": "2413", "gb_free": "5.3", "wall": "187268"}
[2022-07-20 14:59:31,868][train_inner][INFO] - {"epoch": 2, "update": 1.848, "loss": "2.415", "ppl": "5.33", "wps": "18714.3", "ups": "0.08", "wpb": "235440", "bsz": "512", "num_updates": "32800", "lr": "7.376e-06", "gnorm": "1.39", "loss_scale": "2", "train_wall": "2419", "gb_free": "5.4", "wall": "189784"}
[2022-07-20 15:41:27,881][train_inner][INFO] - {"epoch": 2, "update": 1.859, "loss": "2.407", "ppl": "5.3", "wps": "18688.3", "ups": "0.08", "wpb": "235100", "bsz": "512", "num_updates": "33000", "lr": "7.36e-06", "gnorm": "1.395", "loss_scale": "4", "train_wall": "2421", "gb_free": "5.4", "wall": "192300"}
[2022-07-20 15:43:08,904][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 16:23:31,248][train_inner][INFO] - {"epoch": 2, "update": 1.87, "loss": "2.424", "ppl": "5.37", "wps": "18631.2", "ups": "0.08", "wpb": "235067", "bsz": "512", "num_updates": "33200", "lr": "7.344e-06", "gnorm": "1.386", "loss_scale": "2", "train_wall": "2430", "gb_free": "5.4", "wall": "194823"}
[2022-07-20 16:41:45,544][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 17:05:40,836][train_inner][INFO] - {"epoch": 2, "update": 1.882, "loss": "2.423", "ppl": "5.36", "wps": "18608.1", "ups": "0.08", "wpb": "235353", "bsz": "512", "num_updates": "33400", "lr": "7.328e-06", "gnorm": "1.411", "loss_scale": "2", "train_wall": "2431", "gb_free": "5.4", "wall": "197353"}
[2022-07-20 17:44:16,034][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 17:47:48,550][train_inner][INFO] - {"epoch": 2, "update": 1.893, "loss": "2.423", "ppl": "5.36", "wps": "18590", "ups": "0.08", "wpb": "234950", "bsz": "512", "num_updates": "33600", "lr": "7.312e-06", "gnorm": "1.408", "loss_scale": "2", "train_wall": "2430", "gb_free": "5.4", "wall": "199881"}
[2022-07-20 18:29:47,101][train_inner][INFO] - {"epoch": 2, "update": 1.904, "loss": "2.415", "ppl": "5.33", "wps": "18701.8", "ups": "0.08", "wpb": "235506", "bsz": "512", "num_updates": "33800", "lr": "7.296e-06", "gnorm": "1.442", "loss_scale": "2", "train_wall": "2421", "gb_free": "5.4", "wall": "202399"}
[2022-07-20 18:53:57,241][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 19:11:56,437][train_inner][INFO] - {"epoch": 2, "update": 1.915, "loss": "2.417", "ppl": "5.34", "wps": "18598.3", "ups": "0.08", "wpb": "235207", "bsz": "512", "num_updates": "34000", "lr": "7.28e-06", "gnorm": "1.418", "loss_scale": "2", "train_wall": "2430", "gb_free": "5.3", "wall": "204929"}
[2022-07-20 19:43:20,936][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2022-07-20 19:54:03,354][train_inner][INFO] - {"epoch": 2, "update": 1.927, "loss": "2.418", "ppl": "5.34", "wps": "18649.1", "ups": "0.08", "wpb": "235623", "bsz": "512", "num_updates": "34200", "lr": "7.264e-06", "gnorm": "1.457", "loss_scale": "1", "train_wall": "2434", "gb_free": "5.4", "wall": "207455"}
[2022-07-20 20:35:55,177][train_inner][INFO] - {"epoch": 2, "update": 1.938, "loss": "2.42", "ppl": "5.35", "wps": "18735.6", "ups": "0.08", "wpb": "235302", "bsz": "512", "num_updates": "34400", "lr": "7.248e-06", "gnorm": "1.393", "loss_scale": "1", "train_wall": "2419", "gb_free": "5.4", "wall": "209967"}
[2022-07-20 21:17:40,112][train_inner][INFO] - {"epoch": 2, "update": 1.949, "loss": "2.41", "ppl": "5.32", "wps": "18752.4", "ups": "0.08", "wpb": "234868", "bsz": "512", "num_updates": "34600", "lr": "7.232e-06", "gnorm": "1.403", "loss_scale": "2", "train_wall": "2413", "gb_free": "5.4", "wall": "212472"}
[2022-07-20 21:59:24,404][train_inner][INFO] - {"epoch": 2, "update": 1.96, "loss": "2.408", "ppl": "5.31", "wps": "18761.6", "ups": "0.08", "wpb": "234922", "bsz": "512", "num_updates": "34800", "lr": "7.216e-06", "gnorm": "1.364", "loss_scale": "4", "train_wall": "2412", "gb_free": "5.4", "wall": "214976"}
[2022-07-20 22:18:13,406][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 22:41:24,289][train_inner][INFO] - {"epoch": 2, "update": 1.972, "loss": "2.406", "ppl": "5.3", "wps": "18645.9", "ups": "0.08", "wpb": "234927", "bsz": "512", "num_updates": "35000", "lr": "7.2e-06", "gnorm": "1.445", "loss_scale": "2", "train_wall": "2428", "gb_free": "5.4", "wall": "217496"}
[2022-07-20 23:15:30,205][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 23:23:28,706][train_inner][INFO] - {"epoch": 2, "update": 1.983, "loss": "2.408", "ppl": "5.31", "wps": "18658.4", "ups": "0.08", "wpb": "235508", "bsz": "512", "num_updates": "35200", "lr": "7.184e-06", "gnorm": "1.388", "loss_scale": "2", "train_wall": "2432", "gb_free": "5.4", "wall": "220021"}
[2022-07-21 00:05:15,513][train_inner][INFO] - {"epoch": 2, "update": 1.994, "loss": "2.406", "ppl": "5.3", "wps": "18743.6", "ups": "0.08", "wpb": "234932", "bsz": "512", "num_updates": "35400", "lr": "7.168e-06", "gnorm": "1.408", "loss_scale": "2", "train_wall": "2415", "gb_free": "5.4", "wall": "222528"}
[2022-07-21 00:26:36,818][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-21 00:26:53,758][valid][INFO] - {"epoch": 2, "valid_loss": "2.349", "valid_ppl": "5.09", "valid_wps": "51970.9", "valid_wpb": "7295.6", "valid_bsz": "15.8", "valid_num_updates": "35503", "valid_best_loss": "2.349"}
[2022-07-21 00:26:53,761][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 35503 updates
[2022-07-21 00:26:53,763][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-18/10-16-09/0/checkpoints/checkpoint2.pt
[2022-07-21 00:27:19,976][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-18/10-16-09/0/checkpoints/checkpoint2.pt
[2022-07-21 00:28:02,755][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 35503 updates, score 2.349) (writing took 68.9933302840218 seconds)
[2022-07-21 00:28:02,756][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2022-07-21 00:28:02,779][train][INFO] - {"epoch": 2, "train_loss": "2.459", "train_ppl": "5.5", "train_wps": "18655.8", "train_ups": "0.08", "train_wpb": "235189", "train_bsz": "512", "train_num_updates": "35503", "train_lr": "7.15976e-06", "train_gnorm": "1.408", "train_loss_scale": "4", "train_train_wall": "215180", "train_gb_free": "5.4", "train_wall": "223895"}
[2022-07-21 00:28:02,794][fairseq_cli.train][INFO] - done training in 223858.8 seconds
slurmstepd: error: *** JOB 9752789 ON r32n2 CANCELLED AT 2022-07-21T09:48:44 ***
