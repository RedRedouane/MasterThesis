[2022-07-20 09:39:27,067][HYDRA] Launching 1 jobs locally
[2022-07-20 09:39:27,067][HYDRA] 	#0 : 
[2022-07-20 09:39:29,351][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'RoBERTa_encdec', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 4, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 4, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 20000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [64], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'roberta_enc_dec', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'adaptive_input': False, 'encoder': {'_name': None, 'embed_path': None, 'embed_dim': 768, 'ffn_embed_dim': 3072, 'layers': 12, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None}, 'max_source_positions': 1024, 'decoder': {'_name': None, 'embed_path': None, 'embed_dim': 512, 'ffn_embed_dim': 2048, 'layers': 6, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None, 'input_dim': 512, 'output_dim': 512}, 'max_target_positions': 1024, 'share_decoder_input_output_embed': True, 'share_all_embeddings': True, 'no_token_positional_embeddings': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'layernorm_embedding': False, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'no_cross_attention': False, 'cross_self_attention': False, 'quant_noise': {'_name': None, 'pq': 0.0, 'pq_block_size': 8, 'scalar': 0.0}, 'min_params_to_wrap': 100000000, 'char_inputs': False, 'relu_dropout': 0.0, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'export': False, 'no_decoder_final_norm': False, 'max_positions': 512, 'encoder_embed_dim': 768, 'encoder_layers': 12, 'encoder_ffn_embed_dim': 3072, 'hack_layernorm_embedding': False, 'pretrained_mlm_checkpoint': '/home/dahmanir/lisa/Models/RobBERT_maskless.pt', 'load_pretrained_mlm_checkpoint': None}, 'task': {'_name': 'translation', 'data': '/home/dahmanir/lisa/Datasets/wiki_binarized', 'source_lang': 'source', 'target_lang': 'target', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 512, 'max_target_positions': 512, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 20000.0, 'lr': [1e-05]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2022-07-20 09:39:29,521][fairseq.tasks.translation][INFO] - [source] dictionary: 39984 types
[2022-07-20 09:39:29,521][fairseq.tasks.translation][INFO] - [target] dictionary: 39984 types
[2022-07-20 09:39:41,455][fairseq.models.fairseq_model][WARNING] - using 'args' is deprecated, please update your code to use dataclass config
encoder.embed_tokens.weight False torch.Size([39984, 768])
encoder.layers.0.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.0.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.0.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.0.fc1.weight False torch.Size([3072, 768])
encoder.layers.0.fc1.bias False torch.Size([3072])
encoder.layers.0.fc2.weight False torch.Size([768, 3072])
encoder.layers.0.fc2.bias False torch.Size([768])
encoder.layers.0.final_layer_norm.weight False torch.Size([768])
encoder.layers.0.final_layer_norm.bias False torch.Size([768])
encoder.layers.1.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.1.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.1.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.1.fc1.weight False torch.Size([3072, 768])
encoder.layers.1.fc1.bias False torch.Size([3072])
encoder.layers.1.fc2.weight False torch.Size([768, 3072])
encoder.layers.1.fc2.bias False torch.Size([768])
encoder.layers.1.final_layer_norm.weight False torch.Size([768])
encoder.layers.1.final_layer_norm.bias False torch.Size([768])
encoder.layers.2.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.2.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.2.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.2.fc1.weight False torch.Size([3072, 768])
encoder.layers.2.fc1.bias False torch.Size([3072])
encoder.layers.2.fc2.weight False torch.Size([768, 3072])
encoder.layers.2.fc2.bias False torch.Size([768])
encoder.layers.2.final_layer_norm.weight False torch.Size([768])
encoder.layers.2.final_layer_norm.bias False torch.Size([768])
encoder.layers.3.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.3.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.3.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.3.fc1.weight False torch.Size([3072, 768])
encoder.layers.3.fc1.bias False torch.Size([3072])
encoder.layers.3.fc2.weight False torch.Size([768, 3072])
encoder.layers.3.fc2.bias False torch.Size([768])
encoder.layers.3.final_layer_norm.weight False torch.Size([768])
encoder.layers.3.final_layer_norm.bias False torch.Size([768])
encoder.layers.4.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.4.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.4.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.4.fc1.weight False torch.Size([3072, 768])
encoder.layers.4.fc1.bias False torch.Size([3072])
encoder.layers.4.fc2.weight False torch.Size([768, 3072])
encoder.layers.4.fc2.bias False torch.Size([768])
encoder.layers.4.final_layer_norm.weight False torch.Size([768])
encoder.layers.4.final_layer_norm.bias False torch.Size([768])
encoder.layers.5.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.5.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.5.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.5.fc1.weight False torch.Size([3072, 768])
encoder.layers.5.fc1.bias False torch.Size([3072])
encoder.layers.5.fc2.weight False torch.Size([768, 3072])
encoder.layers.5.fc2.bias False torch.Size([768])
encoder.layers.5.final_layer_norm.weight False torch.Size([768])
encoder.layers.5.final_layer_norm.bias False torch.Size([768])
encoder.layers.6.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.6.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.6.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.6.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.6.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.6.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.6.fc1.weight False torch.Size([3072, 768])
encoder.layers.6.fc1.bias False torch.Size([3072])
encoder.layers.6.fc2.weight False torch.Size([768, 3072])
encoder.layers.6.fc2.bias False torch.Size([768])
encoder.layers.6.final_layer_norm.weight False torch.Size([768])
encoder.layers.6.final_layer_norm.bias False torch.Size([768])
encoder.layers.7.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.7.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.7.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.7.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.7.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.7.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.7.fc1.weight False torch.Size([3072, 768])
encoder.layers.7.fc1.bias False torch.Size([3072])
encoder.layers.7.fc2.weight False torch.Size([768, 3072])
encoder.layers.7.fc2.bias False torch.Size([768])
encoder.layers.7.final_layer_norm.weight False torch.Size([768])
encoder.layers.7.final_layer_norm.bias False torch.Size([768])
encoder.layers.8.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.8.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.8.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.8.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.8.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.8.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.8.fc1.weight False torch.Size([3072, 768])
encoder.layers.8.fc1.bias False torch.Size([3072])
encoder.layers.8.fc2.weight False torch.Size([768, 3072])
encoder.layers.8.fc2.bias False torch.Size([768])
encoder.layers.8.final_layer_norm.weight False torch.Size([768])
encoder.layers.8.final_layer_norm.bias False torch.Size([768])
encoder.layers.9.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.9.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.9.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.9.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.9.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.9.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.9.fc1.weight False torch.Size([3072, 768])
encoder.layers.9.fc1.bias False torch.Size([3072])
encoder.layers.9.fc2.weight False torch.Size([768, 3072])
encoder.layers.9.fc2.bias False torch.Size([768])
encoder.layers.9.final_layer_norm.weight False torch.Size([768])
encoder.layers.9.final_layer_norm.bias False torch.Size([768])
encoder.layers.10.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.10.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.10.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.10.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.10.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.10.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.10.fc1.weight False torch.Size([3072, 768])
encoder.layers.10.fc1.bias False torch.Size([3072])
encoder.layers.10.fc2.weight False torch.Size([768, 3072])
encoder.layers.10.fc2.bias False torch.Size([768])
encoder.layers.10.final_layer_norm.weight False torch.Size([768])
encoder.layers.10.final_layer_norm.bias False torch.Size([768])
encoder.layers.11.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.11.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.11.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.11.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.11.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.11.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.11.fc1.weight False torch.Size([3072, 768])
encoder.layers.11.fc1.bias False torch.Size([3072])
encoder.layers.11.fc2.weight False torch.Size([768, 3072])
encoder.layers.11.fc2.bias False torch.Size([768])
encoder.layers.11.final_layer_norm.weight False torch.Size([768])
encoder.layers.11.final_layer_norm.bias False torch.Size([768])
decoder.layers.0.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.0.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.0.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.0.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.0.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.0.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.0.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.0.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.0.fc1.weight True torch.Size([3072, 768])
decoder.layers.0.fc1.bias True torch.Size([3072])
decoder.layers.0.fc2.weight True torch.Size([768, 3072])
decoder.layers.0.fc2.bias True torch.Size([768])
decoder.layers.0.final_layer_norm.weight True torch.Size([768])
decoder.layers.0.final_layer_norm.bias True torch.Size([768])
decoder.layers.1.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.1.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.1.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.1.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.1.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.1.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.1.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.1.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.1.fc1.weight True torch.Size([3072, 768])
decoder.layers.1.fc1.bias True torch.Size([3072])
decoder.layers.1.fc2.weight True torch.Size([768, 3072])
decoder.layers.1.fc2.bias True torch.Size([768])
decoder.layers.1.final_layer_norm.weight True torch.Size([768])
decoder.layers.1.final_layer_norm.bias True torch.Size([768])
decoder.layers.2.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.2.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.2.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.2.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.2.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.2.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.2.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.2.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.2.fc1.weight True torch.Size([3072, 768])
decoder.layers.2.fc1.bias True torch.Size([3072])
decoder.layers.2.fc2.weight True torch.Size([768, 3072])
decoder.layers.2.fc2.bias True torch.Size([768])
decoder.layers.2.final_layer_norm.weight True torch.Size([768])
decoder.layers.2.final_layer_norm.bias True torch.Size([768])
decoder.layers.3.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.3.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.3.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.3.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.3.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.3.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.3.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.3.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.3.fc1.weight True torch.Size([3072, 768])
decoder.layers.3.fc1.bias True torch.Size([3072])
decoder.layers.3.fc2.weight True torch.Size([768, 3072])
decoder.layers.3.fc2.bias True torch.Size([768])
decoder.layers.3.final_layer_norm.weight True torch.Size([768])
decoder.layers.3.final_layer_norm.bias True torch.Size([768])
decoder.layers.4.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.4.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.4.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.4.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.4.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.4.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.4.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.4.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.4.fc1.weight True torch.Size([3072, 768])
decoder.layers.4.fc1.bias True torch.Size([3072])
decoder.layers.4.fc2.weight True torch.Size([768, 3072])
decoder.layers.4.fc2.bias True torch.Size([768])
decoder.layers.4.final_layer_norm.weight True torch.Size([768])
decoder.layers.4.final_layer_norm.bias True torch.Size([768])
decoder.layers.5.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.5.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.5.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.5.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.5.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.5.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.5.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.5.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.5.fc1.weight True torch.Size([3072, 768])
decoder.layers.5.fc1.bias True torch.Size([3072])
decoder.layers.5.fc2.weight True torch.Size([768, 3072])
decoder.layers.5.fc2.bias True torch.Size([768])
decoder.layers.5.final_layer_norm.weight True torch.Size([768])
decoder.layers.5.final_layer_norm.bias True torch.Size([768])
decoder.layers.6.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.6.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.6.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.6.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.6.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.6.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.6.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.6.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.6.fc1.weight True torch.Size([3072, 768])
decoder.layers.6.fc1.bias True torch.Size([3072])
decoder.layers.6.fc2.weight True torch.Size([768, 3072])
decoder.layers.6.fc2.bias True torch.Size([768])
decoder.layers.6.final_layer_norm.weight True torch.Size([768])
decoder.layers.6.final_layer_norm.bias True torch.Size([768])
decoder.layers.7.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.7.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.7.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.7.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.7.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.7.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.7.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.7.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.7.fc1.weight True torch.Size([3072, 768])
decoder.layers.7.fc1.bias True torch.Size([3072])
decoder.layers.7.fc2.weight True torch.Size([768, 3072])
decoder.layers.7.fc2.bias True torch.Size([768])
decoder.layers.7.final_layer_norm.weight True torch.Size([768])
decoder.layers.7.final_layer_norm.bias True torch.Size([768])
decoder.layers.8.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.8.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.8.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.8.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.8.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.8.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.8.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.8.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.8.fc1.weight True torch.Size([3072, 768])
decoder.layers.8.fc1.bias True torch.Size([3072])
decoder.layers.8.fc2.weight True torch.Size([768, 3072])
decoder.layers.8.fc2.bias True torch.Size([768])
decoder.layers.8.final_layer_norm.weight True torch.Size([768])
decoder.layers.8.final_layer_norm.bias True torch.Size([768])
decoder.layers.9.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.9.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.9.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.9.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.9.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.9.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.9.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.9.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.9.fc1.weight True torch.Size([3072, 768])
decoder.layers.9.fc1.bias True torch.Size([3072])
decoder.layers.9.fc2.weight True torch.Size([768, 3072])
decoder.layers.9.fc2.bias True torch.Size([768])
decoder.layers.9.final_layer_norm.weight True torch.Size([768])
decoder.layers.9.final_layer_norm.bias True torch.Size([768])
decoder.layers.10.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.10.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.10.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.10.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.10.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.10.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.10.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.10.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.10.fc1.weight True torch.Size([3072, 768])
decoder.layers.10.fc1.bias True torch.Size([3072])
decoder.layers.10.fc2.weight True torch.Size([768, 3072])
decoder.layers.10.fc2.bias True torch.Size([768])
decoder.layers.10.final_layer_norm.weight True torch.Size([768])
decoder.layers.10.final_layer_norm.bias True torch.Size([768])
decoder.layers.11.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.11.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.11.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.11.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.11.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.11.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.11.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.11.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.11.fc1.weight True torch.Size([3072, 768])
decoder.layers.11.fc1.bias True torch.Size([3072])
decoder.layers.11.fc2.weight True torch.Size([768, 3072])
decoder.layers.11.fc2.bias True torch.Size([768])
decoder.layers.11.final_layer_norm.weight True torch.Size([768])
decoder.layers.11.final_layer_norm.bias True torch.Size([768])
decoder.output_projection.bias False torch.Size([39984])
decoder.output_projection.dense.weight True torch.Size([768, 768])
decoder.output_projection.dense.bias True torch.Size([768])
decoder.output_projection.layer_norm.weight True torch.Size([768])
decoder.output_projection.layer_norm.bias True torch.Size([768])
[2022-07-20 09:39:45,827][fairseq_cli.train][INFO] - RobertaEncDecModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39984, 768, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39984, 768, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): RobertaLMHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[2022-07-20 09:39:45,833][fairseq_cli.train][INFO] - task: TranslationTask
[2022-07-20 09:39:45,833][fairseq_cli.train][INFO] - model: RobertaEncDecModel
[2022-07-20 09:39:45,833][fairseq_cli.train][INFO] - criterion: LabelSmoothedCrossEntropyCriterion
[2022-07-20 09:39:45,837][fairseq_cli.train][INFO] - num. shared model params: 229,815,600 (num. trained: 114,013,440)
[2022-07-20 09:39:45,840][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2022-07-20 09:39:45,847][fairseq.data.data_utils][INFO] - loaded 3,123 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/valid.source-target.source
[2022-07-20 09:39:45,848][fairseq.data.data_utils][INFO] - loaded 3,123 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/valid.source-target.target
[2022-07-20 09:39:45,848][fairseq.tasks.translation][INFO] - /home/dahmanir/lisa/Datasets/wiki_binarized valid source-target 3123 examples
[2022-07-20 09:39:49,888][fairseq.trainer][INFO] - detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
[2022-07-20 09:39:49,888][fairseq.trainer][INFO] - detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
[2022-07-20 09:39:49,889][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-07-20 09:39:49,889][fairseq.utils][INFO] - rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = NVIDIA GeForce GTX 1080 Ti              
[2022-07-20 09:39:49,889][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-07-20 09:39:49,889][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2022-07-20 09:39:49,890][fairseq_cli.train][INFO] - max tokens per device = None and max sentences per device = 4
[2022-07-20 09:39:49,892][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2022-07-20 09:39:49,892][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2022-07-20 09:39:49,892][fairseq.trainer][INFO] - loading train data for epoch 1
[2022-07-20 09:39:49,906][fairseq.data.data_utils][INFO] - loaded 21,866 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/train.source-target.source
[2022-07-20 09:39:49,908][fairseq.data.data_utils][INFO] - loaded 21,866 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/train.source-target.target
[2022-07-20 09:39:49,909][fairseq.tasks.translation][INFO] - /home/dahmanir/lisa/Datasets/wiki_binarized train source-target 21866 examples
[2022-07-20 09:39:49,918][fairseq.tasks.fairseq_task][WARNING] - 6 samples have invalid sizes and will be skipped, max_positions=(512, 512), first few sample ids=[4345, 8071, 5665, 126, 8210, 2220]
[2022-07-20 09:39:49,936][fairseq.trainer][INFO] - NOTE: your device does NOT support faster training with --fp16 or --amp, please switch to FP32 which is likely to be faster
[2022-07-20 09:39:49,965][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
wandb: Currently logged in as: redredouane. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.21 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.19
wandb: Run data is saved locally in /home/dahmanir/multirun/2022-07-20/09-39-25/0/wandb/run-20220720_093954-2cueo7h7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run checkpoints
wandb:  View project at https://wandb.ai/redredouane/RoBERTa_encdec
wandb:  View run at https://wandb.ai/redredouane/RoBERTa_encdec/runs/2cueo7h7
[2022-07-20 09:39:59,502][fairseq.trainer][INFO] - begin training epoch 1
[2022-07-20 09:39:59,503][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 09:40:10,756][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
[2022-07-20 09:40:22,852][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 09:40:34,654][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2022-07-20 09:40:47,137][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2022-07-20 09:40:59,573][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-20 09:41:12,374][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-20 09:52:18,027][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 09:53:23,960][valid][INFO] - {"epoch": 1, "valid_loss": "11.955", "valid_nll_loss": "11.262", "valid_ppl": "2456.01", "valid_wps": "2321.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "80"}
[2022-07-20 09:53:23,965][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 80 updates
[2022-07-20 09:53:23,968][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:53:29,936][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 09:53:35,561][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 80 updates, score 11.955) (writing took 11.596113320905715 seconds)
[2022-07-20 09:53:35,562][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2022-07-20 09:53:35,568][train][INFO] - {"epoch": 1, "train_loss": "16.564", "train_nll_loss": "15.867", "train_ppl": "59767.3", "train_wps": "1342.2", "train_ups": "0.11", "train_wpb": "12418.6", "train_bsz": "254.1", "train_num_updates": "80", "train_lr": "1.6e-06", "train_gnorm": "45.512", "train_clip": "100", "train_loss_scale": "2", "train_train_wall": "717", "train_gb_free": "8", "train_wall": "826"}
[2022-07-20 09:53:35,585][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 09:53:35,618][fairseq.trainer][INFO] - begin training epoch 2
[2022-07-20 09:53:35,620][fairseq_cli.train][INFO] - Start iterating over samples
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-20 10:05:27,990][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 10:06:34,608][valid][INFO] - {"epoch": 2, "valid_loss": "10.863", "valid_nll_loss": "10.116", "valid_ppl": "1109.59", "valid_wps": "2340.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "166", "valid_best_loss": "10.863"}
[2022-07-20 10:06:34,613][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 166 updates
[2022-07-20 10:06:34,616][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:06:41,187][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:06:45,240][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 166 updates, score 10.863) (writing took 10.627889851108193 seconds)
[2022-07-20 10:06:45,241][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2022-07-20 10:06:45,245][train][INFO] - {"epoch": 2, "train_loss": "11.368", "train_nll_loss": "10.67", "train_ppl": "1629.59", "train_wps": "1359.6", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "166", "train_lr": "3.32e-06", "train_gnorm": "2.126", "train_clip": "100", "train_loss_scale": "2", "train_train_wall": "692", "train_gb_free": "7.7", "train_wall": "1615"}
[2022-07-20 10:06:45,259][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 10:06:45,294][fairseq.trainer][INFO] - begin training epoch 3
[2022-07-20 10:06:45,295][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 10:11:30,844][train_inner][INFO] - {"epoch": 3, "update": 2.395, "loss": "13.324", "nll_loss": "12.612", "ppl": "6260.84", "wps": "1377.8", "ups": "0.11", "wpb": "12505.1", "bsz": "254.4", "num_updates": "200", "lr": "4e-06", "gnorm": "19.459", "clip": "100", "loss_scale": "2", "train_wall": "1686", "gb_free": "7.6", "wall": "1901"}
wandb: Network error (ReadTimeout), entering retry loop.
[2022-07-20 10:18:41,199][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 10:19:46,466][valid][INFO] - {"epoch": 3, "valid_loss": "9.911", "valid_nll_loss": "8.944", "valid_ppl": "492.39", "valid_wps": "2346.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "252", "valid_best_loss": "9.911"}
[2022-07-20 10:19:46,471][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 252 updates
[2022-07-20 10:19:46,474][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:19:54,906][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:20:00,035][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 252 updates, score 9.911) (writing took 13.563522787066177 seconds)
[2022-07-20 10:20:00,036][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2022-07-20 10:20:00,043][train][INFO] - {"epoch": 3, "train_loss": "10.467", "train_nll_loss": "9.63", "train_ppl": "792.45", "train_wps": "1350.8", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "252", "train_lr": "5.04e-06", "train_gnorm": "1.979", "train_clip": "100", "train_loss_scale": "2", "train_train_wall": "695", "train_gb_free": "7.7", "train_wall": "2410"}
[2022-07-20 10:20:00,056][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 10:20:00,164][fairseq.trainer][INFO] - begin training epoch 4
[2022-07-20 10:20:00,165][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 10:31:57,890][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 10:33:04,822][valid][INFO] - {"epoch": 4, "valid_loss": "9.374", "valid_nll_loss": "8.268", "valid_ppl": "308.26", "valid_wps": "2347.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "338", "valid_best_loss": "9.374"}
[2022-07-20 10:33:04,825][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 338 updates
[2022-07-20 10:33:04,827][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:33:11,165][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:33:16,124][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 338 updates, score 9.374) (writing took 11.298931493191049 seconds)
[2022-07-20 10:33:16,125][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2022-07-20 10:33:16,131][train][INFO] - {"epoch": 4, "train_loss": "9.771", "train_nll_loss": "8.794", "train_ppl": "443.82", "train_wps": "1348.6", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "338", "train_lr": "6.76e-06", "train_gnorm": "2.303", "train_clip": "100", "train_loss_scale": "4", "train_train_wall": "697", "train_gb_free": "8.1", "train_wall": "3206"}
[2022-07-20 10:33:16,146][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 10:33:16,185][fairseq.trainer][INFO] - begin training epoch 5
[2022-07-20 10:33:16,187][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 10:41:57,149][train_inner][INFO] - {"epoch": 5, "update": 4.721, "loss": "9.785", "nll_loss": "8.812", "ppl": "449.52", "wps": "1364", "ups": "0.11", "wpb": "12455.6", "bsz": "254.4", "num_updates": "400", "lr": "8e-06", "gnorm": "2.313", "clip": "100", "loss_scale": "4", "train_wall": "1620", "gb_free": "7.5", "wall": "3727"}
[2022-07-20 10:45:14,110][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 10:46:19,418][valid][INFO] - {"epoch": 5, "valid_loss": "9.053", "valid_nll_loss": "7.895", "valid_ppl": "237.98", "valid_wps": "2344.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "424", "valid_best_loss": "9.053"}
[2022-07-20 10:46:19,423][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 424 updates
[2022-07-20 10:46:19,426][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:46:27,035][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:46:31,134][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 424 updates, score 9.053) (writing took 11.710659313946962 seconds)
[2022-07-20 10:46:31,135][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2022-07-20 10:46:31,141][train][INFO] - {"epoch": 5, "train_loss": "9.353", "train_nll_loss": "8.298", "train_ppl": "314.68", "train_wps": "1350.5", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "424", "train_lr": "8.48e-06", "train_gnorm": "2.737", "train_clip": "100", "train_loss_scale": "4", "train_train_wall": "697", "train_gb_free": "6.9", "train_wall": "4001"}
[2022-07-20 10:46:31,155][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 10:46:31,197][fairseq.trainer][INFO] - begin training epoch 6
[2022-07-20 10:46:31,198][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 10:58:27,698][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 10:59:32,977][valid][INFO] - {"epoch": 6, "valid_loss": "8.831", "valid_nll_loss": "7.6", "valid_ppl": "194", "valid_wps": "2345.6", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "510", "valid_best_loss": "8.831"}
[2022-07-20 10:59:32,981][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 510 updates
[2022-07-20 10:59:32,985][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:59:39,441][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 10:59:44,310][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 510 updates, score 8.831) (writing took 11.328147228807211 seconds)
[2022-07-20 10:59:44,311][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2022-07-20 10:59:44,317][train][INFO] - {"epoch": 6, "train_loss": "9.062", "train_nll_loss": "7.956", "train_ppl": "248.28", "train_wps": "1353.6", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "510", "train_lr": "9.99487e-06", "train_gnorm": "2.63", "train_clip": "100", "train_loss_scale": "4", "train_train_wall": "695", "train_gb_free": "7.6", "train_wall": "4794"}
[2022-07-20 10:59:44,331][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 10:59:44,373][fairseq.trainer][INFO] - begin training epoch 7
[2022-07-20 10:59:44,375][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 11:11:41,682][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 11:12:47,510][valid][INFO] - {"epoch": 7, "valid_loss": "8.621", "valid_nll_loss": "7.38", "valid_ppl": "166.52", "valid_wps": "2344.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "596", "valid_best_loss": "8.621"}
[2022-07-20 11:12:47,514][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 596 updates
[2022-07-20 11:12:47,517][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:12:53,905][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:12:58,243][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 596 updates, score 8.621) (writing took 10.729095997055992 seconds)
[2022-07-20 11:12:58,244][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2022-07-20 11:12:58,252][train][INFO] - {"epoch": 7, "train_loss": "8.811", "train_nll_loss": "7.662", "train_ppl": "202.59", "train_wps": "1352.3", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "596", "train_lr": "9.95077e-06", "train_gnorm": "2.505", "train_clip": "100", "train_loss_scale": "8", "train_train_wall": "696", "train_gb_free": "7.9", "train_wall": "5588"}
[2022-07-20 11:12:58,264][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 11:12:58,298][fairseq.trainer][INFO] - begin training epoch 8
[2022-07-20 11:12:58,299][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 11:13:32,167][train_inner][INFO] - {"epoch": 8, "update": 7.047, "loss": "8.963", "nll_loss": "7.841", "ppl": "229.25", "wps": "1316.8", "ups": "0.11", "wpb": "12476.9", "bsz": "253.7", "num_updates": "600", "lr": "9.94872e-06", "gnorm": "2.636", "clip": "100", "loss_scale": "8", "train_wall": "1616", "gb_free": "7.2", "wall": "5622"}
[2022-07-20 11:24:56,107][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 11:26:01,302][valid][INFO] - {"epoch": 8, "valid_loss": "8.453", "valid_nll_loss": "7.186", "valid_ppl": "145.66", "valid_wps": "2347.4", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "682", "valid_best_loss": "8.453"}
[2022-07-20 11:26:01,307][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 682 updates
[2022-07-20 11:26:01,310][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:26:07,724][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:26:11,728][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 682 updates, score 8.453) (writing took 10.420796909136698 seconds)
[2022-07-20 11:26:11,728][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2022-07-20 11:26:11,734][train][INFO] - {"epoch": 8, "train_loss": "8.63", "train_nll_loss": "7.452", "train_ppl": "175.06", "train_wps": "1353.1", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "682", "train_lr": "9.90667e-06", "train_gnorm": "2.404", "train_clip": "100", "train_loss_scale": "8", "train_train_wall": "697", "train_gb_free": "7.9", "train_wall": "6382"}
[2022-07-20 11:26:11,750][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 11:26:11,791][fairseq.trainer][INFO] - begin training epoch 9
[2022-07-20 11:26:11,793][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 11:38:08,610][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 11:39:13,687][valid][INFO] - {"epoch": 9, "valid_loss": "8.326", "valid_nll_loss": "7.043", "valid_ppl": "131.84", "valid_wps": "2351.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "768", "valid_best_loss": "8.326"}
[2022-07-20 11:39:13,691][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 768 updates
[2022-07-20 11:39:13,692][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:39:20,106][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:39:24,173][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 9 @ 768 updates, score 8.326) (writing took 10.481951687019318 seconds)
[2022-07-20 11:39:24,174][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2022-07-20 11:39:24,180][train][INFO] - {"epoch": 9, "train_loss": "8.478", "train_nll_loss": "7.275", "train_ppl": "154.92", "train_wps": "1354.8", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "768", "train_lr": "9.86256e-06", "train_gnorm": "2.392", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "696", "train_gb_free": "8", "train_wall": "7174"}
[2022-07-20 11:39:24,194][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 11:39:24,231][fairseq.trainer][INFO] - begin training epoch 10
[2022-07-20 11:39:24,232][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 11:43:52,197][train_inner][INFO] - {"epoch": 10, "update": 9.372, "loss": "8.529", "nll_loss": "7.335", "ppl": "161.43", "wps": "1375.7", "ups": "0.11", "wpb": "12519.2", "bsz": "254.4", "num_updates": "800", "lr": "9.84615e-06", "gnorm": "2.344", "clip": "100", "loss_scale": "16", "train_wall": "1619", "gb_free": "7", "wall": "7442"}
[2022-07-20 11:51:21,652][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 11:52:26,901][valid][INFO] - {"epoch": 10, "valid_loss": "8.204", "valid_nll_loss": "6.907", "valid_ppl": "120.02", "valid_wps": "2345", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "854", "valid_best_loss": "8.204"}
[2022-07-20 11:52:26,904][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 854 updates
[2022-07-20 11:52:26,906][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:52:33,734][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 11:52:37,850][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 854 updates, score 8.204) (writing took 10.94591578305699 seconds)
[2022-07-20 11:52:37,854][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2022-07-20 11:52:37,863][train][INFO] - {"epoch": 10, "train_loss": "8.348", "train_nll_loss": "7.124", "train_ppl": "139.49", "train_wps": "1352.7", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "854", "train_lr": "9.81846e-06", "train_gnorm": "2.209", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "696", "train_gb_free": "7.9", "train_wall": "7968"}
[2022-07-20 11:52:37,879][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 11:52:37,923][fairseq.trainer][INFO] - begin training epoch 11
[2022-07-20 11:52:37,925][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 12:04:35,227][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 12:05:40,489][valid][INFO] - {"epoch": 11, "valid_loss": "8.109", "valid_nll_loss": "6.799", "valid_ppl": "111.33", "valid_wps": "2344.6", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "940", "valid_best_loss": "8.109"}
[2022-07-20 12:05:40,494][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 940 updates
[2022-07-20 12:05:40,497][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:05:47,196][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:05:51,274][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 11 @ 940 updates, score 8.109) (writing took 10.780129028949887 seconds)
[2022-07-20 12:05:51,275][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2022-07-20 12:05:51,281][train][INFO] - {"epoch": 11, "train_loss": "8.234", "train_nll_loss": "6.99", "train_ppl": "127.11", "train_wps": "1353.2", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "940", "train_lr": "9.77436e-06", "train_gnorm": "2.397", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "696", "train_gb_free": "8.1", "train_wall": "8761"}
[2022-07-20 12:05:51,296][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 12:05:51,335][fairseq.trainer][INFO] - begin training epoch 12
[2022-07-20 12:05:51,336][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 12:14:19,265][train_inner][INFO] - {"epoch": 12, "update": 11.698, "loss": "8.229", "nll_loss": "6.984", "ppl": "126.62", "wps": "1363.6", "ups": "0.11", "wpb": "12456.9", "bsz": "254.4", "num_updates": "1000", "lr": "9.74359e-06", "gnorm": "2.288", "clip": "100", "loss_scale": "16", "train_wall": "1625", "gb_free": "7.6", "wall": "9269"}
[2022-07-20 12:17:55,212][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 12:19:01,055][valid][INFO] - {"epoch": 12, "valid_loss": "8.026", "valid_nll_loss": "6.69", "valid_ppl": "103.26", "valid_wps": "2324.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1026", "valid_best_loss": "8.026"}
[2022-07-20 12:19:01,058][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 1026 updates
[2022-07-20 12:19:01,060][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:19:08,117][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:19:12,229][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 1026 updates, score 8.026) (writing took 11.170659902971238 seconds)
[2022-07-20 12:19:12,230][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2022-07-20 12:19:12,236][train][INFO] - {"epoch": 12, "train_loss": "8.136", "train_nll_loss": "6.876", "train_ppl": "117.49", "train_wps": "1340.5", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1026", "train_lr": "9.73026e-06", "train_gnorm": "2.237", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "702", "train_gb_free": "7.2", "train_wall": "9562"}
[2022-07-20 12:19:12,253][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 12:19:13,586][fairseq.trainer][INFO] - begin training epoch 13
[2022-07-20 12:19:13,587][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 12:31:18,699][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 12:32:25,373][valid][INFO] - {"epoch": 13, "valid_loss": "7.945", "valid_nll_loss": "6.602", "valid_ppl": "97.14", "valid_wps": "2295.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1112", "valid_best_loss": "7.945"}
[2022-07-20 12:32:25,377][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 1112 updates
[2022-07-20 12:32:25,379][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:32:31,869][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:32:36,049][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 13 @ 1112 updates, score 7.945) (writing took 10.671744867926463 seconds)
[2022-07-20 12:32:36,050][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2022-07-20 12:32:36,056][train][INFO] - {"epoch": 13, "train_loss": "8.045", "train_nll_loss": "6.771", "train_ppl": "109.19", "train_wps": "1335.7", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1112", "train_lr": "9.68615e-06", "train_gnorm": "2.182", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "702", "train_gb_free": "7.7", "train_wall": "10366"}
[2022-07-20 12:32:36,071][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 12:32:36,118][fairseq.trainer][INFO] - begin training epoch 14
[2022-07-20 12:32:36,119][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 12:44:42,602][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 12:45:49,142][valid][INFO] - {"epoch": 14, "valid_loss": "7.875", "valid_nll_loss": "6.527", "valid_ppl": "92.23", "valid_wps": "2300.4", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1198", "valid_best_loss": "7.875"}
[2022-07-20 12:45:49,145][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 1198 updates
[2022-07-20 12:45:49,147][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:45:55,558][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:45:59,658][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 1198 updates, score 7.875) (writing took 10.511705052107573 seconds)
[2022-07-20 12:45:59,660][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2022-07-20 12:45:59,665][train][INFO] - {"epoch": 14, "train_loss": "7.965", "train_nll_loss": "6.676", "train_ppl": "102.28", "train_wps": "1336", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1198", "train_lr": "9.64205e-06", "train_gnorm": "2.234", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "702", "train_gb_free": "8", "train_wall": "11170"}
[2022-07-20 12:45:59,681][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 12:45:59,732][fairseq.trainer][INFO] - begin training epoch 15
[2022-07-20 12:45:59,733][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 12:46:16,825][train_inner][INFO] - {"epoch": 15, "update": 14.023, "loss": "8.017", "nll_loss": "6.738", "ppl": "106.75", "wps": "1300", "ups": "0.1", "wpb": "12464.1", "bsz": "253.7", "num_updates": "1200", "lr": "9.64103e-06", "gnorm": "2.23", "clip": "100", "loss_scale": "32", "train_wall": "1630", "gb_free": "6.8", "wall": "11187"}
[2022-07-20 12:58:07,168][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 12:59:13,801][valid][INFO] - {"epoch": 15, "valid_loss": "7.811", "valid_nll_loss": "6.446", "valid_ppl": "87.19", "valid_wps": "2297", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1284", "valid_best_loss": "7.811"}
[2022-07-20 12:59:13,805][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 1284 updates
[2022-07-20 12:59:13,806][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:59:21,016][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 12:59:25,190][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 15 @ 1284 updates, score 7.811) (writing took 11.385431824950501 seconds)
[2022-07-20 12:59:25,191][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2022-07-20 12:59:25,194][train][INFO] - {"epoch": 15, "train_loss": "7.893", "train_nll_loss": "6.593", "train_ppl": "96.56", "train_wps": "1332.8", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1284", "train_lr": "9.59795e-06", "train_gnorm": "2.324", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "703", "train_gb_free": "7.4", "train_wall": "11975"}
[2022-07-20 12:59:25,207][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 12:59:25,255][fairseq.trainer][INFO] - begin training epoch 16
[2022-07-20 12:59:25,257][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 13:00:16,748][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 13:11:35,020][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 13:12:41,794][valid][INFO] - {"epoch": 16, "valid_loss": "7.76", "valid_nll_loss": "6.399", "valid_ppl": "84.37", "valid_wps": "2292.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1369", "valid_best_loss": "7.76"}
[2022-07-20 13:12:41,798][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 1369 updates
[2022-07-20 13:12:41,799][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 13:12:48,378][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 13:12:53,597][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 16 @ 1369 updates, score 7.76) (writing took 11.799408352002501 seconds)
[2022-07-20 13:12:53,598][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2022-07-20 13:12:53,604][train][INFO] - {"epoch": 16, "train_loss": "7.825", "train_nll_loss": "6.514", "train_ppl": "91.41", "train_wps": "1311.9", "train_ups": "0.11", "train_wpb": "12477.4", "train_bsz": "254.2", "train_num_updates": "1369", "train_lr": "9.55436e-06", "train_gnorm": "2.181", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "705", "train_gb_free": "8.1", "train_wall": "12784"}
[2022-07-20 13:12:53,618][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 13:12:53,665][fairseq.trainer][INFO] - begin training epoch 17
[2022-07-20 13:12:53,666][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 13:17:18,782][train_inner][INFO] - {"epoch": 17, "update": 16.36, "loss": "7.843", "nll_loss": "6.535", "ppl": "92.74", "wps": "1343", "ups": "0.11", "wpb": "12503.1", "bsz": "254.4", "num_updates": "1400", "lr": "9.53846e-06", "gnorm": "2.253", "clip": "100", "loss_scale": "32", "train_wall": "1648", "gb_free": "7.5", "wall": "13049"}
[2022-07-20 13:25:03,896][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 13:26:10,542][valid][INFO] - {"epoch": 17, "valid_loss": "7.712", "valid_nll_loss": "6.323", "valid_ppl": "80.09", "valid_wps": "2296.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1455", "valid_best_loss": "7.712"}
[2022-07-20 13:26:10,545][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 1455 updates
[2022-07-20 13:26:10,547][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 13:26:17,077][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 13:26:21,215][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 17 @ 1455 updates, score 7.712) (writing took 10.669361364096403 seconds)
[2022-07-20 13:26:21,216][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2022-07-20 13:26:21,222][train][INFO] - {"epoch": 17, "train_loss": "7.763", "train_nll_loss": "6.443", "train_ppl": "86.98", "train_wps": "1329.4", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1455", "train_lr": "9.51026e-06", "train_gnorm": "2.236", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "706", "train_gb_free": "7", "train_wall": "13591"}
[2022-07-20 13:26:21,238][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 13:26:21,283][fairseq.trainer][INFO] - begin training epoch 18
[2022-07-20 13:26:21,285][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 13:38:28,794][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 13:39:34,005][valid][INFO] - {"epoch": 18, "valid_loss": "7.664", "valid_nll_loss": "6.274", "valid_ppl": "77.39", "valid_wps": "2346.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1541", "valid_best_loss": "7.664"}
[2022-07-20 13:39:34,010][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 1541 updates
[2022-07-20 13:39:34,013][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 13:39:40,401][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 13:39:44,425][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 18 @ 1541 updates, score 7.664) (writing took 10.415387132903561 seconds)
[2022-07-20 13:39:44,426][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2022-07-20 13:39:44,429][train][INFO] - {"epoch": 18, "train_loss": "7.705", "train_nll_loss": "6.375", "train_ppl": "83.01", "train_wps": "1336.7", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1541", "train_lr": "9.46615e-06", "train_gnorm": "2.231", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "704", "train_gb_free": "7.7", "train_wall": "14395"}
[2022-07-20 13:39:44,442][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 13:39:44,485][fairseq.trainer][INFO] - begin training epoch 19
[2022-07-20 13:39:44,486][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 13:42:24,256][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 13:48:10,402][train_inner][INFO] - {"epoch": 19, "update": 18.698, "loss": "7.709", "nll_loss": "6.379", "ppl": "83.23", "wps": "1350.9", "ups": "0.11", "wpb": "12506.4", "bsz": "254.4", "num_updates": "1600", "lr": "9.4359e-06", "gnorm": "2.216", "clip": "100", "loss_scale": "32", "train_wall": "1645", "gb_free": "7.3", "wall": "14901"}
[2022-07-20 13:51:44,259][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 13:52:49,441][valid][INFO] - {"epoch": 19, "valid_loss": "7.615", "valid_nll_loss": "6.215", "valid_ppl": "74.27", "valid_wps": "2347.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1626", "valid_best_loss": "7.615"}
[2022-07-20 13:52:49,444][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 1626 updates
[2022-07-20 13:52:49,446][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 13:52:55,762][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 13:53:01,784][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 19 @ 1626 updates, score 7.615) (writing took 12.340187366120517 seconds)
[2022-07-20 13:53:01,786][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2022-07-20 13:53:01,791][train][INFO] - {"epoch": 19, "train_loss": "7.65", "train_nll_loss": "6.311", "train_ppl": "79.39", "train_wps": "1328.8", "train_ups": "0.11", "train_wpb": "12464.6", "train_bsz": "254.2", "train_num_updates": "1626", "train_lr": "9.42256e-06", "train_gnorm": "2.188", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "699", "train_gb_free": "7.6", "train_wall": "15192"}
[2022-07-20 13:53:01,808][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 13:53:03,211][fairseq.trainer][INFO] - begin training epoch 20
[2022-07-20 13:53:03,213][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 14:05:02,861][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 14:06:08,038][valid][INFO] - {"epoch": 20, "valid_loss": "7.572", "valid_nll_loss": "6.176", "valid_ppl": "72.32", "valid_wps": "2347.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1712", "valid_best_loss": "7.572"}
[2022-07-20 14:06:08,041][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 1712 updates
[2022-07-20 14:06:08,043][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 14:06:14,312][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 14:06:20,787][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 20 @ 1712 updates, score 7.572) (writing took 12.74636576906778 seconds)
[2022-07-20 14:06:20,788][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2022-07-20 14:06:20,794][train][INFO] - {"epoch": 20, "train_loss": "7.603", "train_nll_loss": "6.256", "train_ppl": "76.43", "train_wps": "1343.7", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1712", "train_lr": "9.37846e-06", "train_gnorm": "2.206", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "698", "train_gb_free": "7.8", "train_wall": "15991"}
[2022-07-20 14:06:20,809][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 14:06:20,859][fairseq.trainer][INFO] - begin training epoch 21
[2022-07-20 14:06:20,861][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 14:18:20,618][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 14:19:26,117][valid][INFO] - {"epoch": 21, "valid_loss": "7.536", "valid_nll_loss": "6.122", "valid_ppl": "69.64", "valid_wps": "2336.4", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1798", "valid_best_loss": "7.536"}
[2022-07-20 14:19:26,122][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 1798 updates
[2022-07-20 14:19:26,125][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 14:19:32,455][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 14:19:36,482][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 21 @ 1798 updates, score 7.536) (writing took 10.36032264586538 seconds)
[2022-07-20 14:19:36,483][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2022-07-20 14:19:36,486][train][INFO] - {"epoch": 21, "train_loss": "7.554", "train_nll_loss": "6.199", "train_ppl": "73.48", "train_wps": "1349.3", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1798", "train_lr": "9.33436e-06", "train_gnorm": "2.125", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "699", "train_gb_free": "7.7", "train_wall": "16787"}
[2022-07-20 14:19:36,499][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 14:19:36,552][fairseq.trainer][INFO] - begin training epoch 22
[2022-07-20 14:19:36,553][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 14:19:53,912][train_inner][INFO] - {"epoch": 22, "update": 21.023, "loss": "7.584", "nll_loss": "6.234", "ppl": "75.29", "wps": "1306.3", "ups": "0.11", "wpb": "12432.5", "bsz": "253.7", "num_updates": "1800", "lr": "9.33333e-06", "gnorm": "2.17", "clip": "100", "loss_scale": "32", "train_wall": "1621", "gb_free": "7.5", "wall": "16804"}
[2022-07-20 14:27:44,120][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 14:31:36,243][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 14:32:41,730][valid][INFO] - {"epoch": 22, "valid_loss": "7.502", "valid_nll_loss": "6.088", "valid_ppl": "68.03", "valid_wps": "2336.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1883", "valid_best_loss": "7.502"}
[2022-07-20 14:32:41,734][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 1883 updates
[2022-07-20 14:32:41,736][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 14:32:48,055][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 14:32:52,079][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 22 @ 1883 updates, score 7.502) (writing took 10.344626575009897 seconds)
[2022-07-20 14:32:52,080][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2022-07-20 14:32:52,085][train][INFO] - {"epoch": 22, "train_loss": "7.51", "train_nll_loss": "6.148", "train_ppl": "70.92", "train_wps": "1332.3", "train_ups": "0.11", "train_wpb": "12470.7", "train_bsz": "254.2", "train_num_updates": "1883", "train_lr": "9.29077e-06", "train_gnorm": "2.19", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "698", "train_gb_free": "7.6", "train_wall": "17582"}
[2022-07-20 14:32:52,099][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 14:32:52,152][fairseq.trainer][INFO] - begin training epoch 23
[2022-07-20 14:32:52,153][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 14:44:52,131][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 14:45:57,365][valid][INFO] - {"epoch": 23, "valid_loss": "7.467", "valid_nll_loss": "6.05", "valid_ppl": "66.28", "valid_wps": "2346.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1969", "valid_best_loss": "7.467"}
[2022-07-20 14:45:57,370][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 1969 updates
[2022-07-20 14:45:57,373][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 14:46:05,153][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 14:46:09,421][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 23 @ 1969 updates, score 7.467) (writing took 12.050759912934154 seconds)
[2022-07-20 14:46:09,421][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2022-07-20 14:46:09,425][train][INFO] - {"epoch": 23, "train_loss": "7.469", "train_nll_loss": "6.101", "train_ppl": "68.62", "train_wps": "1346.5", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1969", "train_lr": "9.24667e-06", "train_gnorm": "2.182", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "699", "train_gb_free": "7.2", "train_wall": "18380"}
[2022-07-20 14:46:09,438][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 14:46:09,485][fairseq.trainer][INFO] - begin training epoch 24
[2022-07-20 14:46:09,486][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 14:50:31,255][train_inner][INFO] - {"epoch": 24, "update": 23.36, "loss": "7.48", "nll_loss": "6.113", "ppl": "69.2", "wps": "1362.1", "ups": "0.11", "wpb": "12513.5", "bsz": "254.4", "num_updates": "2000", "lr": "9.23077e-06", "gnorm": "2.195", "clip": "100", "loss_scale": "32", "train_wall": "1634", "gb_free": "6.9", "wall": "18641"}
[2022-07-20 14:58:11,623][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 14:59:16,908][valid][INFO] - {"epoch": 24, "valid_loss": "7.44", "valid_nll_loss": "6.026", "valid_ppl": "65.18", "valid_wps": "2344.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2055", "valid_best_loss": "7.44"}
[2022-07-20 14:59:16,912][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 2055 updates
[2022-07-20 14:59:16,913][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 14:59:23,223][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 14:59:27,230][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 24 @ 2055 updates, score 7.44) (writing took 10.318738972069696 seconds)
[2022-07-20 14:59:27,231][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2022-07-20 14:59:27,238][train][INFO] - {"epoch": 24, "train_loss": "7.427", "train_nll_loss": "6.052", "train_ppl": "66.33", "train_wps": "1345.7", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2055", "train_lr": "9.20256e-06", "train_gnorm": "2.206", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "701", "train_gb_free": "8.1", "train_wall": "19177"}
[2022-07-20 14:59:27,252][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 14:59:27,303][fairseq.trainer][INFO] - begin training epoch 25
[2022-07-20 14:59:27,304][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 15:11:26,840][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 15:12:32,275][valid][INFO] - {"epoch": 25, "valid_loss": "7.412", "valid_nll_loss": "5.986", "valid_ppl": "63.39", "valid_wps": "2338.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2141", "valid_best_loss": "7.412"}
[2022-07-20 15:12:32,279][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 2141 updates
[2022-07-20 15:12:32,280][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 15:12:38,584][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 15:12:42,599][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 25 @ 2141 updates, score 7.412) (writing took 10.320585418958217 seconds)
[2022-07-20 15:12:42,600][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2022-07-20 15:12:42,603][train][INFO] - {"epoch": 25, "train_loss": "7.392", "train_nll_loss": "6.011", "train_ppl": "64.48", "train_wps": "1349.9", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2141", "train_lr": "9.15846e-06", "train_gnorm": "2.198", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "698", "train_gb_free": "7.7", "train_wall": "19973"}
[2022-07-20 15:12:42,616][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 15:12:42,664][fairseq.trainer][INFO] - begin training epoch 26
[2022-07-20 15:12:42,666][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 15:21:01,561][train_inner][INFO] - {"epoch": 26, "update": 25.686, "loss": "7.385", "nll_loss": "6.003", "ppl": "64.11", "wps": "1365.4", "ups": "0.11", "wpb": "12495.6", "bsz": "254.4", "num_updates": "2200", "lr": "9.12821e-06", "gnorm": "2.171", "clip": "100", "loss_scale": "64", "train_wall": "1629", "gb_free": "7.8", "wall": "20472"}
[2022-07-20 15:24:42,293][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 15:25:47,634][valid][INFO] - {"epoch": 26, "valid_loss": "7.383", "valid_nll_loss": "5.958", "valid_ppl": "62.14", "valid_wps": "2342.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2227", "valid_best_loss": "7.383"}
[2022-07-20 15:25:47,637][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 2227 updates
[2022-07-20 15:25:47,638][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 15:25:53,970][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 15:25:58,044][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 26 @ 2227 updates, score 7.383) (writing took 10.40736643387936 seconds)
[2022-07-20 15:25:58,045][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2022-07-20 15:25:58,052][train][INFO] - {"epoch": 26, "train_loss": "7.357", "train_nll_loss": "5.971", "train_ppl": "62.71", "train_wps": "1349.7", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2227", "train_lr": "9.11436e-06", "train_gnorm": "2.129", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "698", "train_gb_free": "7.9", "train_wall": "20768"}
[2022-07-20 15:25:58,066][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 15:25:58,126][fairseq.trainer][INFO] - begin training epoch 27
[2022-07-20 15:25:58,128][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 15:32:17,090][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 15:37:57,995][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 15:39:03,448][valid][INFO] - {"epoch": 27, "valid_loss": "7.368", "valid_nll_loss": "5.925", "valid_ppl": "60.78", "valid_wps": "2337.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2312", "valid_best_loss": "7.368"}
[2022-07-20 15:39:03,451][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 2312 updates
[2022-07-20 15:39:03,455][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 15:39:09,829][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 15:39:13,874][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 27 @ 2312 updates, score 7.368) (writing took 10.422741213114932 seconds)
[2022-07-20 15:39:13,875][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2022-07-20 15:39:13,881][train][INFO] - {"epoch": 27, "train_loss": "7.323", "train_nll_loss": "5.931", "train_ppl": "61", "train_wps": "1330.7", "train_ups": "0.11", "train_wpb": "12458.8", "train_bsz": "254.2", "train_num_updates": "2312", "train_lr": "9.07077e-06", "train_gnorm": "2.137", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "699", "train_gb_free": "7.9", "train_wall": "21564"}
[2022-07-20 15:39:13,900][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 15:39:13,954][fairseq.trainer][INFO] - begin training epoch 28
[2022-07-20 15:39:13,955][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 15:51:13,853][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 15:52:19,339][valid][INFO] - {"epoch": 28, "valid_loss": "7.334", "valid_nll_loss": "5.901", "valid_ppl": "59.74", "valid_wps": "2337.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2398", "valid_best_loss": "7.334"}
[2022-07-20 15:52:19,342][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 2398 updates
[2022-07-20 15:52:19,344][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 15:52:25,586][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 15:52:29,619][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 28 @ 2398 updates, score 7.334) (writing took 10.276671356987208 seconds)
[2022-07-20 15:52:29,620][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2022-07-20 15:52:29,627][train][INFO] - {"epoch": 28, "train_loss": "7.291", "train_nll_loss": "5.893", "train_ppl": "59.43", "train_wps": "1349.2", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2398", "train_lr": "9.02667e-06", "train_gnorm": "2.1", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "698", "train_gb_free": "7.9", "train_wall": "22360"}
[2022-07-20 15:52:29,641][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 15:52:29,696][fairseq.trainer][INFO] - begin training epoch 29
[2022-07-20 15:52:29,697][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 15:52:46,658][train_inner][INFO] - {"epoch": 29, "update": 28.023, "loss": "7.32", "nll_loss": "5.927", "ppl": "60.84", "wps": "1304.9", "ups": "0.1", "wpb": "12429.3", "bsz": "253.7", "num_updates": "2400", "lr": "9.02564e-06", "gnorm": "2.123", "clip": "100", "loss_scale": "32", "train_wall": "1628", "gb_free": "7.6", "wall": "22377"}
[2022-07-20 16:04:28,756][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 16:05:34,351][valid][INFO] - {"epoch": 29, "valid_loss": "7.316", "valid_nll_loss": "5.864", "valid_ppl": "58.23", "valid_wps": "2332.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2484", "valid_best_loss": "7.316"}
[2022-07-20 16:05:34,355][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 2484 updates
[2022-07-20 16:05:34,356][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 16:05:40,715][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 16:05:44,821][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 29 @ 2484 updates, score 7.316) (writing took 10.466513348044828 seconds)
[2022-07-20 16:05:44,822][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2022-07-20 16:05:44,828][train][INFO] - {"epoch": 29, "train_loss": "7.258", "train_nll_loss": "5.856", "train_ppl": "57.91", "train_wps": "1350.2", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2484", "train_lr": "8.98256e-06", "train_gnorm": "2.173", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "698", "train_gb_free": "7.8", "train_wall": "23155"}
[2022-07-20 16:05:44,842][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 16:05:44,914][fairseq.trainer][INFO] - begin training epoch 30
[2022-07-20 16:05:44,915][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 16:13:44,983][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 16:17:44,128][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 16:18:49,520][valid][INFO] - {"epoch": 30, "valid_loss": "7.292", "valid_nll_loss": "5.845", "valid_ppl": "57.48", "valid_wps": "2341.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2569", "valid_best_loss": "7.292"}
[2022-07-20 16:18:49,524][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 2569 updates
[2022-07-20 16:18:49,525][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 16:18:55,879][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 16:18:59,960][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 30 @ 2569 updates, score 7.292) (writing took 10.436361063970253 seconds)
[2022-07-20 16:18:59,961][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2022-07-20 16:18:59,969][train][INFO] - {"epoch": 30, "train_loss": "7.237", "train_nll_loss": "5.83", "train_ppl": "56.89", "train_wps": "1330.3", "train_ups": "0.11", "train_wpb": "12444", "train_bsz": "254.2", "train_num_updates": "2569", "train_lr": "8.93897e-06", "train_gnorm": "2.185", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "698", "train_gb_free": "7.7", "train_wall": "23950"}
[2022-07-20 16:18:59,983][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 16:19:00,034][fairseq.trainer][INFO] - begin training epoch 31
[2022-07-20 16:19:00,035][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 16:23:22,773][train_inner][INFO] - {"epoch": 31, "update": 30.36, "loss": "7.24", "nll_loss": "5.834", "ppl": "57.03", "wps": "1359.2", "ups": "0.11", "wpb": "12478.3", "bsz": "254.4", "num_updates": "2600", "lr": "8.92308e-06", "gnorm": "2.168", "clip": "100", "loss_scale": "32", "train_wall": "1634", "gb_free": "7", "wall": "24213"}
[2022-07-20 16:31:00,882][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 16:32:06,477][valid][INFO] - {"epoch": 31, "valid_loss": "7.27", "valid_nll_loss": "5.824", "valid_ppl": "56.65", "valid_wps": "2332.6", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2655", "valid_best_loss": "7.27"}
[2022-07-20 16:32:06,482][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 2655 updates
[2022-07-20 16:32:06,486][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 16:32:12,923][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 16:32:17,010][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 31 @ 2655 updates, score 7.27) (writing took 10.527969365939498 seconds)
[2022-07-20 16:32:17,011][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2022-07-20 16:32:17,018][train][INFO] - {"epoch": 31, "train_loss": "7.201", "train_nll_loss": "5.788", "train_ppl": "55.26", "train_wps": "1347", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2655", "train_lr": "8.89487e-06", "train_gnorm": "2.153", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "699", "train_gb_free": "7.9", "train_wall": "24747"}
[2022-07-20 16:32:17,031][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 16:32:17,092][fairseq.trainer][INFO] - begin training epoch 32
[2022-07-20 16:32:17,093][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 16:44:19,570][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 16:45:24,841][valid][INFO] - {"epoch": 32, "valid_loss": "7.251", "valid_nll_loss": "5.8", "valid_ppl": "55.7", "valid_wps": "2346.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2741", "valid_best_loss": "7.251"}
[2022-07-20 16:45:24,846][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 2741 updates
[2022-07-20 16:45:24,851][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 16:45:31,195][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 16:45:35,260][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 32 @ 2741 updates, score 7.251) (writing took 10.414351521991193 seconds)
[2022-07-20 16:45:35,261][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2022-07-20 16:45:35,266][train][INFO] - {"epoch": 32, "train_loss": "7.175", "train_nll_loss": "5.759", "train_ppl": "54.14", "train_wps": "1345", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2741", "train_lr": "8.85077e-06", "train_gnorm": "2.22", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "701", "train_gb_free": "6.6", "train_wall": "25545"}
[2022-07-20 16:45:35,282][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 16:45:35,341][fairseq.trainer][INFO] - begin training epoch 33
[2022-07-20 16:45:35,342][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 16:53:53,909][train_inner][INFO] - {"epoch": 33, "update": 32.686, "loss": "7.176", "nll_loss": "5.76", "ppl": "54.18", "wps": "1365.7", "ups": "0.11", "wpb": "12504.1", "bsz": "254.4", "num_updates": "2800", "lr": "8.82051e-06", "gnorm": "2.192", "clip": "100", "loss_scale": "64", "train_wall": "1630", "gb_free": "6.6", "wall": "26044"}
[2022-07-20 16:57:36,780][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 16:58:42,343][valid][INFO] - {"epoch": 33, "valid_loss": "7.231", "valid_nll_loss": "5.776", "valid_ppl": "54.79", "valid_wps": "2336.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2827", "valid_best_loss": "7.231"}
[2022-07-20 16:58:42,346][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 2827 updates
[2022-07-20 16:58:42,348][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 16:58:48,658][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 16:58:53,783][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 33 @ 2827 updates, score 7.231) (writing took 11.436861293157563 seconds)
[2022-07-20 16:58:53,784][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2022-07-20 16:58:53,791][train][INFO] - {"epoch": 33, "train_loss": "7.148", "train_nll_loss": "5.728", "train_ppl": "52.99", "train_wps": "1344.5", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2827", "train_lr": "8.80667e-06", "train_gnorm": "2.162", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "700", "train_gb_free": "8", "train_wall": "26344"}
[2022-07-20 16:58:53,804][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 16:58:53,856][fairseq.trainer][INFO] - begin training epoch 34
[2022-07-20 16:58:53,858][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 17:08:54,024][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 17:10:55,985][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 17:12:01,500][valid][INFO] - {"epoch": 34, "valid_loss": "7.212", "valid_nll_loss": "5.752", "valid_ppl": "53.9", "valid_wps": "2337.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2912", "valid_best_loss": "7.212"}
[2022-07-20 17:12:01,506][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 2912 updates
[2022-07-20 17:12:01,509][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 17:12:07,918][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 17:12:12,066][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 34 @ 2912 updates, score 7.212) (writing took 10.559893758967519 seconds)
[2022-07-20 17:12:12,067][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2022-07-20 17:12:12,074][train][INFO] - {"epoch": 34, "train_loss": "7.124", "train_nll_loss": "5.699", "train_ppl": "51.97", "train_wps": "1329.5", "train_ups": "0.11", "train_wpb": "12485.9", "train_bsz": "254.2", "train_num_updates": "2912", "train_lr": "8.76308e-06", "train_gnorm": "2.17", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "701", "train_gb_free": "7.8", "train_wall": "27142"}
[2022-07-20 17:12:12,094][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 17:12:12,153][fairseq.trainer][INFO] - begin training epoch 35
[2022-07-20 17:12:12,154][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 17:24:12,180][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 17:25:17,188][valid][INFO] - {"epoch": 35, "valid_loss": "7.199", "valid_nll_loss": "5.733", "valid_ppl": "53.2", "valid_wps": "2356.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2998", "valid_best_loss": "7.199"}
[2022-07-20 17:25:17,191][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 2998 updates
[2022-07-20 17:25:17,193][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 17:25:23,693][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 17:25:28,060][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 35 @ 2998 updates, score 7.199) (writing took 10.86886443104595 seconds)
[2022-07-20 17:25:28,061][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2022-07-20 17:25:28,068][train][INFO] - {"epoch": 35, "train_loss": "7.099", "train_nll_loss": "5.67", "train_ppl": "50.92", "train_wps": "1348.8", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2998", "train_lr": "8.71897e-06", "train_gnorm": "2.173", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "699", "train_gb_free": "7.9", "train_wall": "27938"}
[2022-07-20 17:25:28,081][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 17:25:28,135][fairseq.trainer][INFO] - begin training epoch 36
[2022-07-20 17:25:28,136][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 17:25:45,940][train_inner][INFO] - {"epoch": 36, "update": 35.023, "loss": "7.112", "nll_loss": "5.685", "ppl": "51.45", "wps": "1304.4", "ups": "0.1", "wpb": "12469.8", "bsz": "253.7", "num_updates": "3000", "lr": "8.71795e-06", "gnorm": "2.171", "clip": "100", "loss_scale": "32", "train_wall": "1633", "gb_free": "7.5", "wall": "27956"}
[2022-07-20 17:37:38,080][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 17:38:44,854][valid][INFO] - {"epoch": 36, "valid_loss": "7.178", "valid_nll_loss": "5.711", "valid_ppl": "52.4", "valid_wps": "2294.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3084", "valid_best_loss": "7.178"}
[2022-07-20 17:38:44,858][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 3084 updates
[2022-07-20 17:38:44,860][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 17:38:51,705][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 17:38:56,129][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 36 @ 3084 updates, score 7.178) (writing took 11.271305526141077 seconds)
[2022-07-20 17:38:56,130][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2022-07-20 17:38:56,137][train][INFO] - {"epoch": 36, "train_loss": "7.074", "train_nll_loss": "5.641", "train_ppl": "49.89", "train_wps": "1328.7", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3084", "train_lr": "8.67487e-06", "train_gnorm": "2.153", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "705", "train_gb_free": "8.1", "train_wall": "28746"}
[2022-07-20 17:38:56,152][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 17:38:56,209][fairseq.trainer][INFO] - begin training epoch 37
[2022-07-20 17:38:56,211][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 17:51:05,355][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 17:52:12,292][valid][INFO] - {"epoch": 37, "valid_loss": "7.167", "valid_nll_loss": "5.698", "valid_ppl": "51.93", "valid_wps": "2288.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3170", "valid_best_loss": "7.167"}
[2022-07-20 17:52:12,296][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 3170 updates
[2022-07-20 17:52:12,298][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 17:52:19,028][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 17:52:23,387][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 37 @ 3170 updates, score 7.167) (writing took 11.090978690888733 seconds)
[2022-07-20 17:52:23,388][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2022-07-20 17:52:23,395][train][INFO] - {"epoch": 37, "train_loss": "7.051", "train_nll_loss": "5.615", "train_ppl": "49", "train_wps": "1330", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3170", "train_lr": "8.63077e-06", "train_gnorm": "2.167", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "705", "train_gb_free": "7.6", "train_wall": "29554"}
[2022-07-20 17:52:23,411][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 17:52:23,485][fairseq.trainer][INFO] - begin training epoch 38
[2022-07-20 17:52:23,486][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 17:56:40,135][train_inner][INFO] - {"epoch": 38, "update": 37.349, "loss": "7.063", "nll_loss": "5.628", "ppl": "49.47", "wps": "1349", "ups": "0.11", "wpb": "12506.4", "bsz": "254.4", "num_updates": "3200", "lr": "8.61538e-06", "gnorm": "2.152", "clip": "100", "loss_scale": "64", "train_wall": "1641", "gb_free": "7.3", "wall": "29810"}
[2022-07-20 18:04:32,277][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 18:05:39,008][valid][INFO] - {"epoch": 38, "valid_loss": "7.147", "valid_nll_loss": "5.678", "valid_ppl": "51.21", "valid_wps": "2297", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3256", "valid_best_loss": "7.147"}
[2022-07-20 18:05:39,011][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 3256 updates
[2022-07-20 18:05:39,015][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 18:05:45,660][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 18:05:50,072][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 38 @ 3256 updates, score 7.147) (writing took 11.059766605030745 seconds)
[2022-07-20 18:05:50,075][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2022-07-20 18:05:50,083][train][INFO] - {"epoch": 38, "train_loss": "7.028", "train_nll_loss": "5.588", "train_ppl": "48.09", "train_wps": "1330.9", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3256", "train_lr": "8.58667e-06", "train_gnorm": "2.135", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "705", "train_gb_free": "7.4", "train_wall": "30360"}
[2022-07-20 18:05:50,099][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 18:05:50,180][fairseq.trainer][INFO] - begin training epoch 39
[2022-07-20 18:05:50,182][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 18:14:06,256][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 18:17:59,239][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 18:19:05,932][valid][INFO] - {"epoch": 39, "valid_loss": "7.138", "valid_nll_loss": "5.665", "valid_ppl": "50.75", "valid_wps": "2295.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3341", "valid_best_loss": "7.138"}
[2022-07-20 18:19:05,939][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 3341 updates
[2022-07-20 18:19:05,941][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 18:19:12,800][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 18:19:17,295][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 39 @ 3341 updates, score 7.138) (writing took 11.356060642981902 seconds)
[2022-07-20 18:19:17,296][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2022-07-20 18:19:17,302][train][INFO] - {"epoch": 39, "train_loss": "7.004", "train_nll_loss": "5.559", "train_ppl": "47.16", "train_wps": "1313.4", "train_ups": "0.11", "train_wpb": "12473.2", "train_bsz": "254.2", "train_num_updates": "3341", "train_lr": "8.54308e-06", "train_gnorm": "2.129", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "705", "train_gb_free": "7.7", "train_wall": "31167"}
[2022-07-20 18:19:17,315][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 18:19:17,379][fairseq.trainer][INFO] - begin training epoch 40
[2022-07-20 18:19:17,380][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 18:27:42,050][train_inner][INFO] - {"epoch": 40, "update": 39.686, "loss": "7", "nll_loss": "5.555", "ppl": "47.01", "wps": "1339.6", "ups": "0.11", "wpb": "12470.9", "bsz": "254.4", "num_updates": "3400", "lr": "8.51282e-06", "gnorm": "2.139", "clip": "100", "loss_scale": "32", "train_wall": "1649", "gb_free": "7.2", "wall": "31672"}
[2022-07-20 18:31:26,475][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 18:32:33,063][valid][INFO] - {"epoch": 40, "valid_loss": "7.122", "valid_nll_loss": "5.653", "valid_ppl": "50.32", "valid_wps": "2299.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3427", "valid_best_loss": "7.122"}
[2022-07-20 18:32:33,066][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 3427 updates
[2022-07-20 18:32:33,070][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 18:32:39,883][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 18:32:44,137][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 40 @ 3427 updates, score 7.122) (writing took 11.070313435979187 seconds)
[2022-07-20 18:32:44,138][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2022-07-20 18:32:44,143][train][INFO] - {"epoch": 40, "train_loss": "6.987", "train_nll_loss": "5.54", "train_ppl": "46.54", "train_wps": "1330.7", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3427", "train_lr": "8.49897e-06", "train_gnorm": "2.131", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "704", "train_gb_free": "8", "train_wall": "31974"}
[2022-07-20 18:32:44,158][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 18:32:44,227][fairseq.trainer][INFO] - begin training epoch 41
[2022-07-20 18:32:44,228][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 18:44:53,062][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 18:45:59,748][valid][INFO] - {"epoch": 41, "valid_loss": "7.111", "valid_nll_loss": "5.629", "valid_ppl": "49.48", "valid_wps": "2295.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3513", "valid_best_loss": "7.111"}
[2022-07-20 18:45:59,750][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 3513 updates
[2022-07-20 18:45:59,752][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 18:46:06,442][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 18:46:10,763][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 41 @ 3513 updates, score 7.111) (writing took 11.012236361857504 seconds)
[2022-07-20 18:46:10,763][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2022-07-20 18:46:10,767][train][INFO] - {"epoch": 41, "train_loss": "6.968", "train_nll_loss": "5.517", "train_ppl": "45.81", "train_wps": "1331", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3513", "train_lr": "8.45487e-06", "train_gnorm": "2.181", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "705", "train_gb_free": "7.9", "train_wall": "32781"}
[2022-07-20 18:46:10,780][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 18:46:10,846][fairseq.trainer][INFO] - begin training epoch 42
[2022-07-20 18:46:10,848][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 18:58:19,879][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 18:59:26,684][valid][INFO] - {"epoch": 42, "valid_loss": "7.098", "valid_nll_loss": "5.619", "valid_ppl": "49.13", "valid_wps": "2291.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3599", "valid_best_loss": "7.098"}
[2022-07-20 18:59:26,687][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 3599 updates
[2022-07-20 18:59:26,689][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 18:59:33,352][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 18:59:37,747][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 42 @ 3599 updates, score 7.098) (writing took 11.059578024083748 seconds)
[2022-07-20 18:59:37,748][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2022-07-20 18:59:37,754][train][INFO] - {"epoch": 42, "train_loss": "6.947", "train_nll_loss": "5.493", "train_ppl": "45.04", "train_wps": "1330.4", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3599", "train_lr": "8.41077e-06", "train_gnorm": "2.18", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "705", "train_gb_free": "7.9", "train_wall": "33588"}
[2022-07-20 18:59:37,770][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 18:59:37,831][fairseq.trainer][INFO] - begin training epoch 43
[2022-07-20 18:59:37,833][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 18:59:46,687][train_inner][INFO] - {"epoch": 43, "update": 42.012, "loss": "6.963", "nll_loss": "5.512", "ppl": "45.64", "wps": "1293.9", "ups": "0.1", "wpb": "12451.8", "bsz": "253.7", "num_updates": "3600", "lr": "8.41026e-06", "gnorm": "2.169", "clip": "100", "loss_scale": "64", "train_wall": "1635", "gb_free": "7.7", "wall": "33597"}
[2022-07-20 19:11:47,896][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 19:12:54,556][valid][INFO] - {"epoch": 43, "valid_loss": "7.075", "valid_nll_loss": "5.604", "valid_ppl": "48.65", "valid_wps": "2296.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3685", "valid_best_loss": "7.075"}
[2022-07-20 19:12:54,559][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 3685 updates
[2022-07-20 19:12:54,561][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 19:13:01,162][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 19:13:05,492][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 43 @ 3685 updates, score 7.075) (writing took 10.932806149823591 seconds)
[2022-07-20 19:13:05,493][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2022-07-20 19:13:05,496][train][INFO] - {"epoch": 43, "train_loss": "6.927", "train_nll_loss": "5.471", "train_ppl": "44.34", "train_wps": "1329.2", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3685", "train_lr": "8.36667e-06", "train_gnorm": "2.151", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "706", "train_gb_free": "7.5", "train_wall": "34396"}
[2022-07-20 19:13:05,514][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 19:13:05,578][fairseq.trainer][INFO] - begin training epoch 44
[2022-07-20 19:13:05,580][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 19:25:09,083][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 19:26:14,562][valid][INFO] - {"epoch": 44, "valid_loss": "7.078", "valid_nll_loss": "5.585", "valid_ppl": "47.99", "valid_wps": "2337.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3771", "valid_best_loss": "7.075"}
[2022-07-20 19:26:14,565][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 3771 updates
[2022-07-20 19:26:14,568][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_last.pt
[2022-07-20 19:26:21,120][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_last.pt
[2022-07-20 19:26:21,143][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 44 @ 3771 updates, score 7.078) (writing took 6.5779226219747216 seconds)
[2022-07-20 19:26:21,143][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2022-07-20 19:26:21,146][train][INFO] - {"epoch": 44, "train_loss": "6.909", "train_nll_loss": "5.45", "train_ppl": "43.71", "train_wps": "1349.4", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3771", "train_lr": "8.32256e-06", "train_gnorm": "2.171", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "701", "train_gb_free": "7.8", "train_wall": "35191"}
[2022-07-20 19:26:21,169][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 19:26:21,233][fairseq.trainer][INFO] - begin training epoch 45
[2022-07-20 19:26:21,234][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 19:30:26,281][train_inner][INFO] - {"epoch": 45, "update": 44.337, "loss": "6.911", "nll_loss": "5.452", "ppl": "43.77", "wps": "1361.6", "ups": "0.11", "wpb": "12524.1", "bsz": "254.4", "num_updates": "3800", "lr": "8.30769e-06", "gnorm": "2.159", "clip": "100", "loss_scale": "64", "train_wall": "1636", "gb_free": "7.5", "wall": "35436"}
[2022-07-20 19:34:03,983][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
[2022-07-20 19:38:16,838][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 19:39:21,348][valid][INFO] - {"epoch": 45, "valid_loss": "7.06", "valid_nll_loss": "5.576", "valid_ppl": "47.69", "valid_wps": "2373.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3856", "valid_best_loss": "7.06"}
[2022-07-20 19:39:21,351][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 45 @ 3856 updates
[2022-07-20 19:39:21,353][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 19:39:27,950][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 19:39:32,194][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 45 @ 3856 updates, score 7.06) (writing took 10.842979341046885 seconds)
[2022-07-20 19:39:32,195][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2022-07-20 19:39:32,202][train][INFO] - {"epoch": 45, "train_loss": "6.892", "train_nll_loss": "5.43", "train_ppl": "43.11", "train_wps": "1340.4", "train_ups": "0.11", "train_wpb": "12474.7", "train_bsz": "254.2", "train_num_updates": "3856", "train_lr": "8.27897e-06", "train_gnorm": "2.154", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "694", "train_gb_free": "7.3", "train_wall": "35982"}
[2022-07-20 19:39:32,215][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 19:39:32,276][fairseq.trainer][INFO] - begin training epoch 46
[2022-07-20 19:39:32,278][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 19:39:49,239][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 19:51:26,180][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 19:52:30,933][valid][INFO] - {"epoch": 46, "valid_loss": "7.046", "valid_nll_loss": "5.559", "valid_ppl": "47.15", "valid_wps": "2363.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3941", "valid_best_loss": "7.046"}
[2022-07-20 19:52:30,936][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 3941 updates
[2022-07-20 19:52:30,940][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 19:52:37,533][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 19:52:41,701][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 46 @ 3941 updates, score 7.046) (writing took 10.765118594048545 seconds)
[2022-07-20 19:52:41,702][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2022-07-20 19:52:41,708][train][INFO] - {"epoch": 46, "train_loss": "6.873", "train_nll_loss": "5.408", "train_ppl": "42.46", "train_wps": "1343.8", "train_ups": "0.11", "train_wpb": "12481.6", "train_bsz": "254.2", "train_num_updates": "3941", "train_lr": "8.23538e-06", "train_gnorm": "2.145", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "693", "train_gb_free": "7.2", "train_wall": "36772"}
[2022-07-20 19:52:41,722][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 19:52:41,787][fairseq.trainer][INFO] - begin training epoch 47
[2022-07-20 19:52:41,788][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 20:00:55,900][train_inner][INFO] - {"epoch": 47, "update": 46.686, "loss": "6.878", "nll_loss": "5.413", "ppl": "42.61", "wps": "1366.9", "ups": "0.11", "wpb": "12504.5", "bsz": "254.4", "num_updates": "4000", "lr": "8.20513e-06", "gnorm": "2.143", "clip": "100", "loss_scale": "32", "train_wall": "1629", "gb_free": "7.4", "wall": "37266"}
[2022-07-20 20:04:36,005][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 20:05:40,854][valid][INFO] - {"epoch": 47, "valid_loss": "7.043", "valid_nll_loss": "5.547", "valid_ppl": "46.76", "valid_wps": "2360.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "4027", "valid_best_loss": "7.043"}
[2022-07-20 20:05:40,858][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 47 @ 4027 updates
[2022-07-20 20:05:40,861][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 20:05:47,434][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 20:05:51,704][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 47 @ 4027 updates, score 7.043) (writing took 10.845720584969968 seconds)
[2022-07-20 20:05:51,707][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2022-07-20 20:05:51,715][train][INFO] - {"epoch": 47, "train_loss": "6.856", "train_nll_loss": "5.388", "train_ppl": "41.87", "train_wps": "1359", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "4027", "train_lr": "8.19128e-06", "train_gnorm": "2.134", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "693", "train_gb_free": "7.3", "train_wall": "37562"}
[2022-07-20 20:05:51,737][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 20:05:52,067][fairseq.trainer][INFO] - begin training epoch 48
[2022-07-20 20:05:52,068][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 20:17:45,973][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 20:18:50,916][valid][INFO] - {"epoch": 48, "valid_loss": "7.024", "valid_nll_loss": "5.539", "valid_ppl": "46.49", "valid_wps": "2357.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "4113", "valid_best_loss": "7.024"}
[2022-07-20 20:18:50,920][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 4113 updates
[2022-07-20 20:18:50,922][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 20:18:57,542][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 20:19:01,885][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 48 @ 4113 updates, score 7.024) (writing took 10.965032854117453 seconds)
[2022-07-20 20:19:01,886][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2022-07-20 20:19:01,896][train][INFO] - {"epoch": 48, "train_loss": "6.84", "train_nll_loss": "5.368", "train_ppl": "41.31", "train_wps": "1358.7", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "4113", "train_lr": "8.14718e-06", "train_gnorm": "2.153", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "693", "train_gb_free": "7.9", "train_wall": "38352"}
[2022-07-20 20:19:01,908][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 20:19:01,977][fairseq.trainer][INFO] - begin training epoch 49
[2022-07-20 20:19:01,978][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 20:30:55,968][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 20:32:00,560][valid][INFO] - {"epoch": 49, "valid_loss": "7.023", "valid_nll_loss": "5.523", "valid_ppl": "46", "valid_wps": "2370.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "4199", "valid_best_loss": "7.023"}
[2022-07-20 20:32:00,565][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 49 @ 4199 updates
[2022-07-20 20:32:00,568][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 20:32:07,130][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 20:32:12,827][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 49 @ 4199 updates, score 7.023) (writing took 12.262736222008243 seconds)
[2022-07-20 20:32:12,829][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2022-07-20 20:32:12,834][train][INFO] - {"epoch": 49, "train_loss": "6.822", "train_nll_loss": "5.348", "train_ppl": "40.73", "train_wps": "1357.4", "train_ups": "0.11", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "4199", "train_lr": "8.10308e-06", "train_gnorm": "2.138", "train_clip": "100", "train_loss_scale": "64", "train_train_wall": "693", "train_gb_free": "7.7", "train_wall": "39143"}
[2022-07-20 20:32:12,849][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-20 20:32:13,369][fairseq.trainer][INFO] - begin training epoch 50
[2022-07-20 20:32:13,370][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-20 20:32:21,854][train_inner][INFO] - {"epoch": 50, "update": 49.012, "loss": "6.833", "nll_loss": "5.361", "ppl": "41.1", "wps": "1316.9", "ups": "0.11", "wpb": "12417.8", "bsz": "253.7", "num_updates": "4200", "lr": "8.10256e-06", "gnorm": "2.147", "clip": "100", "loss_scale": "64", "train_wall": "1608", "gb_free": "7.5", "wall": "39152"}
[2022-07-20 20:33:20,670][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-20 20:44:07,774][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-20 20:45:12,288][valid][INFO] - {"epoch": 50, "valid_loss": "7.011", "valid_nll_loss": "5.512", "valid_ppl": "45.63", "valid_wps": "2373.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "4284", "valid_best_loss": "7.011"}
[2022-07-20 20:45:12,291][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 4284 updates
[2022-07-20 20:45:12,293][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 20:45:18,716][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-20/09-39-25/0/checkpoints/checkpoint_best.pt
[2022-07-20 20:45:22,965][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 50 @ 4284 updates, score 7.011) (writing took 10.674630732974038 seconds)
[2022-07-20 20:45:22,967][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2022-07-20 20:45:22,973][train][INFO] - {"epoch": 50, "train_loss": "6.808", "train_nll_loss": "5.332", "train_ppl": "40.28", "train_wps": "1339", "train_ups": "0.11", "train_wpb": "12446.9", "train_bsz": "254.2", "train_num_updates": "4284", "train_lr": "8.05949e-06", "train_gnorm": "2.151", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "693", "train_gb_free": "7.9", "train_wall": "39933"}
[2022-07-20 20:45:22,980][fairseq_cli.train][INFO] - done training in 39933.0 seconds
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: - 0.012 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: \ 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: | 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: / 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: - 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: \ 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: | 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: / 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb: - 0.103 MB of 0.103 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:              train/bsz 254.2
wandb:             train/clip 100.0
wandb:          train/gb_free 7.9
wandb:            train/gnorm 2.151
wandb:             train/loss 6.808
wandb:       train/loss_scale 32.0
wandb:               train/lr 1e-05
wandb:         train/nll_loss 5.332
wandb:              train/ppl 40.28
wandb:       train/train_wall 693.0
wandb:              train/ups 0.11
wandb:             train/wall 39933.0
wandb:              train/wpb 12446.9
wandb:              train/wps 1339.0
wandb:        train_inner/bsz 253.7
wandb:       train_inner/clip 100.0
wandb:    train_inner/gb_free 7.5
wandb:      train_inner/gnorm 2.147
wandb:       train_inner/loss 6.833
wandb: train_inner/loss_scale 64.0
wandb:         train_inner/lr 1e-05
wandb:   train_inner/nll_loss 5.361
wandb:        train_inner/ppl 41.1
wandb: train_inner/train_wall 1608.0
wandb:        train_inner/ups 0.11
wandb:       train_inner/wall 39152.0
wandb:        train_inner/wpb 12417.8
wandb:        train_inner/wps 1316.9
wandb:        valid/best_loss 7.011
wandb:              valid/bsz 4.0
wandb:             valid/loss 7.011
wandb:         valid/nll_loss 5.512
wandb:              valid/ppl 45.63
wandb:              valid/wpb 195.7
wandb:              valid/wps 2373.2
wandb: 
wandb: Synced checkpoints: https://wandb.ai/redredouane/RoBERTa_encdec/runs/2cueo7h7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./multirun/2022-07-20/09-39-25/0/wandb/run-20220720_093954-2cueo7h7/logs
