[2022-07-22 06:35:51,484][HYDRA] Launching 1 jobs locally
[2022-07-22 06:35:51,484][HYDRA] 	#0 : 
[2022-07-22 06:35:53,739][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'RoBERTa_encdec', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 4, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 4, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 20000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [64], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'roberta_enc_dec', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'adaptive_input': False, 'encoder': {'_name': None, 'embed_path': None, 'embed_dim': 768, 'ffn_embed_dim': 3072, 'layers': 12, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None}, 'max_source_positions': 1024, 'decoder': {'_name': None, 'embed_path': None, 'embed_dim': 512, 'ffn_embed_dim': 2048, 'layers': 6, 'attention_heads': 8, 'normalize_before': False, 'learned_pos': False, 'layerdrop': 0.0, 'layers_to_keep': None, 'input_dim': 512, 'output_dim': 512}, 'max_target_positions': 1024, 'share_decoder_input_output_embed': True, 'share_all_embeddings': True, 'no_token_positional_embeddings': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'layernorm_embedding': False, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'no_cross_attention': False, 'cross_self_attention': False, 'quant_noise': {'_name': None, 'pq': 0.0, 'pq_block_size': 8, 'scalar': 0.0}, 'min_params_to_wrap': 100000000, 'char_inputs': False, 'relu_dropout': 0.0, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'export': False, 'no_decoder_final_norm': False, 'max_positions': 512, 'encoder_embed_dim': 768, 'encoder_layers': 12, 'encoder_ffn_embed_dim': 3072, 'hack_layernorm_embedding': False, 'pretrained_mlm_checkpoint': '/home/dahmanir/lisa/Models/RoBERTa_50_finetune_maskless.pt', 'load_pretrained_mlm_checkpoint': None}, 'task': {'_name': 'translation', 'data': '/home/dahmanir/lisa/Datasets/wiki_binarized', 'source_lang': 'source', 'target_lang': 'target', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 512, 'max_target_positions': 512, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 20000.0, 'lr': [1e-05]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2022-07-22 06:35:53,911][fairseq.tasks.translation][INFO] - [source] dictionary: 39984 types
[2022-07-22 06:35:53,911][fairseq.tasks.translation][INFO] - [target] dictionary: 39984 types
[2022-07-22 06:36:04,119][fairseq.models.fairseq_model][WARNING] - using 'args' is deprecated, please update your code to use dataclass config
encoder.embed_tokens.weight False torch.Size([39984, 768])
encoder.layers.0.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.0.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.0.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.0.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.0.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.0.fc1.weight False torch.Size([3072, 768])
encoder.layers.0.fc1.bias False torch.Size([3072])
encoder.layers.0.fc2.weight False torch.Size([768, 3072])
encoder.layers.0.fc2.bias False torch.Size([768])
encoder.layers.0.final_layer_norm.weight False torch.Size([768])
encoder.layers.0.final_layer_norm.bias False torch.Size([768])
encoder.layers.1.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.1.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.1.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.1.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.1.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.1.fc1.weight False torch.Size([3072, 768])
encoder.layers.1.fc1.bias False torch.Size([3072])
encoder.layers.1.fc2.weight False torch.Size([768, 3072])
encoder.layers.1.fc2.bias False torch.Size([768])
encoder.layers.1.final_layer_norm.weight False torch.Size([768])
encoder.layers.1.final_layer_norm.bias False torch.Size([768])
encoder.layers.2.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.2.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.2.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.2.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.2.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.2.fc1.weight False torch.Size([3072, 768])
encoder.layers.2.fc1.bias False torch.Size([3072])
encoder.layers.2.fc2.weight False torch.Size([768, 3072])
encoder.layers.2.fc2.bias False torch.Size([768])
encoder.layers.2.final_layer_norm.weight False torch.Size([768])
encoder.layers.2.final_layer_norm.bias False torch.Size([768])
encoder.layers.3.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.3.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.3.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.3.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.3.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.3.fc1.weight False torch.Size([3072, 768])
encoder.layers.3.fc1.bias False torch.Size([3072])
encoder.layers.3.fc2.weight False torch.Size([768, 3072])
encoder.layers.3.fc2.bias False torch.Size([768])
encoder.layers.3.final_layer_norm.weight False torch.Size([768])
encoder.layers.3.final_layer_norm.bias False torch.Size([768])
encoder.layers.4.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.4.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.4.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.4.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.4.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.4.fc1.weight False torch.Size([3072, 768])
encoder.layers.4.fc1.bias False torch.Size([3072])
encoder.layers.4.fc2.weight False torch.Size([768, 3072])
encoder.layers.4.fc2.bias False torch.Size([768])
encoder.layers.4.final_layer_norm.weight False torch.Size([768])
encoder.layers.4.final_layer_norm.bias False torch.Size([768])
encoder.layers.5.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.5.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.5.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.5.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.5.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.5.fc1.weight False torch.Size([3072, 768])
encoder.layers.5.fc1.bias False torch.Size([3072])
encoder.layers.5.fc2.weight False torch.Size([768, 3072])
encoder.layers.5.fc2.bias False torch.Size([768])
encoder.layers.5.final_layer_norm.weight False torch.Size([768])
encoder.layers.5.final_layer_norm.bias False torch.Size([768])
encoder.layers.6.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.6.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.6.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.6.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.6.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.6.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.6.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.6.fc1.weight False torch.Size([3072, 768])
encoder.layers.6.fc1.bias False torch.Size([3072])
encoder.layers.6.fc2.weight False torch.Size([768, 3072])
encoder.layers.6.fc2.bias False torch.Size([768])
encoder.layers.6.final_layer_norm.weight False torch.Size([768])
encoder.layers.6.final_layer_norm.bias False torch.Size([768])
encoder.layers.7.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.7.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.7.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.7.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.7.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.7.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.7.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.7.fc1.weight False torch.Size([3072, 768])
encoder.layers.7.fc1.bias False torch.Size([3072])
encoder.layers.7.fc2.weight False torch.Size([768, 3072])
encoder.layers.7.fc2.bias False torch.Size([768])
encoder.layers.7.final_layer_norm.weight False torch.Size([768])
encoder.layers.7.final_layer_norm.bias False torch.Size([768])
encoder.layers.8.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.8.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.8.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.8.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.8.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.8.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.8.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.8.fc1.weight False torch.Size([3072, 768])
encoder.layers.8.fc1.bias False torch.Size([3072])
encoder.layers.8.fc2.weight False torch.Size([768, 3072])
encoder.layers.8.fc2.bias False torch.Size([768])
encoder.layers.8.final_layer_norm.weight False torch.Size([768])
encoder.layers.8.final_layer_norm.bias False torch.Size([768])
encoder.layers.9.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.9.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.9.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.9.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.9.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.9.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.9.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.9.fc1.weight False torch.Size([3072, 768])
encoder.layers.9.fc1.bias False torch.Size([3072])
encoder.layers.9.fc2.weight False torch.Size([768, 3072])
encoder.layers.9.fc2.bias False torch.Size([768])
encoder.layers.9.final_layer_norm.weight False torch.Size([768])
encoder.layers.9.final_layer_norm.bias False torch.Size([768])
encoder.layers.10.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.10.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.10.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.10.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.10.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.10.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.10.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.10.fc1.weight False torch.Size([3072, 768])
encoder.layers.10.fc1.bias False torch.Size([3072])
encoder.layers.10.fc2.weight False torch.Size([768, 3072])
encoder.layers.10.fc2.bias False torch.Size([768])
encoder.layers.10.final_layer_norm.weight False torch.Size([768])
encoder.layers.10.final_layer_norm.bias False torch.Size([768])
encoder.layers.11.self_attn.k_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.k_proj.bias False torch.Size([768])
encoder.layers.11.self_attn.v_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.v_proj.bias False torch.Size([768])
encoder.layers.11.self_attn.q_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.q_proj.bias False torch.Size([768])
encoder.layers.11.self_attn.out_proj.weight False torch.Size([768, 768])
encoder.layers.11.self_attn.out_proj.bias False torch.Size([768])
encoder.layers.11.self_attn_layer_norm.weight False torch.Size([768])
encoder.layers.11.self_attn_layer_norm.bias False torch.Size([768])
encoder.layers.11.fc1.weight False torch.Size([3072, 768])
encoder.layers.11.fc1.bias False torch.Size([3072])
encoder.layers.11.fc2.weight False torch.Size([768, 3072])
encoder.layers.11.fc2.bias False torch.Size([768])
encoder.layers.11.final_layer_norm.weight False torch.Size([768])
encoder.layers.11.final_layer_norm.bias False torch.Size([768])
decoder.layers.0.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.0.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.0.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.0.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.0.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.0.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.0.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.0.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.0.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.0.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.0.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.0.fc1.weight True torch.Size([3072, 768])
decoder.layers.0.fc1.bias True torch.Size([3072])
decoder.layers.0.fc2.weight True torch.Size([768, 3072])
decoder.layers.0.fc2.bias True torch.Size([768])
decoder.layers.0.final_layer_norm.weight True torch.Size([768])
decoder.layers.0.final_layer_norm.bias True torch.Size([768])
decoder.layers.1.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.1.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.1.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.1.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.1.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.1.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.1.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.1.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.1.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.1.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.1.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.1.fc1.weight True torch.Size([3072, 768])
decoder.layers.1.fc1.bias True torch.Size([3072])
decoder.layers.1.fc2.weight True torch.Size([768, 3072])
decoder.layers.1.fc2.bias True torch.Size([768])
decoder.layers.1.final_layer_norm.weight True torch.Size([768])
decoder.layers.1.final_layer_norm.bias True torch.Size([768])
decoder.layers.2.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.2.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.2.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.2.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.2.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.2.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.2.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.2.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.2.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.2.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.2.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.2.fc1.weight True torch.Size([3072, 768])
decoder.layers.2.fc1.bias True torch.Size([3072])
decoder.layers.2.fc2.weight True torch.Size([768, 3072])
decoder.layers.2.fc2.bias True torch.Size([768])
decoder.layers.2.final_layer_norm.weight True torch.Size([768])
decoder.layers.2.final_layer_norm.bias True torch.Size([768])
decoder.layers.3.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.3.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.3.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.3.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.3.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.3.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.3.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.3.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.3.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.3.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.3.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.3.fc1.weight True torch.Size([3072, 768])
decoder.layers.3.fc1.bias True torch.Size([3072])
decoder.layers.3.fc2.weight True torch.Size([768, 3072])
decoder.layers.3.fc2.bias True torch.Size([768])
decoder.layers.3.final_layer_norm.weight True torch.Size([768])
decoder.layers.3.final_layer_norm.bias True torch.Size([768])
decoder.layers.4.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.4.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.4.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.4.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.4.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.4.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.4.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.4.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.4.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.4.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.4.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.4.fc1.weight True torch.Size([3072, 768])
decoder.layers.4.fc1.bias True torch.Size([3072])
decoder.layers.4.fc2.weight True torch.Size([768, 3072])
decoder.layers.4.fc2.bias True torch.Size([768])
decoder.layers.4.final_layer_norm.weight True torch.Size([768])
decoder.layers.4.final_layer_norm.bias True torch.Size([768])
decoder.layers.5.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.5.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.5.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.5.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.5.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.5.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.5.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.5.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.5.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.5.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.5.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.5.fc1.weight True torch.Size([3072, 768])
decoder.layers.5.fc1.bias True torch.Size([3072])
decoder.layers.5.fc2.weight True torch.Size([768, 3072])
decoder.layers.5.fc2.bias True torch.Size([768])
decoder.layers.5.final_layer_norm.weight True torch.Size([768])
decoder.layers.5.final_layer_norm.bias True torch.Size([768])
decoder.layers.6.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.6.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.6.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.6.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.6.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.6.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.6.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.6.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.6.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.6.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.6.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.6.fc1.weight True torch.Size([3072, 768])
decoder.layers.6.fc1.bias True torch.Size([3072])
decoder.layers.6.fc2.weight True torch.Size([768, 3072])
decoder.layers.6.fc2.bias True torch.Size([768])
decoder.layers.6.final_layer_norm.weight True torch.Size([768])
decoder.layers.6.final_layer_norm.bias True torch.Size([768])
decoder.layers.7.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.7.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.7.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.7.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.7.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.7.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.7.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.7.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.7.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.7.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.7.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.7.fc1.weight True torch.Size([3072, 768])
decoder.layers.7.fc1.bias True torch.Size([3072])
decoder.layers.7.fc2.weight True torch.Size([768, 3072])
decoder.layers.7.fc2.bias True torch.Size([768])
decoder.layers.7.final_layer_norm.weight True torch.Size([768])
decoder.layers.7.final_layer_norm.bias True torch.Size([768])
decoder.layers.8.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.8.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.8.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.8.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.8.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.8.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.8.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.8.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.8.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.8.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.8.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.8.fc1.weight True torch.Size([3072, 768])
decoder.layers.8.fc1.bias True torch.Size([3072])
decoder.layers.8.fc2.weight True torch.Size([768, 3072])
decoder.layers.8.fc2.bias True torch.Size([768])
decoder.layers.8.final_layer_norm.weight True torch.Size([768])
decoder.layers.8.final_layer_norm.bias True torch.Size([768])
decoder.layers.9.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.9.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.9.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.9.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.9.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.9.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.9.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.9.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.9.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.9.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.9.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.9.fc1.weight True torch.Size([3072, 768])
decoder.layers.9.fc1.bias True torch.Size([3072])
decoder.layers.9.fc2.weight True torch.Size([768, 3072])
decoder.layers.9.fc2.bias True torch.Size([768])
decoder.layers.9.final_layer_norm.weight True torch.Size([768])
decoder.layers.9.final_layer_norm.bias True torch.Size([768])
decoder.layers.10.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.10.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.10.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.10.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.10.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.10.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.10.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.10.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.10.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.10.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.10.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.10.fc1.weight True torch.Size([3072, 768])
decoder.layers.10.fc1.bias True torch.Size([3072])
decoder.layers.10.fc2.weight True torch.Size([768, 3072])
decoder.layers.10.fc2.bias True torch.Size([768])
decoder.layers.10.final_layer_norm.weight True torch.Size([768])
decoder.layers.10.final_layer_norm.bias True torch.Size([768])
decoder.layers.11.self_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.k_proj.bias True torch.Size([768])
decoder.layers.11.self_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.v_proj.bias True torch.Size([768])
decoder.layers.11.self_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.q_proj.bias True torch.Size([768])
decoder.layers.11.self_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.11.self_attn.out_proj.bias True torch.Size([768])
decoder.layers.11.self_attn_layer_norm.weight True torch.Size([768])
decoder.layers.11.self_attn_layer_norm.bias True torch.Size([768])
decoder.layers.11.encoder_attn.k_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.k_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn.v_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.v_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn.q_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.q_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn.out_proj.weight True torch.Size([768, 768])
decoder.layers.11.encoder_attn.out_proj.bias True torch.Size([768])
decoder.layers.11.encoder_attn_layer_norm.weight True torch.Size([768])
decoder.layers.11.encoder_attn_layer_norm.bias True torch.Size([768])
decoder.layers.11.fc1.weight True torch.Size([3072, 768])
decoder.layers.11.fc1.bias True torch.Size([3072])
decoder.layers.11.fc2.weight True torch.Size([768, 3072])
decoder.layers.11.fc2.bias True torch.Size([768])
decoder.layers.11.final_layer_norm.weight True torch.Size([768])
decoder.layers.11.final_layer_norm.bias True torch.Size([768])
decoder.output_projection.bias False torch.Size([39984])
decoder.output_projection.dense.weight True torch.Size([768, 768])
decoder.output_projection.dense.bias True torch.Size([768])
decoder.output_projection.layer_norm.weight True torch.Size([768])
decoder.output_projection.layer_norm.bias True torch.Size([768])
[2022-07-22 06:36:09,224][fairseq_cli.train][INFO] - RobertaEncDecModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39984, 768, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39984, 768, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): RobertaLMHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[2022-07-22 06:36:09,230][fairseq_cli.train][INFO] - task: TranslationTask
[2022-07-22 06:36:09,230][fairseq_cli.train][INFO] - model: RobertaEncDecModel
[2022-07-22 06:36:09,230][fairseq_cli.train][INFO] - criterion: LabelSmoothedCrossEntropyCriterion
[2022-07-22 06:36:09,234][fairseq_cli.train][INFO] - num. shared model params: 229,815,600 (num. trained: 114,013,440)
[2022-07-22 06:36:09,237][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2022-07-22 06:36:09,261][fairseq.data.data_utils][INFO] - loaded 3,123 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/valid.source-target.source
[2022-07-22 06:36:09,263][fairseq.data.data_utils][INFO] - loaded 3,123 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/valid.source-target.target
[2022-07-22 06:36:09,263][fairseq.tasks.translation][INFO] - /home/dahmanir/lisa/Datasets/wiki_binarized valid source-target 3123 examples
[2022-07-22 06:36:14,584][fairseq.trainer][INFO] - detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
[2022-07-22 06:36:14,585][fairseq.trainer][INFO] - detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
[2022-07-22 06:36:14,586][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-07-22 06:36:14,586][fairseq.utils][INFO] - rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = NVIDIA GeForce GTX 1080 Ti              
[2022-07-22 06:36:14,586][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2022-07-22 06:36:14,587][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2022-07-22 06:36:14,587][fairseq_cli.train][INFO] - max tokens per device = None and max sentences per device = 4
[2022-07-22 06:36:14,589][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2022-07-22 06:36:14,589][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2022-07-22 06:36:14,590][fairseq.trainer][INFO] - loading train data for epoch 1
[2022-07-22 06:36:14,625][fairseq.data.data_utils][INFO] - loaded 21,866 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/train.source-target.source
[2022-07-22 06:36:14,632][fairseq.data.data_utils][INFO] - loaded 21,866 examples from: /home/dahmanir/lisa/Datasets/wiki_binarized/train.source-target.target
[2022-07-22 06:36:14,632][fairseq.tasks.translation][INFO] - /home/dahmanir/lisa/Datasets/wiki_binarized train source-target 21866 examples
[2022-07-22 06:36:14,641][fairseq.tasks.fairseq_task][WARNING] - 6 samples have invalid sizes and will be skipped, max_positions=(512, 512), first few sample ids=[4345, 8071, 5665, 126, 8210, 2220]
[2022-07-22 06:36:14,663][fairseq.trainer][INFO] - NOTE: your device does NOT support faster training with --fp16 or --amp, please switch to FP32 which is likely to be faster
[2022-07-22 06:36:14,695][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
wandb: Currently logged in as: redredouane. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.21 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.19
wandb: Run data is saved locally in /home/dahmanir/multirun/2022-07-22/06-35-49/0/wandb/run-20220722_063621-3voxyynd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run checkpoints
wandb:  View project at https://wandb.ai/redredouane/RoBERTa_encdec
wandb:  View run at https://wandb.ai/redredouane/RoBERTa_encdec/runs/3voxyynd
[2022-07-22 06:36:29,544][fairseq.trainer][INFO] - begin training epoch 1
[2022-07-22 06:36:29,560][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 06:36:42,955][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
[2022-07-22 06:36:55,079][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-22 06:37:06,607][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2022-07-22 06:37:19,634][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2022-07-22 06:37:31,360][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2022-07-22 06:37:43,222][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2022-07-22 06:49:49,643][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 06:50:56,897][valid][INFO] - {"epoch": 1, "valid_loss": "12.073", "valid_nll_loss": "11.36", "valid_ppl": "2628.27", "valid_wps": "2276.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "80"}
[2022-07-22 06:50:56,913][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 80 updates
[2022-07-22 06:50:56,914][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 06:51:03,076][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 06:51:06,849][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 80 updates, score 12.073) (writing took 9.935862437007017 seconds)
[2022-07-22 06:51:06,851][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2022-07-22 06:51:06,856][train][INFO] - {"epoch": 1, "train_loss": "15.889", "train_nll_loss": "15.455", "train_ppl": "44918.6", "train_wps": "1238.6", "train_ups": "0.1", "train_wpb": "12418.6", "train_bsz": "254.1", "train_num_updates": "80", "train_lr": "1.6e-06", "train_gnorm": "35.079", "train_clip": "100", "train_loss_scale": "2", "train_train_wall": "774", "train_gb_free": "8", "train_wall": "892"}
[2022-07-22 06:51:06,871][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 06:51:06,908][fairseq.trainer][INFO] - begin training epoch 2
[2022-07-22 06:51:06,909][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 07:04:21,035][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 07:05:28,545][valid][INFO] - {"epoch": 2, "valid_loss": "11.3", "valid_nll_loss": "10.542", "valid_ppl": "1491.45", "valid_wps": "2268.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "166", "valid_best_loss": "11.3"}
[2022-07-22 07:05:28,548][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 166 updates
[2022-07-22 07:05:28,552][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 07:05:36,648][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 07:05:40,706][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 166 updates, score 11.3) (writing took 12.158065604045987 seconds)
[2022-07-22 07:05:40,707][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2022-07-22 07:05:40,715][train][INFO] - {"epoch": 2, "train_loss": "11.674", "train_nll_loss": "10.95", "train_ppl": "1978.17", "train_wps": "1228.6", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "166", "train_lr": "3.32e-06", "train_gnorm": "1.956", "train_clip": "100", "train_loss_scale": "2", "train_train_wall": "767", "train_gb_free": "7.7", "train_wall": "1766"}
[2022-07-22 07:05:40,730][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 07:05:40,763][fairseq.trainer][INFO] - begin training epoch 3
[2022-07-22 07:05:40,764][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 07:11:01,613][train_inner][INFO] - {"epoch": 3, "update": 2.395, "loss": "13.285", "nll_loss": "12.672", "ppl": "6525.28", "wps": "1252.5", "ups": "0.1", "wpb": "12505.1", "bsz": "254.4", "num_updates": "200", "lr": "4e-06", "gnorm": "15.091", "clip": "100", "loss_scale": "2", "train_wall": "1851", "gb_free": "7.6", "wall": "2087"}
[2022-07-22 07:18:55,755][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 07:20:04,987][valid][INFO] - {"epoch": 3, "valid_loss": "10.368", "valid_nll_loss": "9.426", "valid_ppl": "687.69", "valid_wps": "2215.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "252", "valid_best_loss": "10.368"}
[2022-07-22 07:20:04,990][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 252 updates
[2022-07-22 07:20:04,991][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 07:20:11,540][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 07:20:15,647][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 252 updates, score 10.368) (writing took 10.657364098005928 seconds)
[2022-07-22 07:20:15,648][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2022-07-22 07:20:15,652][train][INFO] - {"epoch": 3, "train_loss": "11.044", "train_nll_loss": "10.246", "train_ppl": "1214.08", "train_wps": "1227.1", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "252", "train_lr": "5.04e-06", "train_gnorm": "1.802", "train_clip": "100", "train_loss_scale": "2", "train_train_wall": "768", "train_gb_free": "7.7", "train_wall": "2641"}
[2022-07-22 07:20:15,664][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 07:20:15,708][fairseq.trainer][INFO] - begin training epoch 4
[2022-07-22 07:20:15,710][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 07:33:22,993][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 07:34:44,335][valid][INFO] - {"epoch": 4, "valid_loss": "9.683", "valid_nll_loss": "8.598", "valid_ppl": "387.43", "valid_wps": "1884.6", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "338", "valid_best_loss": "9.683"}
[2022-07-22 07:34:44,350][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 338 updates
[2022-07-22 07:34:44,353][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 07:34:50,931][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 07:34:55,048][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 338 updates, score 9.683) (writing took 10.697705943952315 seconds)
[2022-07-22 07:34:55,048][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2022-07-22 07:34:55,056][train][INFO] - {"epoch": 4, "train_loss": "10.205", "train_nll_loss": "9.254", "train_ppl": "610.53", "train_wps": "1220.9", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "338", "train_lr": "6.76e-06", "train_gnorm": "2.85", "train_clip": "100", "train_loss_scale": "4", "train_train_wall": "761", "train_gb_free": "8.1", "train_wall": "3520"}
[2022-07-22 07:34:55,073][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 07:34:55,109][fairseq.trainer][INFO] - begin training epoch 5
[2022-07-22 07:34:55,110][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 07:44:20,591][train_inner][INFO] - {"epoch": 5, "update": 4.721, "loss": "10.221", "nll_loss": "9.275", "ppl": "619.47", "wps": "1246.2", "ups": "0.1", "wpb": "12455.6", "bsz": "254.4", "num_updates": "400", "lr": "8e-06", "gnorm": "2.67", "clip": "100", "loss_scale": "4", "train_wall": "1766", "gb_free": "7.5", "wall": "4086"}
[2022-07-22 07:47:52,654][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 07:49:10,748][valid][INFO] - {"epoch": 5, "valid_loss": "9.297", "valid_nll_loss": "8.154", "valid_ppl": "284.78", "valid_wps": "1963", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "424", "valid_best_loss": "9.297"}
[2022-07-22 07:49:10,767][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 424 updates
[2022-07-22 07:49:10,769][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 07:49:18,591][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 07:49:23,220][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 424 updates, score 9.297) (writing took 12.452834011986852 seconds)
[2022-07-22 07:49:23,220][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2022-07-22 07:49:23,223][train][INFO] - {"epoch": 5, "train_loss": "9.663", "train_nll_loss": "8.62", "train_ppl": "393.58", "train_wps": "1236.7", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "424", "train_lr": "8.48e-06", "train_gnorm": "3.139", "train_clip": "100", "train_loss_scale": "4", "train_train_wall": "752", "train_gb_free": "6.9", "train_wall": "4389"}
[2022-07-22 07:49:23,238][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 07:49:23,299][fairseq.trainer][INFO] - begin training epoch 6
[2022-07-22 07:49:23,301][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 08:02:26,035][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 08:03:33,775][valid][INFO] - {"epoch": 6, "valid_loss": "9.048", "valid_nll_loss": "7.85", "valid_ppl": "230.77", "valid_wps": "2260.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "510", "valid_best_loss": "9.048"}
[2022-07-22 08:03:33,779][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 510 updates
[2022-07-22 08:03:33,780][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 08:03:40,222][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 08:03:44,544][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 510 updates, score 9.048) (writing took 10.764757795957848 seconds)
[2022-07-22 08:03:44,547][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2022-07-22 08:03:44,554][train][INFO] - {"epoch": 6, "train_loss": "9.327", "train_nll_loss": "8.237", "train_ppl": "301.77", "train_wps": "1246.5", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "510", "train_lr": "9.99487e-06", "train_gnorm": "3", "train_clip": "100", "train_loss_scale": "4", "train_train_wall": "756", "train_gb_free": "7.6", "train_wall": "5250"}
[2022-07-22 08:03:44,573][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 08:03:44,619][fairseq.trainer][INFO] - begin training epoch 7
[2022-07-22 08:03:44,621][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 08:16:52,757][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 08:18:00,216][valid][INFO] - {"epoch": 7, "valid_loss": "8.812", "valid_nll_loss": "7.592", "valid_ppl": "192.92", "valid_wps": "2268.4", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "596", "valid_best_loss": "8.812"}
[2022-07-22 08:18:00,221][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 596 updates
[2022-07-22 08:18:00,224][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 08:18:06,766][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 08:18:10,884][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 596 updates, score 8.812) (writing took 10.662882067961618 seconds)
[2022-07-22 08:18:10,884][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2022-07-22 08:18:10,888][train][INFO] - {"epoch": 7, "train_loss": "9.054", "train_nll_loss": "7.928", "train_ppl": "243.47", "train_wps": "1239.3", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "596", "train_lr": "9.95077e-06", "train_gnorm": "3.124", "train_clip": "100", "train_loss_scale": "8", "train_train_wall": "761", "train_gb_free": "7.9", "train_wall": "6116"}
[2022-07-22 08:18:10,900][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 08:18:10,936][fairseq.trainer][INFO] - begin training epoch 8
[2022-07-22 08:18:10,938][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 08:18:46,573][train_inner][INFO] - {"epoch": 8, "update": 7.047, "loss": "9.222", "nll_loss": "8.118", "ppl": "277.78", "wps": "1207.8", "ups": "0.1", "wpb": "12476.9", "bsz": "253.7", "num_updates": "600", "lr": "9.94872e-06", "gnorm": "3.151", "clip": "100", "loss_scale": "8", "train_wall": "1757", "gb_free": "7.2", "wall": "6152"}
[2022-07-22 08:31:25,560][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 08:32:34,061][valid][INFO] - {"epoch": 8, "valid_loss": "8.619", "valid_nll_loss": "7.377", "valid_ppl": "166.27", "valid_wps": "2236.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "682", "valid_best_loss": "8.619"}
[2022-07-22 08:32:34,065][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 682 updates
[2022-07-22 08:32:34,067][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 08:32:40,517][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 08:32:44,742][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 682 updates, score 8.619) (writing took 10.67748810199555 seconds)
[2022-07-22 08:32:44,743][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2022-07-22 08:32:44,751][train][INFO] - {"epoch": 8, "train_loss": "8.843", "train_nll_loss": "7.691", "train_ppl": "206.67", "train_wps": "1228.6", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "682", "train_lr": "9.90667e-06", "train_gnorm": "2.858", "train_clip": "100", "train_loss_scale": "8", "train_train_wall": "768", "train_gb_free": "7.9", "train_wall": "6990"}
[2022-07-22 08:32:44,765][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 08:32:44,807][fairseq.trainer][INFO] - begin training epoch 9
[2022-07-22 08:32:44,809][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 08:45:49,058][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 08:47:04,151][valid][INFO] - {"epoch": 9, "valid_loss": "8.489", "valid_nll_loss": "7.223", "valid_ppl": "149.37", "valid_wps": "2040.6", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "768", "valid_best_loss": "8.489"}
[2022-07-22 08:47:04,154][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 768 updates
[2022-07-22 08:47:04,157][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 08:47:10,968][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 08:47:15,107][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 9 @ 768 updates, score 8.489) (writing took 10.95262020593509 seconds)
[2022-07-22 08:47:15,108][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2022-07-22 08:47:15,114][train][INFO] - {"epoch": 9, "train_loss": "8.673", "train_nll_loss": "7.5", "train_ppl": "181.02", "train_wps": "1233.6", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "768", "train_lr": "9.86256e-06", "train_gnorm": "2.875", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "759", "train_gb_free": "8", "train_wall": "7861"}
[2022-07-22 08:47:15,129][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 08:47:15,170][fairseq.trainer][INFO] - begin training epoch 10
[2022-07-22 08:47:15,171][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 08:51:57,597][train_inner][INFO] - {"epoch": 10, "update": 9.372, "loss": "8.73", "nll_loss": "7.564", "ppl": "189.29", "wps": "1257.6", "ups": "0.1", "wpb": "12519.2", "bsz": "254.4", "num_updates": "800", "lr": "9.84615e-06", "gnorm": "2.84", "clip": "100", "loss_scale": "16", "train_wall": "1765", "gb_free": "7", "wall": "8143"}
[2022-07-22 09:00:12,222][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 09:01:34,052][valid][INFO] - {"epoch": 10, "valid_loss": "8.375", "valid_nll_loss": "7.102", "valid_ppl": "137.36", "valid_wps": "1871.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "854", "valid_best_loss": "8.375"}
[2022-07-22 09:01:34,059][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 854 updates
[2022-07-22 09:01:34,071][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 09:01:42,321][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 09:01:47,600][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 854 updates, score 8.375) (writing took 13.54044090397656 seconds)
[2022-07-22 09:01:47,600][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2022-07-22 09:01:47,604][train][INFO] - {"epoch": 10, "train_loss": "8.536", "train_nll_loss": "7.345", "train_ppl": "162.58", "train_wps": "1230.5", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "854", "train_lr": "9.81846e-06", "train_gnorm": "2.792", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "752", "train_gb_free": "7.9", "train_wall": "8733"}
[2022-07-22 09:01:47,616][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 09:01:47,684][fairseq.trainer][INFO] - begin training epoch 11
[2022-07-22 09:01:47,685][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 09:14:47,399][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 09:16:07,167][valid][INFO] - {"epoch": 11, "valid_loss": "8.266", "valid_nll_loss": "6.982", "valid_ppl": "126.39", "valid_wps": "1922.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "940", "valid_best_loss": "8.266"}
[2022-07-22 09:16:07,184][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 940 updates
[2022-07-22 09:16:07,186][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 09:16:14,972][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 09:16:19,744][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 11 @ 940 updates, score 8.266) (writing took 12.559275193023495 seconds)
[2022-07-22 09:16:19,745][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2022-07-22 09:16:19,757][train][INFO] - {"epoch": 11, "train_loss": "8.418", "train_nll_loss": "7.211", "train_ppl": "148.12", "train_wps": "1231", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "940", "train_lr": "9.77436e-06", "train_gnorm": "2.933", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "754", "train_gb_free": "8.1", "train_wall": "9605"}
[2022-07-22 09:16:19,771][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 09:16:19,887][fairseq.trainer][INFO] - begin training epoch 12
[2022-07-22 09:16:19,892][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 09:25:36,202][train_inner][INFO] - {"epoch": 12, "update": 11.698, "loss": "8.414", "nll_loss": "7.205", "ppl": "147.54", "wps": "1234.2", "ups": "0.1", "wpb": "12456.9", "bsz": "254.4", "num_updates": "1000", "lr": "9.74359e-06", "gnorm": "2.856", "clip": "100", "loss_scale": "16", "train_wall": "1770", "gb_free": "7.6", "wall": "10162"}
[2022-07-22 09:29:23,013][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 09:30:36,098][valid][INFO] - {"epoch": 12, "valid_loss": "8.192", "valid_nll_loss": "6.89", "valid_ppl": "118.59", "valid_wps": "2095.6", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1026", "valid_best_loss": "8.192"}
[2022-07-22 09:30:36,101][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 1026 updates
[2022-07-22 09:30:36,102][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 09:30:42,734][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 09:30:47,907][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 1026 updates, score 8.192) (writing took 11.806629238999449 seconds)
[2022-07-22 09:30:47,912][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2022-07-22 09:30:47,915][train][INFO] - {"epoch": 12, "train_loss": "8.318", "train_nll_loss": "7.095", "train_ppl": "136.72", "train_wps": "1236.7", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1026", "train_lr": "9.73026e-06", "train_gnorm": "2.752", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "757", "train_gb_free": "7.2", "train_wall": "10473"}
[2022-07-22 09:30:47,933][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 09:30:47,989][fairseq.trainer][INFO] - begin training epoch 13
[2022-07-22 09:30:47,991][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 09:30:59,236][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2022-07-22 09:44:05,267][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 09:45:12,504][valid][INFO] - {"epoch": 13, "valid_loss": "8.107", "valid_nll_loss": "6.786", "valid_ppl": "110.38", "valid_wps": "2275.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1111", "valid_best_loss": "8.107"}
[2022-07-22 09:45:12,509][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 1111 updates
[2022-07-22 09:45:12,512][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 09:45:19,098][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 09:45:23,538][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 13 @ 1111 updates, score 8.107) (writing took 11.028784215915948 seconds)
[2022-07-22 09:45:23,539][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2022-07-22 09:45:23,546][train][INFO] - {"epoch": 13, "train_loss": "8.222", "train_nll_loss": "6.984", "train_ppl": "126.58", "train_wps": "1210.6", "train_ups": "0.1", "train_wpb": "12470.5", "train_bsz": "254.2", "train_num_updates": "1111", "train_lr": "9.68667e-06", "train_gnorm": "2.637", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "770", "train_gb_free": "7.7", "train_wall": "11349"}
[2022-07-22 09:45:23,561][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 09:45:23,612][fairseq.trainer][INFO] - begin training epoch 14
[2022-07-22 09:45:23,613][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 09:58:38,126][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 09:59:45,581][valid][INFO] - {"epoch": 14, "valid_loss": "8.014", "valid_nll_loss": "6.694", "valid_ppl": "103.53", "valid_wps": "2269.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1197", "valid_best_loss": "8.014"}
[2022-07-22 09:59:45,586][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 1197 updates
[2022-07-22 09:59:45,590][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 09:59:52,103][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 09:59:56,197][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 1197 updates, score 8.014) (writing took 10.610250644036569 seconds)
[2022-07-22 09:59:56,198][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2022-07-22 09:59:56,205][train][INFO] - {"epoch": 14, "train_loss": "8.136", "train_nll_loss": "6.885", "train_ppl": "118.16", "train_wps": "1230.3", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1197", "train_lr": "9.64256e-06", "train_gnorm": "2.615", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "768", "train_gb_free": "8", "train_wall": "12222"}
[2022-07-22 09:59:56,220][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 09:59:56,278][fairseq.trainer][INFO] - begin training epoch 15
[2022-07-22 09:59:56,280][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 10:00:23,165][train_inner][INFO] - {"epoch": 15, "update": 14.035, "loss": "8.192", "nll_loss": "6.949", "ppl": "123.58", "wps": "1194.4", "ups": "0.1", "wpb": "12463.2", "bsz": "253.7", "num_updates": "1200", "lr": "9.64103e-06", "gnorm": "2.646", "clip": "100", "loss_scale": "16", "train_wall": "1784", "gb_free": "7.6", "wall": "12249"}
[2022-07-22 10:13:02,988][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 10:14:17,527][valid][INFO] - {"epoch": 15, "valid_loss": "7.964", "valid_nll_loss": "6.62", "valid_ppl": "98.39", "valid_wps": "2056.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1283", "valid_best_loss": "7.964"}
[2022-07-22 10:14:17,532][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 1283 updates
[2022-07-22 10:14:17,534][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 10:14:24,136][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 10:14:28,568][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 15 @ 1283 updates, score 7.964) (writing took 11.03565898805391 seconds)
[2022-07-22 10:14:28,568][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2022-07-22 10:14:28,571][train][INFO] - {"epoch": 15, "train_loss": "8.059", "train_nll_loss": "6.796", "train_ppl": "111.12", "train_wps": "1230.7", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1283", "train_lr": "9.59846e-06", "train_gnorm": "2.836", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "760", "train_gb_free": "7.4", "train_wall": "13094"}
[2022-07-22 10:14:28,584][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 10:14:28,633][fairseq.trainer][INFO] - begin training epoch 16
[2022-07-22 10:14:28,635][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 10:20:04,142][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2022-07-22 10:27:29,370][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 10:28:51,037][valid][INFO] - {"epoch": 16, "valid_loss": "7.893", "valid_nll_loss": "6.55", "valid_ppl": "93.71", "valid_wps": "1875.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1368", "valid_best_loss": "7.893"}
[2022-07-22 10:28:51,054][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 1368 updates
[2022-07-22 10:28:51,055][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 10:28:58,969][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 10:29:03,860][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 16 @ 1368 updates, score 7.893) (writing took 12.805982742924243 seconds)
[2022-07-22 10:29:03,860][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2022-07-22 10:29:03,863][train][INFO] - {"epoch": 16, "train_loss": "7.989", "train_nll_loss": "6.715", "train_ppl": "105.05", "train_wps": "1211.7", "train_ups": "0.1", "train_wpb": "12477.6", "train_bsz": "254.2", "train_num_updates": "1368", "train_lr": "9.55487e-06", "train_gnorm": "2.715", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "755", "train_gb_free": "8.1", "train_wall": "13969"}
[2022-07-22 10:29:03,876][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 10:29:03,955][fairseq.trainer][INFO] - begin training epoch 17
[2022-07-22 10:29:03,957][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 10:33:56,149][train_inner][INFO] - {"epoch": 17, "update": 16.372, "loss": "8.007", "nll_loss": "6.735", "ppl": "106.55", "wps": "1241.9", "ups": "0.1", "wpb": "12499.4", "bsz": "254.4", "num_updates": "1400", "lr": "9.53846e-06", "gnorm": "2.762", "clip": "100", "loss_scale": "16", "train_wall": "1772", "gb_free": "7.9", "wall": "14262"}
[2022-07-22 10:42:03,005][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 10:43:10,195][valid][INFO] - {"epoch": 17, "valid_loss": "7.846", "valid_nll_loss": "6.486", "valid_ppl": "89.62", "valid_wps": "2278.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1454", "valid_best_loss": "7.846"}
[2022-07-22 10:43:10,201][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 1454 updates
[2022-07-22 10:43:10,203][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 10:43:16,705][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 10:43:20,763][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 17 @ 1454 updates, score 7.846) (writing took 10.562398876994848 seconds)
[2022-07-22 10:43:20,764][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2022-07-22 10:43:20,772][train][INFO] - {"epoch": 17, "train_loss": "7.925", "train_nll_loss": "6.641", "train_ppl": "99.83", "train_wps": "1252.9", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1454", "train_lr": "9.51077e-06", "train_gnorm": "2.702", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "755", "train_gb_free": "7", "train_wall": "14826"}
[2022-07-22 10:43:20,790][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 10:43:20,841][fairseq.trainer][INFO] - begin training epoch 18
[2022-07-22 10:43:20,842][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 10:56:00,231][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 10:57:07,356][valid][INFO] - {"epoch": 18, "valid_loss": "7.786", "valid_nll_loss": "6.417", "valid_ppl": "85.43", "valid_wps": "2280.4", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1540", "valid_best_loss": "7.786"}
[2022-07-22 10:57:07,360][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 1540 updates
[2022-07-22 10:57:07,362][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 10:57:13,867][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 10:57:18,086][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 18 @ 1540 updates, score 7.786) (writing took 10.725504503003322 seconds)
[2022-07-22 10:57:18,087][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2022-07-22 10:57:18,093][train][INFO] - {"epoch": 18, "train_loss": "7.863", "train_nll_loss": "6.568", "train_ppl": "94.89", "train_wps": "1282.2", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1540", "train_lr": "9.46667e-06", "train_gnorm": "2.611", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "737", "train_gb_free": "7.7", "train_wall": "15664"}
[2022-07-22 10:57:18,108][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 10:57:18,152][fairseq.trainer][INFO] - begin training epoch 19
[2022-07-22 10:57:18,153][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 11:06:10,665][train_inner][INFO] - {"epoch": 19, "update": 18.698, "loss": "7.865", "nll_loss": "6.571", "ppl": "95.08", "wps": "1293.7", "ups": "0.1", "wpb": "12513.6", "bsz": "254.4", "num_updates": "1600", "lr": "9.4359e-06", "gnorm": "2.684", "clip": "100", "loss_scale": "32", "train_wall": "1725", "gb_free": "7.3", "wall": "16196"}
[2022-07-22 11:09:55,215][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 11:11:02,321][valid][INFO] - {"epoch": 19, "valid_loss": "7.74", "valid_nll_loss": "6.36", "valid_ppl": "82.16", "valid_wps": "2281.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1626", "valid_best_loss": "7.74"}
[2022-07-22 11:11:02,324][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 1626 updates
[2022-07-22 11:11:02,326][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 11:11:08,798][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 11:11:12,944][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 19 @ 1626 updates, score 7.74) (writing took 10.619422236108221 seconds)
[2022-07-22 11:11:12,945][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2022-07-22 11:11:12,950][train][INFO] - {"epoch": 19, "train_loss": "7.803", "train_nll_loss": "6.5", "train_ppl": "90.48", "train_wps": "1286", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1626", "train_lr": "9.42256e-06", "train_gnorm": "2.719", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "734", "train_gb_free": "7.6", "train_wall": "16498"}
[2022-07-22 11:11:12,966][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 11:11:13,020][fairseq.trainer][INFO] - begin training epoch 20
[2022-07-22 11:11:13,022][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 11:23:54,188][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 11:25:01,527][valid][INFO] - {"epoch": 20, "valid_loss": "7.693", "valid_nll_loss": "6.321", "valid_ppl": "79.96", "valid_wps": "2272.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1712", "valid_best_loss": "7.693"}
[2022-07-22 11:25:01,531][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 1712 updates
[2022-07-22 11:25:01,533][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 11:25:08,050][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 11:25:12,178][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 20 @ 1712 updates, score 7.693) (writing took 10.646652868948877 seconds)
[2022-07-22 11:25:12,179][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2022-07-22 11:25:12,182][train][INFO] - {"epoch": 20, "train_loss": "7.752", "train_nll_loss": "6.44", "train_ppl": "86.85", "train_wps": "1279.3", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1712", "train_lr": "9.37846e-06", "train_gnorm": "2.756", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "738", "train_gb_free": "7.8", "train_wall": "17338"}
[2022-07-22 11:25:12,196][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 11:25:12,252][fairseq.trainer][INFO] - begin training epoch 21
[2022-07-22 11:25:12,254][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 11:37:49,957][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 11:38:57,186][valid][INFO] - {"epoch": 21, "valid_loss": "7.652", "valid_nll_loss": "6.261", "valid_ppl": "76.67", "valid_wps": "2276.4", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1798", "valid_best_loss": "7.652"}
[2022-07-22 11:38:57,190][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 1798 updates
[2022-07-22 11:38:57,191][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 11:39:03,744][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 11:39:08,489][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 21 @ 1798 updates, score 7.652) (writing took 11.29945274896454 seconds)
[2022-07-22 11:39:08,490][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2022-07-22 11:39:08,496][train][INFO] - {"epoch": 21, "train_loss": "7.702", "train_nll_loss": "6.383", "train_ppl": "83.46", "train_wps": "1283.8", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1798", "train_lr": "9.33436e-06", "train_gnorm": "2.7", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "735", "train_gb_free": "7.7", "train_wall": "18174"}
[2022-07-22 11:39:08,512][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 11:39:08,563][fairseq.trainer][INFO] - begin training epoch 22
[2022-07-22 11:39:08,564][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 11:39:26,959][train_inner][INFO] - {"epoch": 22, "update": 21.023, "loss": "7.733", "nll_loss": "6.419", "ppl": "85.54", "wps": "1245.6", "ups": "0.1", "wpb": "12432.5", "bsz": "253.7", "num_updates": "1800", "lr": "9.33333e-06", "gnorm": "2.713", "clip": "100", "loss_scale": "32", "train_wall": "1709", "gb_free": "7.5", "wall": "18192"}
[2022-07-22 11:44:08,606][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-22 11:51:45,522][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 11:52:53,029][valid][INFO] - {"epoch": 22, "valid_loss": "7.617", "valid_nll_loss": "6.226", "valid_ppl": "74.86", "valid_wps": "2267.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1883", "valid_best_loss": "7.617"}
[2022-07-22 11:52:53,032][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 1883 updates
[2022-07-22 11:52:53,034][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 11:52:59,525][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 11:53:03,615][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 22 @ 1883 updates, score 7.617) (writing took 10.582123236032203 seconds)
[2022-07-22 11:53:03,616][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2022-07-22 11:53:03,622][train][INFO] - {"epoch": 22, "train_loss": "7.652", "train_nll_loss": "6.324", "train_ppl": "80.11", "train_wps": "1271.5", "train_ups": "0.1", "train_wpb": "12492.6", "train_bsz": "254.2", "train_num_updates": "1883", "train_lr": "9.29077e-06", "train_gnorm": "2.771", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "734", "train_gb_free": "7.6", "train_wall": "19009"}
[2022-07-22 11:53:03,638][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 11:53:03,696][fairseq.trainer][INFO] - begin training epoch 23
[2022-07-22 11:53:03,698][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 12:05:40,955][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 12:06:48,309][valid][INFO] - {"epoch": 23, "valid_loss": "7.567", "valid_nll_loss": "6.18", "valid_ppl": "72.49", "valid_wps": "2272", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "1969", "valid_best_loss": "7.567"}
[2022-07-22 12:06:48,312][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 1969 updates
[2022-07-22 12:06:48,315][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 12:06:57,770][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 12:07:03,790][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 23 @ 1969 updates, score 7.567) (writing took 15.477547140908428 seconds)
[2022-07-22 12:07:03,791][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2022-07-22 12:07:03,794][train][INFO] - {"epoch": 23, "train_loss": "7.609", "train_nll_loss": "6.275", "train_ppl": "77.43", "train_wps": "1277.9", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "1969", "train_lr": "9.24667e-06", "train_gnorm": "2.653", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "734", "train_gb_free": "7.2", "train_wall": "19849"}
[2022-07-22 12:07:03,813][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 12:07:03,869][fairseq.trainer][INFO] - begin training epoch 24
[2022-07-22 12:07:03,871][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 12:11:38,523][train_inner][INFO] - {"epoch": 24, "update": 23.36, "loss": "7.62", "nll_loss": "6.288", "ppl": "78.11", "wps": "1296.7", "ups": "0.1", "wpb": "12522.8", "bsz": "254.4", "num_updates": "2000", "lr": "9.23077e-06", "gnorm": "2.728", "clip": "100", "loss_scale": "32", "train_wall": "1717", "gb_free": "6.9", "wall": "20124"}
[2022-07-22 12:18:46,231][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2022-07-22 12:19:42,207][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 12:20:49,424][valid][INFO] - {"epoch": 24, "valid_loss": "7.553", "valid_nll_loss": "6.157", "valid_ppl": "71.36", "valid_wps": "2276.7", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2054", "valid_best_loss": "7.553"}
[2022-07-22 12:20:49,427][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 2054 updates
[2022-07-22 12:20:49,429][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 12:20:55,889][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 12:21:01,869][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 24 @ 2054 updates, score 7.553) (writing took 12.441135389031842 seconds)
[2022-07-22 12:21:01,872][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2022-07-22 12:21:01,880][train][INFO] - {"epoch": 24, "train_loss": "7.566", "train_nll_loss": "6.225", "train_ppl": "74.82", "train_wps": "1265", "train_ups": "0.1", "train_wpb": "12472.6", "train_bsz": "254.2", "train_num_updates": "2054", "train_lr": "9.20308e-06", "train_gnorm": "2.811", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "736", "train_gb_free": "8.1", "train_wall": "20687"}
[2022-07-22 12:21:01,897][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 12:21:01,949][fairseq.trainer][INFO] - begin training epoch 25
[2022-07-22 12:21:01,950][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 12:33:39,524][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 12:34:46,959][valid][INFO] - {"epoch": 25, "valid_loss": "7.521", "valid_nll_loss": "6.113", "valid_ppl": "69.23", "valid_wps": "2269.6", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2140", "valid_best_loss": "7.521"}
[2022-07-22 12:34:46,963][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 2140 updates
[2022-07-22 12:34:46,964][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 12:34:53,467][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 12:34:57,598][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 25 @ 2140 updates, score 7.521) (writing took 10.635685897897929 seconds)
[2022-07-22 12:34:57,599][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2022-07-22 12:34:57,606][train][INFO] - {"epoch": 25, "train_loss": "7.528", "train_nll_loss": "6.181", "train_ppl": "72.56", "train_wps": "1284.7", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2140", "train_lr": "9.15897e-06", "train_gnorm": "2.689", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "735", "train_gb_free": "7.7", "train_wall": "21523"}
[2022-07-22 12:34:57,624][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 12:34:57,679][fairseq.trainer][INFO] - begin training epoch 26
[2022-07-22 12:34:57,680][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 12:43:53,748][train_inner][INFO] - {"epoch": 26, "update": 25.698, "loss": "7.52", "nll_loss": "6.172", "ppl": "72.12", "wps": "1291.1", "ups": "0.1", "wpb": "12493.1", "bsz": "254.4", "num_updates": "2200", "lr": "9.12821e-06", "gnorm": "2.729", "clip": "100", "loss_scale": "16", "train_wall": "1724", "gb_free": "8", "wall": "22059"}
[2022-07-22 12:47:38,448][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 12:48:45,821][valid][INFO] - {"epoch": 26, "valid_loss": "7.484", "valid_nll_loss": "6.079", "valid_ppl": "67.62", "valid_wps": "2271.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2226", "valid_best_loss": "7.484"}
[2022-07-22 12:48:45,826][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 2226 updates
[2022-07-22 12:48:45,827][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 12:48:52,270][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 12:48:56,378][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 26 @ 2226 updates, score 7.484) (writing took 10.552438118029386 seconds)
[2022-07-22 12:48:56,379][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2022-07-22 12:48:56,385][train][INFO] - {"epoch": 26, "train_loss": "7.488", "train_nll_loss": "6.135", "train_ppl": "70.26", "train_wps": "1280", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2226", "train_lr": "9.11487e-06", "train_gnorm": "2.739", "train_clip": "100", "train_loss_scale": "16", "train_train_wall": "738", "train_gb_free": "7.9", "train_wall": "22362"}
[2022-07-22 12:48:56,401][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 12:48:56,470][fairseq.trainer][INFO] - begin training epoch 27
[2022-07-22 12:48:56,471][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 13:01:37,633][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 13:02:44,978][valid][INFO] - {"epoch": 27, "valid_loss": "7.462", "valid_nll_loss": "6.048", "valid_ppl": "66.15", "valid_wps": "2272.6", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2312", "valid_best_loss": "7.462"}
[2022-07-22 13:02:44,982][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 2312 updates
[2022-07-22 13:02:44,986][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 13:02:51,520][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 13:02:55,739][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 27 @ 2312 updates, score 7.462) (writing took 10.756960410042666 seconds)
[2022-07-22 13:02:55,740][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2022-07-22 13:02:55,744][train][INFO] - {"epoch": 27, "train_loss": "7.453", "train_nll_loss": "6.094", "train_ppl": "68.29", "train_wps": "1279.1", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2312", "train_lr": "9.07077e-06", "train_gnorm": "2.742", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "739", "train_gb_free": "7.9", "train_wall": "23201"}
[2022-07-22 13:02:55,757][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 13:02:55,810][fairseq.trainer][INFO] - begin training epoch 28
[2022-07-22 13:02:55,813][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 13:15:35,901][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 13:16:43,485][valid][INFO] - {"epoch": 28, "valid_loss": "7.429", "valid_nll_loss": "6.016", "valid_ppl": "64.71", "valid_wps": "2265.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2398", "valid_best_loss": "7.429"}
[2022-07-22 13:16:43,488][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 2398 updates
[2022-07-22 13:16:43,492][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 13:16:49,978][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 13:16:54,097][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 28 @ 2398 updates, score 7.429) (writing took 10.609072045073844 seconds)
[2022-07-22 13:16:54,098][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2022-07-22 13:16:54,105][train][INFO] - {"epoch": 28, "train_loss": "7.418", "train_nll_loss": "6.054", "train_ppl": "66.44", "train_wps": "1280.6", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2398", "train_lr": "9.02667e-06", "train_gnorm": "2.653", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "738", "train_gb_free": "7.9", "train_wall": "24040"}
[2022-07-22 13:16:54,120][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 13:16:54,185][fairseq.trainer][INFO] - begin training epoch 29
[2022-07-22 13:16:54,187][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 13:17:12,232][train_inner][INFO] - {"epoch": 29, "update": 28.023, "loss": "7.448", "nll_loss": "6.088", "ppl": "68.02", "wps": "1244.7", "ups": "0.1", "wpb": "12437.7", "bsz": "253.7", "num_updates": "2400", "lr": "9.02564e-06", "gnorm": "2.707", "clip": "100", "loss_scale": "32", "train_wall": "1712", "gb_free": "7.6", "wall": "24058"}
[2022-07-22 13:29:33,848][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 13:30:40,968][valid][INFO] - {"epoch": 29, "valid_loss": "7.411", "valid_nll_loss": "5.987", "valid_ppl": "63.44", "valid_wps": "2280.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2484", "valid_best_loss": "7.411"}
[2022-07-22 13:30:40,973][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 2484 updates
[2022-07-22 13:30:40,976][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 13:30:47,389][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 13:30:51,457][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 29 @ 2484 updates, score 7.411) (writing took 10.483814842998981 seconds)
[2022-07-22 13:30:51,458][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2022-07-22 13:30:51,464][train][INFO] - {"epoch": 29, "train_loss": "7.383", "train_nll_loss": "6.013", "train_ppl": "64.58", "train_wps": "1282.2", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2484", "train_lr": "8.98256e-06", "train_gnorm": "2.748", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "737", "train_gb_free": "7.8", "train_wall": "24877"}
[2022-07-22 13:30:51,481][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 13:30:51,532][fairseq.trainer][INFO] - begin training epoch 30
[2022-07-22 13:30:51,533][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 13:42:05,812][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-22 13:43:29,518][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 13:44:36,539][valid][INFO] - {"epoch": 30, "valid_loss": "7.377", "valid_nll_loss": "5.953", "valid_ppl": "61.96", "valid_wps": "2284.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2569", "valid_best_loss": "7.377"}
[2022-07-22 13:44:36,543][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 2569 updates
[2022-07-22 13:44:36,545][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 13:44:43,035][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 13:44:47,149][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 30 @ 2569 updates, score 7.377) (writing took 10.605734448879957 seconds)
[2022-07-22 13:44:47,150][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2022-07-22 13:44:47,155][train][INFO] - {"epoch": 30, "train_loss": "7.353", "train_nll_loss": "5.979", "train_ppl": "63.07", "train_wps": "1268.9", "train_ups": "0.1", "train_wpb": "12475.4", "train_bsz": "254.2", "train_num_updates": "2569", "train_lr": "8.93897e-06", "train_gnorm": "2.757", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "735", "train_gb_free": "7.7", "train_wall": "25713"}
[2022-07-22 13:44:47,170][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 13:44:47,232][fairseq.trainer][INFO] - begin training epoch 31
[2022-07-22 13:44:47,233][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 13:49:24,890][train_inner][INFO] - {"epoch": 31, "update": 30.36, "loss": "7.36", "nll_loss": "5.987", "ppl": "63.41", "wps": "1292.7", "ups": "0.1", "wpb": "12491.6", "bsz": "254.4", "num_updates": "2600", "lr": "8.92308e-06", "gnorm": "2.733", "clip": "100", "loss_scale": "32", "train_wall": "1724", "gb_free": "7", "wall": "25990"}
[2022-07-22 13:57:26,725][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 13:58:34,205][valid][INFO] - {"epoch": 31, "valid_loss": "7.353", "valid_nll_loss": "5.925", "valid_ppl": "60.74", "valid_wps": "2268.3", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2655", "valid_best_loss": "7.353"}
[2022-07-22 13:58:34,209][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 2655 updates
[2022-07-22 13:58:34,211][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 13:58:40,665][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 13:58:44,734][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 31 @ 2655 updates, score 7.353) (writing took 10.524683395167813 seconds)
[2022-07-22 13:58:44,735][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2022-07-22 13:58:44,740][train][INFO] - {"epoch": 31, "train_loss": "7.321", "train_nll_loss": "5.942", "train_ppl": "61.46", "train_wps": "1281.8", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2655", "train_lr": "8.89487e-06", "train_gnorm": "2.662", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "737", "train_gb_free": "7.9", "train_wall": "26550"}
[2022-07-22 13:58:44,756][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 13:58:44,814][fairseq.trainer][INFO] - begin training epoch 32
[2022-07-22 13:58:44,816][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 14:11:22,858][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 14:12:30,274][valid][INFO] - {"epoch": 32, "valid_loss": "7.333", "valid_nll_loss": "5.9", "valid_ppl": "59.72", "valid_wps": "2271.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2741", "valid_best_loss": "7.333"}
[2022-07-22 14:12:30,279][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 2741 updates
[2022-07-22 14:12:30,282][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 14:12:36,813][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 14:12:40,969][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 32 @ 2741 updates, score 7.333) (writing took 10.689552502939478 seconds)
[2022-07-22 14:12:40,969][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2022-07-22 14:12:40,976][train][INFO] - {"epoch": 32, "train_loss": "7.292", "train_nll_loss": "5.908", "train_ppl": "60.03", "train_wps": "1283.9", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2741", "train_lr": "8.85077e-06", "train_gnorm": "2.767", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "735", "train_gb_free": "6.6", "train_wall": "27386"}
[2022-07-22 14:12:40,990][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 14:12:41,053][fairseq.trainer][INFO] - begin training epoch 33
[2022-07-22 14:12:41,055][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 14:21:23,981][train_inner][INFO] - {"epoch": 33, "update": 32.686, "loss": "7.294", "nll_loss": "5.91", "ppl": "60.11", "wps": "1303.1", "ups": "0.1", "wpb": "12504.1", "bsz": "254.4", "num_updates": "2800", "lr": "8.82051e-06", "gnorm": "2.738", "clip": "100", "loss_scale": "32", "train_wall": "1710", "gb_free": "6.6", "wall": "27909"}
[2022-07-22 14:23:45,638][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-22 14:25:16,827][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 14:26:24,273][valid][INFO] - {"epoch": 33, "valid_loss": "7.315", "valid_nll_loss": "5.879", "valid_ppl": "58.86", "valid_wps": "2271.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2826", "valid_best_loss": "7.315"}
[2022-07-22 14:26:24,278][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 2826 updates
[2022-07-22 14:26:24,281][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 14:26:30,757][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 14:26:34,831][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 33 @ 2826 updates, score 7.315) (writing took 10.552894770167768 seconds)
[2022-07-22 14:26:34,832][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2022-07-22 14:26:34,837][train][INFO] - {"epoch": 33, "train_loss": "7.265", "train_nll_loss": "5.876", "train_ppl": "58.73", "train_wps": "1272.4", "train_ups": "0.1", "train_wpb": "12482.6", "train_bsz": "254.2", "train_num_updates": "2826", "train_lr": "8.80718e-06", "train_gnorm": "2.777", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "733", "train_gb_free": "8", "train_wall": "28220"}
[2022-07-22 14:26:34,853][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 14:26:34,922][fairseq.trainer][INFO] - begin training epoch 34
[2022-07-22 14:26:34,924][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 14:39:15,048][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 14:40:22,303][valid][INFO] - {"epoch": 34, "valid_loss": "7.294", "valid_nll_loss": "5.855", "valid_ppl": "57.86", "valid_wps": "2282.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2912", "valid_best_loss": "7.294"}
[2022-07-22 14:40:22,307][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 2912 updates
[2022-07-22 14:40:22,308][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 14:40:29,225][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 14:40:33,309][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 34 @ 2912 updates, score 7.294) (writing took 11.001966233830899 seconds)
[2022-07-22 14:40:33,310][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2022-07-22 14:40:33,315][train][INFO] - {"epoch": 34, "train_loss": "7.238", "train_nll_loss": "5.845", "train_ppl": "57.46", "train_wps": "1280.5", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2912", "train_lr": "8.76308e-06", "train_gnorm": "2.667", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "737", "train_gb_free": "7.8", "train_wall": "29059"}
[2022-07-22 14:40:33,331][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 14:40:33,680][fairseq.trainer][INFO] - begin training epoch 35
[2022-07-22 14:40:33,681][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 14:53:13,693][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 14:54:21,153][valid][INFO] - {"epoch": 35, "valid_loss": "7.272", "valid_nll_loss": "5.821", "valid_ppl": "56.53", "valid_wps": "2271.1", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "2998", "valid_best_loss": "7.272"}
[2022-07-22 14:54:21,156][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 2998 updates
[2022-07-22 14:54:21,158][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 14:54:27,708][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 14:54:33,622][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 35 @ 2998 updates, score 7.272) (writing took 12.465890294872224 seconds)
[2022-07-22 14:54:33,623][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2022-07-22 14:54:33,626][train][INFO] - {"epoch": 35, "train_loss": "7.211", "train_nll_loss": "5.814", "train_ppl": "56.25", "train_wps": "1277.7", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "2998", "train_lr": "8.71897e-06", "train_gnorm": "2.715", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "737", "train_gb_free": "7.9", "train_wall": "29899"}
[2022-07-22 14:54:33,645][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 14:54:33,705][fairseq.trainer][INFO] - begin training epoch 36
[2022-07-22 14:54:33,707][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 14:54:52,622][train_inner][INFO] - {"epoch": 36, "update": 35.023, "loss": "7.225", "nll_loss": "5.83", "ppl": "56.9", "wps": "1241.5", "ups": "0.1", "wpb": "12468.4", "bsz": "253.7", "num_updates": "3000", "lr": "8.71795e-06", "gnorm": "2.712", "clip": "100", "loss_scale": "32", "train_wall": "1719", "gb_free": "7.5", "wall": "29918"}
[2022-07-22 15:05:31,147][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-22 15:07:11,330][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 15:08:18,880][valid][INFO] - {"epoch": 36, "valid_loss": "7.25", "valid_nll_loss": "5.805", "valid_ppl": "55.91", "valid_wps": "2267.9", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3083", "valid_best_loss": "7.25"}
[2022-07-22 15:08:18,884][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 3083 updates
[2022-07-22 15:08:18,885][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 15:08:25,384][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 15:08:30,594][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 36 @ 3083 updates, score 7.25) (writing took 11.710445079021156 seconds)
[2022-07-22 15:08:30,595][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2022-07-22 15:08:30,602][train][INFO] - {"epoch": 36, "train_loss": "7.185", "train_nll_loss": "5.784", "train_ppl": "55.09", "train_wps": "1266.5", "train_ups": "0.1", "train_wpb": "12470.8", "train_bsz": "254.2", "train_num_updates": "3083", "train_lr": "8.67538e-06", "train_gnorm": "2.664", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "735", "train_gb_free": "8.1", "train_wall": "30736"}
[2022-07-22 15:08:30,617][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 15:08:30,673][fairseq.trainer][INFO] - begin training epoch 37
[2022-07-22 15:08:30,675][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 15:21:08,860][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 15:22:16,014][valid][INFO] - {"epoch": 37, "valid_loss": "7.236", "valid_nll_loss": "5.789", "valid_ppl": "55.31", "valid_wps": "2281.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3169", "valid_best_loss": "7.236"}
[2022-07-22 15:22:16,019][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 3169 updates
[2022-07-22 15:22:16,023][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 15:22:22,480][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 15:22:26,627][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 37 @ 3169 updates, score 7.236) (writing took 10.607411562930793 seconds)
[2022-07-22 15:22:26,630][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2022-07-22 15:22:26,638][train][INFO] - {"epoch": 37, "train_loss": "7.159", "train_nll_loss": "5.753", "train_ppl": "53.94", "train_wps": "1284.2", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3169", "train_lr": "8.63128e-06", "train_gnorm": "2.712", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "736", "train_gb_free": "7.6", "train_wall": "31572"}
[2022-07-22 15:22:26,654][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 15:22:26,718][fairseq.trainer][INFO] - begin training epoch 38
[2022-07-22 15:22:26,720][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 15:27:02,521][train_inner][INFO] - {"epoch": 38, "update": 37.36, "loss": "7.172", "nll_loss": "5.768", "ppl": "54.5", "wps": "1295.4", "ups": "0.1", "wpb": "12500.2", "bsz": "254.4", "num_updates": "3200", "lr": "8.61538e-06", "gnorm": "2.688", "clip": "100", "loss_scale": "32", "train_wall": "1720", "gb_free": "6.9", "wall": "31848"}
[2022-07-22 15:35:05,661][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 15:36:13,055][valid][INFO] - {"epoch": 38, "valid_loss": "7.221", "valid_nll_loss": "5.766", "valid_ppl": "54.43", "valid_wps": "2272.8", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3255", "valid_best_loss": "7.221"}
[2022-07-22 15:36:13,059][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 3255 updates
[2022-07-22 15:36:13,060][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 15:36:19,555][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 15:36:23,710][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 38 @ 3255 updates, score 7.221) (writing took 10.651404006872326 seconds)
[2022-07-22 15:36:23,711][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2022-07-22 15:36:23,719][train][INFO] - {"epoch": 38, "train_loss": "7.136", "train_nll_loss": "5.726", "train_ppl": "52.93", "train_wps": "1282.6", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3255", "train_lr": "8.58718e-06", "train_gnorm": "2.679", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "736", "train_gb_free": "7.4", "train_wall": "32409"}
[2022-07-22 15:36:23,730][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 15:36:23,795][fairseq.trainer][INFO] - begin training epoch 39
[2022-07-22 15:36:23,797][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 15:47:12,870][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-22 15:49:02,316][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 15:50:09,598][valid][INFO] - {"epoch": 39, "valid_loss": "7.224", "valid_nll_loss": "5.757", "valid_ppl": "54.07", "valid_wps": "2275", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3340", "valid_best_loss": "7.221"}
[2022-07-22 15:50:09,602][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 3340 updates
[2022-07-22 15:50:09,605][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_last.pt
[2022-07-22 15:50:16,090][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_last.pt
[2022-07-22 15:50:16,115][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 39 @ 3340 updates, score 7.224) (writing took 6.513114525936544 seconds)
[2022-07-22 15:50:16,116][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2022-07-22 15:50:16,119][train][INFO] - {"epoch": 39, "train_loss": "7.115", "train_nll_loss": "5.702", "train_ppl": "52.07", "train_wps": "1275.4", "train_ups": "0.1", "train_wpb": "12490.4", "train_bsz": "254.2", "train_num_updates": "3340", "train_lr": "8.54359e-06", "train_gnorm": "2.71", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "736", "train_gb_free": "7.7", "train_wall": "33242"}
[2022-07-22 15:50:16,138][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 15:50:16,221][fairseq.trainer][INFO] - begin training epoch 40
[2022-07-22 15:50:16,223][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 15:59:09,018][train_inner][INFO] - {"epoch": 40, "update": 39.698, "loss": "7.107", "nll_loss": "5.693", "ppl": "51.74", "wps": "1296", "ups": "0.1", "wpb": "12483.5", "bsz": "254.4", "num_updates": "3400", "lr": "8.51282e-06", "gnorm": "2.699", "clip": "100", "loss_scale": "32", "train_wall": "1721", "gb_free": "7.4", "wall": "33774"}
[2022-07-22 16:02:52,825][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 16:04:00,150][valid][INFO] - {"epoch": 40, "valid_loss": "7.183", "valid_nll_loss": "5.734", "valid_ppl": "53.22", "valid_wps": "2273.2", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3426", "valid_best_loss": "7.183"}
[2022-07-22 16:04:00,157][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 3426 updates
[2022-07-22 16:04:00,160][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 16:04:06,749][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 16:04:11,270][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 40 @ 3426 updates, score 7.183) (writing took 11.113554782001302 seconds)
[2022-07-22 16:04:11,272][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2022-07-22 16:04:11,277][train][INFO] - {"epoch": 40, "train_loss": "7.091", "train_nll_loss": "5.674", "train_ppl": "51.05", "train_wps": "1285.6", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3426", "train_lr": "8.49949e-06", "train_gnorm": "2.713", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "734", "train_gb_free": "8", "train_wall": "34077"}
[2022-07-22 16:04:11,293][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 16:04:11,368][fairseq.trainer][INFO] - begin training epoch 41
[2022-07-22 16:04:11,370][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 16:16:50,148][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 16:17:57,366][valid][INFO] - {"epoch": 41, "valid_loss": "7.174", "valid_nll_loss": "5.719", "valid_ppl": "52.68", "valid_wps": "2277", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3512", "valid_best_loss": "7.174"}
[2022-07-22 16:17:57,369][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 3512 updates
[2022-07-22 16:17:57,371][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 16:18:03,875][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 16:18:12,690][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 41 @ 3512 updates, score 7.174) (writing took 15.32079514907673 seconds)
[2022-07-22 16:18:12,691][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2022-07-22 16:18:12,697][train][INFO] - {"epoch": 41, "train_loss": "7.068", "train_nll_loss": "5.648", "train_ppl": "50.15", "train_wps": "1276", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3512", "train_lr": "8.45538e-06", "train_gnorm": "2.673", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "736", "train_gb_free": "7.9", "train_wall": "34918"}
[2022-07-22 16:18:12,712][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 16:18:12,778][fairseq.trainer][INFO] - begin training epoch 42
[2022-07-22 16:18:12,779][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 16:29:38,404][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2022-07-22 16:30:53,109][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 16:32:00,905][valid][INFO] - {"epoch": 42, "valid_loss": "7.158", "valid_nll_loss": "5.7", "valid_ppl": "51.97", "valid_wps": "2257.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3597", "valid_best_loss": "7.158"}
[2022-07-22 16:32:00,908][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 3597 updates
[2022-07-22 16:32:00,910][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 16:32:07,411][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 16:32:13,156][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 42 @ 3597 updates, score 7.158) (writing took 12.247173930052668 seconds)
[2022-07-22 16:32:13,156][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2022-07-22 16:32:13,162][train][INFO] - {"epoch": 42, "train_loss": "7.05", "train_nll_loss": "5.626", "train_ppl": "49.39", "train_wps": "1262", "train_ups": "0.1", "train_wpb": "12478.2", "train_bsz": "254.2", "train_num_updates": "3597", "train_lr": "8.41179e-06", "train_gnorm": "2.665", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "738", "train_gb_free": "7.9", "train_wall": "35759"}
[2022-07-22 16:32:13,177][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 16:32:13,248][fairseq.trainer][INFO] - begin training epoch 43
[2022-07-22 16:32:13,249][fairseq_cli.train][INFO] - Start iterating over samples
[2022-07-22 16:32:40,942][train_inner][INFO] - {"epoch": 43, "update": 42.035, "loss": "7.065", "nll_loss": "5.644", "ppl": "50", "wps": "1237.7", "ups": "0.1", "wpb": "12450.7", "bsz": "253.7", "num_updates": "3600", "lr": "8.41026e-06", "gnorm": "2.675", "clip": "100", "loss_scale": "32", "train_wall": "1717", "gb_free": "7", "wall": "35786"}
[2022-07-22 16:44:53,425][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2022-07-22 16:46:00,887][valid][INFO] - {"epoch": 43, "valid_loss": "7.141", "valid_nll_loss": "5.681", "valid_ppl": "51.31", "valid_wps": "2269.5", "valid_wpb": "195.7", "valid_bsz": "4", "valid_num_updates": "3683", "valid_best_loss": "7.141"}
[2022-07-22 16:46:00,891][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 3683 updates
[2022-07-22 16:46:00,892][fairseq.trainer][INFO] - Saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 16:46:07,482][fairseq.trainer][INFO] - Finished saving checkpoint to /home/dahmanir/multirun/2022-07-22/06-35-49/0/checkpoints/checkpoint_best.pt
[2022-07-22 16:46:18,339][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 43 @ 3683 updates, score 7.141) (writing took 17.448632600018755 seconds)
[2022-07-22 16:46:18,340][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2022-07-22 16:46:18,345][train][INFO] - {"epoch": 43, "train_loss": "7.026", "train_nll_loss": "5.599", "train_ppl": "48.45", "train_wps": "1270.3", "train_ups": "0.1", "train_wpb": "12484.1", "train_bsz": "254.2", "train_num_updates": "3683", "train_lr": "8.36769e-06", "train_gnorm": "2.631", "train_clip": "100", "train_loss_scale": "32", "train_train_wall": "737", "train_gb_free": "7.5", "train_wall": "36604"}
[2022-07-22 16:46:18,358][fairseq.data.iterators][INFO] - grouped total_num_itrs = 86
[2022-07-22 16:46:18,418][fairseq.trainer][INFO] - begin training epoch 44
[2022-07-22 16:46:18,420][fairseq_cli.train][INFO] - Start iterating over samples
